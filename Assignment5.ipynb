{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Hi.\\tHallo!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #380701 (cburgmer)',\n",
       " 'Hi.\\tGrüß Gott!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #659813 (Esperantostern)',\n",
       " 'Run!\\tLauf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #906328 (papabear) & #941078 (Fingerhut)',\n",
       " 'Wow!\\tPotzdonner!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #2122382 (Pfirsichbaeumchen)',\n",
       " 'Wow!\\tDonnerwetter!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52027 (Zifre) & #2122391 (Pfirsichbaeumchen)',\n",
       " 'Fire!\\tFeuer!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829639 (Spamster) & #1958697 (Tamy)',\n",
       " 'Help!\\tHilfe!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #575889 (MUIRIEL)',\n",
       " 'Help!\\tZu Hülf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435084 (lukaszpp) & #2122375 (Pfirsichbaeumchen)',\n",
       " 'Stop!\\tStopp!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #448320 (FeuDRenais) & #626467 (jakov)',\n",
       " 'Wait!\\tWarte!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1744314 (belgavox) & #2122378 (Pfirsichbaeumchen)',\n",
       " 'Go on.\\tMach weiter.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2230774 (CK) & #6625701 (Felixjp)',\n",
       " 'Hello!\\tHallo!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #373330 (CK) & #380701 (cburgmer)',\n",
       " 'I ran.\\tIch rannte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828607 (CK) & #6643977 (Felixjp)',\n",
       " 'I see.\\tIch verstehe.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111796 (CK) & #547386 (Espi)',\n",
       " 'I see.\\tAha.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111796 (CK) & #5677459 (eric2)',\n",
       " 'I try.\\tIch probiere es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20776 (CK) & #4944510 (Hans_Adler)',\n",
       " 'I won!\\tIch hab gewonnen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005192 (CK) & #4941621 (Hans_Adler)',\n",
       " 'I won!\\tIch habe gewonnen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005192 (CK) & #4941622 (Hans_Adler)',\n",
       " 'Smile.\\tLächeln!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2764108 (CK) & #4659632 (AC)',\n",
       " 'Cheers!\\tZum Wohl!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #487006 (human600) & #957535 (Sudajaengi)',\n",
       " 'Eat up.\\tIss auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4769344 (garborg) & #8227448 (MisterTrouser)',\n",
       " 'Freeze!\\tKeine Bewegung!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2160489 (Dreamk33) & #3155388 (Pfirsichbaeumchen)',\n",
       " 'Freeze!\\tStehenbleiben!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2160489 (Dreamk33) & #3155390 (Pfirsichbaeumchen)',\n",
       " 'Got it?\\tKapiert?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #455353 (FeuDRenais) & #455377 (MUIRIEL)',\n",
       " 'Got it?\\tVerstanden?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #455353 (FeuDRenais) & #455452 (Zaghawa)',\n",
       " 'Got it?\\tEinverstanden?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #455353 (FeuDRenais) & #1808892 (Tamy)',\n",
       " 'He ran.\\tEr rannte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #672229 (CK) & #1401396 (Esperantostern)',\n",
       " 'He ran.\\tEr lief.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #672229 (CK) & #1401397 (Esperantostern)',\n",
       " 'Hop in.\\tMach mit!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1111548 (Scott) & #4659678 (AC)',\n",
       " 'Hug me.\\tDrück mich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1852504 (CK) & #1846595 (Pfirsichbaeumchen)',\n",
       " 'Hug me.\\tNimm mich in den Arm!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1852504 (CK) & #1846597 (Pfirsichbaeumchen)',\n",
       " 'Hug me.\\tUmarme mich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1852504 (CK) & #1854006 (Pfirsichbaeumchen)',\n",
       " 'I fell.\\tIch fiel.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3824714 (CK) & #4942958 (Hans_Adler)',\n",
       " 'I fell.\\tIch fiel hin.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3824714 (CK) & #4942959 (Hans_Adler)',\n",
       " 'I fell.\\tIch stürzte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3824714 (CK) & #4942960 (Hans_Adler)',\n",
       " 'I fell.\\tIch bin hingefallen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3824714 (CK) & #4942961 (Hans_Adler)',\n",
       " 'I fell.\\tIch bin gestürzt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3824714 (CK) & #4942963 (Hans_Adler)',\n",
       " 'I know.\\tIch weiß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #319990 (CK) & #358382 (MUIRIEL)',\n",
       " 'I lied.\\tIch habe gelogen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3935374 (gianich73) & #3235040 (Manfredo)',\n",
       " 'I lost.\\tIch habe verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1755073 (Eldad) & #4659717 (AC)',\n",
       " 'I paid.\\tIch habe bezahlt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828610 (CK) & #7141489 (Luiaard)',\n",
       " 'I paid.\\tIch zahlte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828610 (CK) & #7141490 (Luiaard)',\n",
       " 'I sang.\\tIch sang.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #7290637 (shekitten) & #4781463 (Tamy)',\n",
       " 'I swim.\\tIch schwimme.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828611 (CK) & #7091837 (Luiaard)',\n",
       " \"I'm 19.\\tIch bin 19 Jahre alt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #442400 (sacredceltic) & #345406 (MUIRIEL)\",\n",
       " \"I'm 19.\\tIch bin 19.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #442400 (sacredceltic) & #442436 (MUIRIEL)\",\n",
       " \"I'm OK.\\tMir geht's gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433763 (CK) & #341452 (MUIRIEL)\",\n",
       " \"I'm OK.\\tEs geht mir gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433763 (CK) & #2705321 (Pfirsichbaeumchen)\",\n",
       " \"I'm up.\\tIch bin wach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2765357 (arnxy20) & #1313714 (Pfirsichbaeumchen)\",\n",
       " \"I'm up.\\tIch bin auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2765357 (arnxy20) & #6626001 (Felixjp)\",\n",
       " 'No way!\\tUnmöglich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2175 (CS) & #976 (MUIRIEL)',\n",
       " 'No way!\\tDas kommt nicht in Frage!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2175 (CS) & #444494 (Espi)',\n",
       " 'No way!\\tDas gibt’s doch nicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2175 (CS) & #722128 (Pfirsichbaeumchen)',\n",
       " 'No way!\\tAusgeschlossen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2175 (CS) & #1670006 (Pfirsichbaeumchen)',\n",
       " 'No way!\\tIn keinster Weise!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2175 (CS) & #2344583 (Vortarulo)',\n",
       " 'Really?\\tWirklich?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #373216 (kotobaboke) & #373376 (lilygilder)',\n",
       " 'Really?\\tEcht?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #373216 (kotobaboke) & #808911 (Manfredo)',\n",
       " 'Really?\\tIm Ernst?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #373216 (kotobaboke) & #937379 (Sudajaengi)',\n",
       " 'Thanks.\\tDanke!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2057650 (nava) & #397489 (MUIRIEL)',\n",
       " 'Try it.\\tVersuch’s!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4756252 (cairnhead) & #4756673 (raggione)',\n",
       " 'We try.\\tWir versuchen es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #247763 (CK) & #1300435 (cumori)',\n",
       " 'We won.\\tWir haben gewonnen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107675 (CK) & #2122757 (Pfirsichbaeumchen)',\n",
       " 'Why me?\\tWarum ich?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #24958 (CK) & #397532 (MUIRIEL)',\n",
       " 'Ask Tom.\\tFrag Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954263 (CK) & #1998578 (Tamy)',\n",
       " 'Ask Tom.\\tFragen Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954263 (CK) & #1998581 (Tamy)',\n",
       " 'Ask Tom.\\tFragt Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954263 (CK) & #1998583 (Tamy)',\n",
       " 'Awesome!\\tFantastisch!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433512 (CK) & #366461 (Wolf)',\n",
       " 'Be cool.\\tEntspann dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1860752 (CK) & #1737099 (Espi)',\n",
       " 'Be fair.\\tSei nicht ungerecht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1916316 (CK) & #2144804 (Pfirsichbaeumchen)',\n",
       " 'Be fair.\\tSei fair!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1916316 (CK) & #2270545 (Vortarulo)',\n",
       " 'Be nice.\\tSei nett!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1916314 (CK) & #1998565 (Tamy)',\n",
       " 'Be nice.\\tSeien Sie nett!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1916314 (CK) & #1998568 (Tamy)',\n",
       " 'Beat it.\\tGeh weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #547378 (Espi)',\n",
       " 'Beat it.\\tHau ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #662285 (stefz)',\n",
       " 'Beat it.\\tVerschwinde!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #722834 (Esperantostern)',\n",
       " 'Beat it.\\tVerdufte!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #795719 (BraveSentry)',\n",
       " 'Beat it.\\tMach dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #795721 (BraveSentry)',\n",
       " 'Beat it.\\tZieh Leine!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #917506 (Sudajaengi)',\n",
       " 'Beat it.\\tMach dich vom Acker!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #917507 (Sudajaengi)',\n",
       " 'Beat it.\\tVerzieh dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #917508 (Sudajaengi)',\n",
       " 'Beat it.\\tVerkrümele dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #1203256 (Espi)',\n",
       " 'Beat it.\\tTroll dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2162750 (Vortarulo)',\n",
       " 'Beat it.\\tZisch ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2494154 (Pfirsichbaeumchen)',\n",
       " 'Beat it.\\tPack dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2494156 (Pfirsichbaeumchen)',\n",
       " 'Beat it.\\tMach ’ne Fliege!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2494157 (Pfirsichbaeumchen)',\n",
       " 'Beat it.\\tSchwirr ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2494158 (Pfirsichbaeumchen)',\n",
       " 'Beat it.\\tMach die Sause!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2494159 (Pfirsichbaeumchen)',\n",
       " 'Beat it.\\tScher dich weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2494160 (Pfirsichbaeumchen)',\n",
       " 'Beat it.\\tScher dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37902 (CM) & #2494161 (Pfirsichbaeumchen)',\n",
       " 'Call me.\\tRuf mich an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1553532 (CK) & #1688178 (Pfirsichbaeumchen)',\n",
       " 'Come in.\\tKomm herein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #348091 (Hertz) & #361822 (Wolf)',\n",
       " 'Come in.\\tHerein!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #348091 (Hertz) & #440148 (MUIRIEL)',\n",
       " 'Come on!\\tKomm!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #24697 (CK) & #4787438 (L3581)',\n",
       " 'Come on!\\tKommt!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #24697 (CK) & #4944467 (Hans_Adler)',\n",
       " 'Come on!\\tMach schon!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #24697 (CK) & #5362934 (RandomUsername)',\n",
       " 'Come on!\\tMacht schon!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #24697 (CK) & #5362935 (RandomUsername)',\n",
       " 'Come on.\\tKomm schon!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954266 (CK) & #6643321 (Felixjp)',\n",
       " 'Get Tom.\\tHol Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972633 (CK) & #5227375 (jaz1313)',\n",
       " 'Get out!\\tRaus!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #266790 (CK) & #474497 (Espi)',\n",
       " 'Get out!\\tGeht raus!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #266790 (CK) & #7036913 (driini)',\n",
       " 'Get out.\\tGeh raus.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #22016 (Eldad) & #4944450 (Hans_Adler)',\n",
       " 'Get out.\\tGeht raus!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #22016 (Eldad) & #7036913 (driini)',\n",
       " 'Go away!\\tGeh weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #547378 (Espi)',\n",
       " 'Go away!\\tHau ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #662285 (stefz)',\n",
       " 'Go away!\\tVerschwinde!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #722834 (Esperantostern)',\n",
       " 'Go away!\\tVerdufte!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #795719 (BraveSentry)',\n",
       " 'Go away!\\tMach dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #795721 (BraveSentry)',\n",
       " 'Go away!\\tZieh Leine!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #917506 (Sudajaengi)',\n",
       " 'Go away!\\tMach dich vom Acker!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #917507 (Sudajaengi)',\n",
       " 'Go away!\\tVerzieh dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #917508 (Sudajaengi)',\n",
       " 'Go away!\\tVerkrümele dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #1203256 (Espi)',\n",
       " 'Go away!\\tTroll dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2162750 (Vortarulo)',\n",
       " 'Go away!\\tZisch ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2494154 (Pfirsichbaeumchen)',\n",
       " 'Go away!\\tPack dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2494156 (Pfirsichbaeumchen)',\n",
       " 'Go away!\\tMach ’ne Fliege!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2494157 (Pfirsichbaeumchen)',\n",
       " 'Go away!\\tSchwirr ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2494158 (Pfirsichbaeumchen)',\n",
       " 'Go away!\\tMach die Sause!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2494159 (Pfirsichbaeumchen)',\n",
       " 'Go away!\\tScher dich weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2494160 (Pfirsichbaeumchen)',\n",
       " 'Go away!\\tScher dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433518 (CK) & #2494161 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tGeh weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #547378 (Espi)',\n",
       " 'Go away.\\tVerpiss dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #584844 (MUIRIEL)',\n",
       " 'Go away.\\tHau ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #662285 (stefz)',\n",
       " 'Go away.\\tVerschwinde!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #722834 (Esperantostern)',\n",
       " 'Go away.\\tVerdufte!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #795719 (BraveSentry)',\n",
       " 'Go away.\\tMach dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #795721 (BraveSentry)',\n",
       " 'Go away.\\tZieh Leine!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #917506 (Sudajaengi)',\n",
       " 'Go away.\\tMach dich vom Acker!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #917507 (Sudajaengi)',\n",
       " 'Go away.\\tVerzieh dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #917508 (Sudajaengi)',\n",
       " 'Go away.\\tVerkrümele dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #1203256 (Espi)',\n",
       " 'Go away.\\tTroll dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2162750 (Vortarulo)',\n",
       " 'Go away.\\tZisch ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2494154 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tPack dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2494156 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tMach ’ne Fliege!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2494157 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tSchwirr ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2494158 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tMach die Sause!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2494159 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tScher dich weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2494160 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tScher dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #2494161 (Pfirsichbaeumchen)',\n",
       " 'Go away.\\tGehen Sie weg.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433491 (CK) & #4942536 (Hans_Adler)',\n",
       " 'Go home.\\tGeh nach Hause.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2230769 (CK) & #6644930 (Felixjp)',\n",
       " 'Go home.\\tGeh heim.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2230769 (CK) & #6644931 (Felixjp)',\n",
       " 'Goodbye!\\tAuf Wiedersehen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #516745 (minshirui) & #409153 (MUIRIEL)',\n",
       " 'Goodbye!\\tLeb wohl!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #516745 (minshirui) & #438753 (xtofu80)',\n",
       " 'Goodbye!\\tTschüss!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #516745 (minshirui) & #647453 (Hans07)',\n",
       " 'Hang on!\\tNicht nachlassen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1894404 (Spamster) & #1921063 (Tamy)',\n",
       " 'He came.\\tEr kam.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #853153 (piksea) & #867797 (Espi)',\n",
       " 'He runs.\\tEr rennt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #672254 (CK) & #672259 (Zaghawa)',\n",
       " 'He runs.\\tEr läuft.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #672254 (CK) & #5687977 (eric2)',\n",
       " 'Help me.\\tHilf mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #266065 (Zifre) & #368205 (Wolf)',\n",
       " 'Help us.\\tHilf uns!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2187193 (CK) & #2199031 (Pfirsichbaeumchen)',\n",
       " 'Help us.\\tHelft uns!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2187193 (CK) & #2199032 (Pfirsichbaeumchen)',\n",
       " 'Help us.\\tHelfen Sie uns!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2187193 (CK) & #2199033 (Pfirsichbaeumchen)',\n",
       " 'Hi, Tom.\\tHallo, Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549834 (CK) & #3298512 (Manfredo)',\n",
       " 'Hit Tom.\\tSchlage Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203859 (CK) & #2203962 (Pfirsichbaeumchen)',\n",
       " 'Hit Tom.\\tSchlagt Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203859 (CK) & #2203963 (Pfirsichbaeumchen)',\n",
       " 'Hit Tom.\\tSchlagen Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203859 (CK) & #2203964 (Pfirsichbaeumchen)',\n",
       " 'Hold on.\\tWarten Sie kurz!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1357000 (sacredceltic) & #2116085 (Pfirsichbaeumchen)',\n",
       " 'Hug Tom.\\tUmarme Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203860 (CK) & #2203965 (Pfirsichbaeumchen)',\n",
       " 'Hug Tom.\\tUmarmen Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203860 (CK) & #2203967 (Pfirsichbaeumchen)',\n",
       " 'Hug Tom.\\tUmarmt Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203860 (CK) & #2203968 (Pfirsichbaeumchen)',\n",
       " 'Hug Tom.\\tDrückt Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203860 (CK) & #2203969 (Pfirsichbaeumchen)',\n",
       " 'Hug Tom.\\tDrücken Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203860 (CK) & #2203970 (Pfirsichbaeumchen)',\n",
       " 'Hug Tom.\\tDrück Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203860 (CK) & #2203971 (Pfirsichbaeumchen)',\n",
       " 'I agree.\\tIch bin einverstanden.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #436281 (lukaszpp) & #472 (MUIRIEL)',\n",
       " 'I cried.\\tIch weinte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828613 (CK) & #7091838 (Luiaard)',\n",
       " 'I cried.\\tIch habe geweint.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828613 (CK) & #7091839 (Luiaard)',\n",
       " 'I snore.\\tIch schnarche.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828618 (CK) & #7091840 (Luiaard)',\n",
       " \"I'll go.\\tIch gehe.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1174876 (CK) & #439366 (Robroy)\",\n",
       " \"I'm bad.\\tIch bin schlecht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828622 (CK) & #8191860 (driini)\",\n",
       " \"I'm fat.\\tIch bin fett.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1120816 (cntrational) & #1137134 (Esperantostern)\",\n",
       " \"I'm fat.\\tIch bin dick.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1120816 (cntrational) & #1187988 (Tlustulimu)\",\n",
       " \"I'm hit!\\tIch wurde getroffen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1829472 (Spamster) & #7369708 (Yorwba)\",\n",
       " \"I'm new.\\tIch bin neu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828624 (CK) & #6644699 (Felixjp)\",\n",
       " \"I'm old.\\tIch bin alt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1924560 (Zaghawa) & #1703743 (Esperantostern)\",\n",
       " \"I'm sad.\\tIch bin traurig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #317741 (CK) & #345365 (MUIRIEL)\",\n",
       " \"I'm shy.\\tIch bin schüchtern.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203346 (CK) & #2655326 (Pfirsichbaeumchen)\",\n",
       " \"I'm wet.\\tIch bin nass.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1780075 (Spamster) & #7354609 (Yorwba)\",\n",
       " \"It's OK.\\tEs ist in Ordnung.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #413756 (CK) & #6626109 (Felixjp)\",\n",
       " \"It's me!\\tIch bin's.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1349952 (sacredceltic) & #356299 (MUIRIEL)\",\n",
       " \"It's me.\\tIch bin's.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #42400 (sysko) & #356299 (MUIRIEL)\",\n",
       " 'Keep it.\\tBehalt es!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211336 (raggione)',\n",
       " 'Keep it.\\tBehalt ihn!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211337 (raggione)',\n",
       " 'Keep it.\\tBehalt sie!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211339 (raggione)',\n",
       " 'Keep it.\\tBehalten Sie ihn!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211451 (raggione)',\n",
       " 'Keep it.\\tBehalten Sie sie!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211453 (raggione)',\n",
       " 'Keep it.\\tBehalten Sie es!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211454 (raggione)',\n",
       " 'Keep it.\\tBehaltet ihn!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211455 (raggione)',\n",
       " 'Keep it.\\tBehaltet sie!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211456 (raggione)',\n",
       " 'Keep it.\\tBehaltet es!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913087 (CK) & #8211457 (raggione)',\n",
       " 'Kiss me.\\tKüsst mich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #565931 (Fish) & #851631 (MUIRIEL)',\n",
       " 'Lock it.\\tSchließ es ab.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5350433 (mailohilohi) & #6138659 (raggione)',\n",
       " 'Lock it.\\tSchließe sie ab.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5350433 (mailohilohi) & #6138660 (raggione)',\n",
       " 'Lock it.\\tSchließ ihn ab.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5350433 (mailohilohi) & #6138661 (raggione)',\n",
       " 'Perfect!\\tPerfekt!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #374568 (blay_paul) & #4659630 (AC)',\n",
       " 'Pull it.\\tZieh dran.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6049384 (mailohilohi) & #8227464 (MisterTrouser)',\n",
       " 'Push it.\\tDrück drauf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6049385 (mailohilohi) & #8227465 (MisterTrouser)',\n",
       " 'See you.\\tWir sehen uns.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1151432 (Roggy) & #2073170 (Espi)',\n",
       " \"Show me.\\tZeig's mir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972649 (CK) & #1973162 (Vortarulo)\",\n",
       " \"Shut up!\\tHalt's Maul!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #462430 (lukaszpp) & #533020 (Espi)\",\n",
       " 'So long.\\tBis später!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2267731 (_undertoad) & #367027 (MUIRIEL)',\n",
       " 'Stop it.\\tHör auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111499 (CK) & #6644774 (Felixjp)',\n",
       " 'Take it.\\tNimm es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913089 (CK) & #4732110 (bonny37)',\n",
       " 'Take it.\\tNehmt es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913089 (CK) & #6023199 (Zaghawa)',\n",
       " 'Take it.\\tNehmen Sie es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913089 (CK) & #6023200 (Zaghawa)',\n",
       " 'Tom ate.\\tTom aß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203599 (CK) & #2205386 (Pfirsichbaeumchen)',\n",
       " 'Tom ate.\\tTom hat gegessen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203599 (CK) & #2205387 (Pfirsichbaeumchen)',\n",
       " 'Tom ran.\\tTom rannte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203766 (CK) & #2204129 (Pfirsichbaeumchen)',\n",
       " 'Tom ran.\\tTom ist gerannt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203766 (CK) & #2204130 (Pfirsichbaeumchen)',\n",
       " 'Tom won.\\tTom hat gewonnen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005191 (CK) & #2153027 (Pfirsichbaeumchen)',\n",
       " 'Wait up.\\tWarte mal!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2954108 (CK) & #2095332 (Pfirsichbaeumchen)',\n",
       " 'Wait up.\\tWarten Sie mal!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2954108 (CK) & #2954330 (Tamy)',\n",
       " 'Wait up.\\tWartet mal!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2954108 (CK) & #2954332 (Tamy)',\n",
       " 'Wake up!\\tWach auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #323780 (CK) & #355345 (MUIRIEL)',\n",
       " 'Wake up!\\tWachen Sie auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #323780 (CK) & #1851545 (Pfirsichbaeumchen)',\n",
       " 'Wake up.\\tWach auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1850243 (CK) & #355345 (MUIRIEL)',\n",
       " 'Wake up.\\tWachen Sie auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1850243 (CK) & #1851545 (Pfirsichbaeumchen)',\n",
       " 'We lost.\\tWir haben verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107666 (CK) & #2122765 (Pfirsichbaeumchen)',\n",
       " 'Welcome.\\tWillkommen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #138919 (CM) & #362772 (Kerstin)',\n",
       " 'Who ate?\\tWer hat gegessen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203598 (CK) & #712317 (conker)',\n",
       " 'Who ate?\\tWer aß?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203598 (CK) & #2205383 (Pfirsichbaeumchen)',\n",
       " 'Who ran?\\tWer rannte?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203765 (CK) & #2204127 (Pfirsichbaeumchen)',\n",
       " 'Who ran?\\tWer ist gerannt?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203765 (CK) & #2204128 (Pfirsichbaeumchen)',\n",
       " 'You run.\\tDu läufst.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #672252 (CK) & #4732107 (bonny37)',\n",
       " 'You run.\\tSie laufen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #672252 (CK) & #4732108 (bonny37)',\n",
       " 'You won.\\tDu hast gewonnen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549833 (CK) & #3481047 (Esperantostern)',\n",
       " 'Am I fat?\\tBin ich dick?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2214296 (Hybrid) & #2214362 (Pfirsichbaeumchen)',\n",
       " 'Ask them.\\tFrag sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3820042 (CK) & #4942946 (Hans_Adler)',\n",
       " 'Back off.\\tKomm nicht näher!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1553533 (CK) & #1846342 (Tamy)',\n",
       " 'Be a man.\\tSei ein Mann!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2221008 (Hybrid) & #2221017 (Pfirsichbaeumchen)',\n",
       " 'Be brave.\\tSei tapfer!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4054941 (CK) & #4057729 (Pfirsichbaeumchen)',\n",
       " 'Be brave.\\tSeien Sie tapfer!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4054941 (CK) & #4057731 (Pfirsichbaeumchen)',\n",
       " 'Be brave.\\tSeid tapfer!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4054941 (CK) & #4057732 (Pfirsichbaeumchen)',\n",
       " 'Be brief.\\tFassen Sie sich kurz.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3826022 (CK) & #4942973 (Hans_Adler)',\n",
       " 'Be brief.\\tFass dich kurz.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3826022 (CK) & #4942974 (Hans_Adler)',\n",
       " 'Be brief.\\tFasst euch kurz.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3826022 (CK) & #4942975 (Hans_Adler)',\n",
       " 'Call Tom.\\tRuf Tom an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203855 (CK) & #2203997 (Pfirsichbaeumchen)',\n",
       " 'Call Tom.\\tRufe Tom an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203855 (CK) & #2203998 (Pfirsichbaeumchen)',\n",
       " 'Call Tom.\\tRufen Sie Tom an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203855 (CK) & #2203999 (Pfirsichbaeumchen)',\n",
       " 'Call Tom.\\tRuft Tom an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203855 (CK) & #2204000 (Pfirsichbaeumchen)',\n",
       " 'Can I go?\\tKann ich gehen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5825527 (CK) & #7789053 (raggione)',\n",
       " 'Cheer up!\\tKopf hoch!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #239441 (CM) & #342091 (lilygilder)',\n",
       " 'Cool off!\\tReg dich ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #280162 (CM) & #446596 (al_ex_an_der)',\n",
       " 'Cuff him.\\tLeg ihm Handschellen an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954267 (CK) & #4941575 (Hans_Adler)',\n",
       " 'Cuff him.\\tLegen Sie ihm Handschellen an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954267 (CK) & #4941576 (Hans_Adler)',\n",
       " \"Don't go.\\tGeh nicht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #773587 (marloncori) & #782815 (Chris)\",\n",
       " 'Find Tom.\\tFinde Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111838 (CK) & #4942841 (Hans_Adler)',\n",
       " 'Find Tom.\\tFindet Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111838 (CK) & #4942842 (Hans_Adler)',\n",
       " 'Find Tom.\\tFinden Sie Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111838 (CK) & #4942843 (Hans_Adler)',\n",
       " 'Fix this.\\tBeheben Sie das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3172185 (CK) & #7281107 (Yorwba)',\n",
       " 'Fix this.\\tBehebe das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3172185 (CK) & #7281109 (Yorwba)',\n",
       " 'Fix this.\\tRepariere das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3172185 (CK) & #7281111 (Yorwba)',\n",
       " 'Fix this.\\tReparieren Sie das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3172185 (CK) & #7281114 (Yorwba)',\n",
       " 'Get away!\\tGeh weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #547378 (Espi)',\n",
       " 'Get away!\\tVerpiss dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #584844 (MUIRIEL)',\n",
       " 'Get away!\\tHau ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #662285 (stefz)',\n",
       " 'Get away!\\tVerschwinde!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #722834 (Esperantostern)',\n",
       " 'Get away!\\tVerdufte!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #795719 (BraveSentry)',\n",
       " 'Get away!\\tMach dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #795721 (BraveSentry)',\n",
       " 'Get away!\\tZieh Leine!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #917506 (Sudajaengi)',\n",
       " 'Get away!\\tMach dich vom Acker!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #917507 (Sudajaengi)',\n",
       " 'Get away!\\tVerzieh dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #917508 (Sudajaengi)',\n",
       " 'Get away!\\tVerkrümele dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #1203256 (Espi)',\n",
       " 'Get away!\\tTroll dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2162750 (Vortarulo)',\n",
       " 'Get away!\\tZisch ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2494154 (Pfirsichbaeumchen)',\n",
       " 'Get away!\\tPack dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2494156 (Pfirsichbaeumchen)',\n",
       " 'Get away!\\tMach ’ne Fliege!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2494157 (Pfirsichbaeumchen)',\n",
       " 'Get away!\\tSchwirr ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2494158 (Pfirsichbaeumchen)',\n",
       " 'Get away!\\tMach die Sause!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2494159 (Pfirsichbaeumchen)',\n",
       " 'Get away!\\tScher dich weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2494160 (Pfirsichbaeumchen)',\n",
       " 'Get away!\\tScher dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240634 (CM) & #2494161 (Pfirsichbaeumchen)',\n",
       " 'Get down!\\tRunter!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #268087 (CM) & #6643143 (Felixjp)',\n",
       " 'Get down!\\tHinlegen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #268087 (CM) & #6644792 (Felixjp)',\n",
       " 'Get down!\\tIn Deckung!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #268087 (CM) & #6681267 (Felixjp)',\n",
       " 'Get down.\\tKomm runter.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954269 (CK) & #4941578 (Hans_Adler)',\n",
       " 'Get down.\\tKommen Sie runter.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954269 (CK) & #4941580 (Hans_Adler)',\n",
       " 'Get lost!\\tGeh weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #547378 (Espi)',\n",
       " 'Get lost!\\tVerpiss dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #584844 (MUIRIEL)',\n",
       " 'Get lost!\\tHau ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #662285 (stefz)',\n",
       " 'Get lost!\\tVerschwinde!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #722834 (Esperantostern)',\n",
       " 'Get lost!\\tVerdufte!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #795719 (BraveSentry)',\n",
       " 'Get lost!\\tMach dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #795721 (BraveSentry)',\n",
       " 'Get lost!\\tZieh Leine!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #917506 (Sudajaengi)',\n",
       " 'Get lost!\\tMach dich vom Acker!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #917507 (Sudajaengi)',\n",
       " 'Get lost!\\tVerzieh dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #917508 (Sudajaengi)',\n",
       " 'Get lost!\\tVerkrümele dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #1203256 (Espi)',\n",
       " 'Get lost!\\tTroll dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2162750 (Vortarulo)',\n",
       " 'Get lost!\\tZisch ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2494154 (Pfirsichbaeumchen)',\n",
       " 'Get lost!\\tPack dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2494156 (Pfirsichbaeumchen)',\n",
       " 'Get lost!\\tMach ’ne Fliege!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2494157 (Pfirsichbaeumchen)',\n",
       " 'Get lost!\\tSchwirr ab!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2494158 (Pfirsichbaeumchen)',\n",
       " 'Get lost!\\tMach die Sause!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2494159 (Pfirsichbaeumchen)',\n",
       " 'Get lost!\\tScher dich weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2494160 (Pfirsichbaeumchen)',\n",
       " 'Get lost!\\tScher dich fort!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #623565 (Dorenda) & #2494161 (Pfirsichbaeumchen)',\n",
       " 'Get real!\\tJetzt mal ernsthaft!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #326067 (CM) & #4659664 (AC)',\n",
       " 'Go ahead!\\tNur zu!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #38590 (CM) & #3331180 (brauchinet)',\n",
       " 'Go ahead.\\tMach weiter!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #38546 (CM) & #1068331 (Hans07)',\n",
       " 'Grab Tom.\\tHol Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972639 (CK) & #5227375 (jaz1313)',\n",
       " 'Grab him.\\tGreif ihn dir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #285184 (CK) & #4659653 (AC)',\n",
       " 'Have fun.\\tViel Vergnügen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #21267 (CK) & #466678 (Espi)',\n",
       " 'Have fun.\\tViel Spaß!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #21267 (CK) & #521359 (Espi)',\n",
       " 'Have fun.\\tFeier schön!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #21267 (CK) & #1079989 (al_ex_an_der)',\n",
       " 'He spoke.\\tEr sprach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1564976 (marcelostockle) & #772231 (samueldora)',\n",
       " 'He tries.\\tEr versucht es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #288166 (CM) & #413525 (MUIRIEL)',\n",
       " 'Help Tom.\\tHilf Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203858 (CK) & #2203959 (Pfirsichbaeumchen)',\n",
       " 'Help Tom.\\tHelft Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203858 (CK) & #2203960 (Pfirsichbaeumchen)',\n",
       " 'Help Tom.\\tHelfen Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203858 (CK) & #2203961 (Pfirsichbaeumchen)',\n",
       " 'How cute!\\tWas ist das nicht süß!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #36097 (CM) & #332133 (SeeVogel)',\n",
       " 'How cute!\\tWie süß!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #36097 (CM) & #356164 (MUIRIEL)',\n",
       " 'How deep?\\tWie tief?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #37612 (CM) & #4659638 (AC)',\n",
       " 'How nice!\\tWie schön!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1913091 (CK) & #3943128 (Kuraimegami)',\n",
       " 'Humor me.\\tTu mir den Gefallen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954271 (CM) & #4941581 (Hans_Adler)',\n",
       " 'Humor me.\\tTun Sie mir den Gefallen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954271 (CM) & #4941582 (Hans_Adler)',\n",
       " 'Hurry up.\\tBeeil dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1329 (brauliobezerra) & #131 (MUIRIEL)',\n",
       " 'Hurry up.\\tBeeil dich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1329 (brauliobezerra) & #355342 (MUIRIEL)',\n",
       " 'Hurry up.\\tMach hin!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1329 (brauliobezerra) & #1367862 (al_ex_an_der)',\n",
       " 'I am fat.\\tIch bin dick.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2523975 (CM) & #1187988 (Tlustulimu)',\n",
       " 'I can go.\\tIch kann gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3092548 (CK) & #4521701 (Emtyra)',\n",
       " 'I did it.\\tIch habe es geschafft.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549832 (CK) & #1223697 (BraveSentry)',\n",
       " \"I did it.\\tIch hab's gemacht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549832 (CK) & #2981343 (pne)\",\n",
       " 'I get by.\\tIch komme klar.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2476830 (arnxy20) & #2800938 (freddy1)',\n",
       " 'I get by.\\tIch komme zurecht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2476830 (arnxy20) & #6644236 (Felixjp)',\n",
       " 'I get by.\\tIch komme über die Runden.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2476830 (arnxy20) & #6644244 (Felixjp)',\n",
       " 'I get it.\\tIch verstehe.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1691393 (aka_aj) & #547386 (Espi)',\n",
       " 'I got it.\\tIch habe es verstanden.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433472 (CK) & #1313701 (al_ex_an_der)',\n",
       " 'I got it.\\tIch habe es bekommen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433472 (CK) & #1313702 (al_ex_an_der)',\n",
       " \"I got it.\\tIch hab's.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433472 (CK) & #3128448 (samueldora)\",\n",
       " 'I helped.\\tIch half.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828630 (CK) & #7091842 (Luiaard)',\n",
       " 'I helped.\\tIch habe geholfen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828630 (CK) & #7091843 (Luiaard)',\n",
       " 'I jumped.\\tIch bin gesprungen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828631 (CK) & #4098517 (Esperantostern)',\n",
       " 'I refuse.\\tIch weigere mich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3930181 (Youmu970) & #4942976 (Hans_Adler)',\n",
       " 'I resign.\\tIch trete zurück.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #731590 (Eldad) & #731476 (jakov)',\n",
       " 'I shaved.\\tIch rasierte mich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828638 (CK) & #4785325 (Tamy)',\n",
       " 'I shaved.\\tIch habe mich rasiert.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828638 (CK) & #7141491 (Luiaard)',\n",
       " 'I smiled.\\tIch lächelte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3651941 (Luciosp) & #4942940 (Hans_Adler)',\n",
       " 'I stayed.\\tIch blieb.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123630 (CK) & #2139195 (Pfirsichbaeumchen)',\n",
       " 'I stayed.\\tIch bin dageblieben.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123630 (CK) & #5606600 (Vortarulo)',\n",
       " 'I use it.\\tIch benutze es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #254813 (CK) & #1401401 (Esperantostern)',\n",
       " 'I waited.\\tIch habe gewartet.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123628 (CK) & #2139192 (Pfirsichbaeumchen)',\n",
       " 'I yawned.\\tIch gähnte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828642 (CK) & #2202301 (Esperantostern)',\n",
       " 'I yawned.\\tIch habe gegähnt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828642 (CK) & #4844863 (Tamy)',\n",
       " \"I'll pay.\\tIch werde zahlen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #411285 (CK) & #6155050 (arved)\",\n",
       " \"I'll try.\\tIch werde es versuchen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111768 (CK) & #643552 (Pfirsichbaeumchen)\",\n",
       " \"I'm back.\\tIch bin wieder da.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #564159 (darinmex) & #817356 (Manfredo)\",\n",
       " \"I'm bald.\\tIch bin glatzköpfig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3330667 (CK) & #2293098 (al_ex_an_der)\",\n",
       " \"I'm bald.\\tIch habe eine Glatze.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3330667 (CK) & #2293099 (al_ex_an_der)\",\n",
       " \"I'm busy.\\tIch bin beschäftigt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #261721 (CK) & #351599 (MUIRIEL)\",\n",
       " \"I'm busy.\\tIch habe zu tun.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #261721 (CK) & #2624978 (Pfirsichbaeumchen)\",\n",
       " \"I'm deaf.\\tIch bin taub.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828646 (CK) & #2730977 (Pfirsichbaeumchen)\",\n",
       " \"I'm fair.\\tIch bin gerecht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2202832 (CK) & #7374179 (Yorwba)\",\n",
       " \"I'm fine.\\tMir geht's gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #257272 (Eldad) & #341452 (MUIRIEL)\",\n",
       " \"I'm fine.\\tMir geht es gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #257272 (Eldad) & #659407 (Esperantostern)\",\n",
       " \"I'm fine.\\tEs geht mir gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #257272 (Eldad) & #2705321 (Pfirsichbaeumchen)\",\n",
       " \"I'm free.\\tIch bin frei.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #23959 (CK) & #340890 (MUIRIEL)\",\n",
       " \"I'm full.\\tIch bin satt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433863 (CK) & #547397 (Espi)\",\n",
       " \"I'm game.\\tIch bin dabei.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #835972 (CM) & #835978 (Pfirsichbaeumchen)\",\n",
       " \"I'm game.\\tIch mache mit.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #835972 (CM) & #835979 (Pfirsichbaeumchen)\",\n",
       " \"I'm good.\\tMir geht's gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828648 (CK) & #341452 (MUIRIEL)\",\n",
       " \"I'm here.\\tIch bin hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111726 (CK) & #2323643 (pne)\",\n",
       " \"I'm home.\\tIch bin zu Hause.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111725 (CK) & #375496 (MUIRIEL)\",\n",
       " \"I'm late.\\tIch komme zu spät.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123622 (CK) & #2139207 (Pfirsichbaeumchen)\",\n",
       " \"I'm lost.\\tIch habe mich verirrt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #455833 (lukaszpp) & #785454 (Pfirsichbaeumchen)\",\n",
       " \"I'm mean.\\tIch bin gemein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203077 (CK) & #7451361 (Yorwba)\",\n",
       " \"I'm next.\\tIch bin der Nächste.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3171892 (CK) & #6546704 (Espi)\",\n",
       " \"I'm okay.\\tEs geht mir gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2283729 (CK) & #2705321 (Pfirsichbaeumchen)\",\n",
       " \"I'm rich.\\tIch bin reich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203289 (CK) & #2067087 (al_ex_an_der)\",\n",
       " \"I'm safe.\\tIch bin sicher.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203309 (CK) & #399166 (MUIRIEL)\",\n",
       " \"I'm sick.\\tIch bin krank!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #780304 (vgigregg) & #448234 (Lars224)\",\n",
       " \"I'm sure.\\tIch bin sicher.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2283730 (CK) & #399166 (MUIRIEL)\",\n",
       " \"I'm tall.\\tIch bin groß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2764439 (CK) & #576296 (Pfirsichbaeumchen)\",\n",
       " \"I'm thin.\\tIch bin dünn.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203445 (CK) & #7413423 (Yorwba)\",\n",
       " \"I'm tidy.\\tIch bin ordentlich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203456 (CK) & #7413435 (Yorwba)\",\n",
       " \"I'm ugly.\\tIch bin hässlich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2428556 (CK) & #2182960 (Esperantostern)\",\n",
       " \"I'm weak.\\tIch bin schwach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203572 (CK) & #2205477 (Pfirsichbaeumchen)\",\n",
       " \"I'm well.\\tMir geht es gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4207033 (stevegrant) & #659407 (Esperantostern)\",\n",
       " 'It helps.\\tDas hilft.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123616 (CK) & #2139206 (Pfirsichbaeumchen)',\n",
       " 'It hurts.\\tEs tut weh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649251 (CK) & #4942926 (Hans_Adler)',\n",
       " 'It hurts.\\tEs schmerzt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649251 (CK) & #4942927 (Hans_Adler)',\n",
       " 'It works.\\tEs klappt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1365269 (sacredceltic) & #4514429 (raggione)',\n",
       " 'It works.\\tEs funktioniert.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1365269 (sacredceltic) & #4949724 (raggione)',\n",
       " \"It's Tom.\\tEs ist Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123573 (CK) & #6626099 (Felixjp)\",\n",
       " \"It's his.\\tEs ist seins.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2187229 (CK) & #6626100 (Felixjp)\",\n",
       " \"It's hot.\\tEs ist heiß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #423405 (cristina_fulginiti) & #438749 (xtofu80)\",\n",
       " \"It's hot.\\tSie ist heiß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #423405 (cristina_fulginiti) & #8193006 (raggione)\",\n",
       " \"It's hot.\\tEr ist heiß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #423405 (cristina_fulginiti) & #8193009 (raggione)\",\n",
       " \"It's new.\\tEs ist neu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #42304 (CK) & #1313704 (al_ex_an_der)\",\n",
       " \"It's sad.\\tEs ist traurig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2515077 (CK) & #3459049 (Zaghawa)\",\n",
       " 'Keep out!\\tEintritt verboten!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #325436 (CK) & #842034 (Espi)',\n",
       " 'Keep out.\\tKomm nicht herein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #325435 (CK) & #469904 (Espi)',\n",
       " 'Keep out.\\tKein Zutritt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #325435 (CK) & #1313706 (al_ex_an_der)',\n",
       " 'Kiss Tom.\\tKüsse Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203863 (CK) & #2203972 (Pfirsichbaeumchen)',\n",
       " 'Kiss Tom.\\tKüssen Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203863 (CK) & #2203973 (Pfirsichbaeumchen)',\n",
       " 'Kiss Tom.\\tKüsst Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203863 (CK) & #2203974 (Pfirsichbaeumchen)',\n",
       " 'Leave us.\\tGeh weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954274 (CK) & #547378 (Espi)',\n",
       " 'Leave us.\\tGehen Sie weg.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954274 (CK) & #4942536 (Hans_Adler)',\n",
       " 'Leave us.\\tLass uns allein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954274 (CK) & #4942538 (Hans_Adler)',\n",
       " 'Leave us.\\tLassen Sie uns allein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954274 (CK) & #4942539 (Hans_Adler)',\n",
       " \"Let's go!\\tLass uns gehen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241077 (CK) & #370055 (xtofu80)\",\n",
       " \"Let's go!\\tAuf geht's!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241077 (CK) & #975131 (Sudajaengi)\",\n",
       " \"Let's go!\\tGehen wir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241077 (CK) & #1225381 (al_ex_an_der)\",\n",
       " \"Let's go!\\tLasst uns gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241077 (CK) & #1324269 (Pfirsichbaeumchen)\",\n",
       " \"Let's go!\\tLass uns losgehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241077 (CK) & #1324270 (Pfirsichbaeumchen)\",\n",
       " \"Let's go!\\tLasst uns losgehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241077 (CK) & #1324271 (Pfirsichbaeumchen)\",\n",
       " \"Let's go!\\tAuf, auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #241077 (CK) & #2456755 (Pfirsichbaeumchen)\",\n",
       " 'Look out!\\tVorsicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20719 (CK) & #834162 (Esperantostern)',\n",
       " 'Marry me.\\tHeirate mich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954275 (CK) & #4732111 (bonny37)',\n",
       " 'May I go?\\tDarf ich gehen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2163138 (Source_VOA) & #879228 (Esperantostern)',\n",
       " 'Save Tom.\\tRette Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203865 (CK) & #2203978 (Pfirsichbaeumchen)',\n",
       " 'Save Tom.\\tRettet Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203865 (CK) & #2203979 (Pfirsichbaeumchen)',\n",
       " 'Save Tom.\\tRetten Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203865 (CK) & #2203980 (Pfirsichbaeumchen)',\n",
       " 'She came.\\tSie kam.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #853152 (piksea) & #867796 (Espi)',\n",
       " 'She lied.\\tSie log.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2774739 (CS) & #3939890 (Giulio)',\n",
       " 'She runs.\\tSie rennt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #672264 (CM) & #672265 (Zaghawa)',\n",
       " 'Sit down!\\tSetz dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1349166 (sacredceltic) & #1174265 (MUIRIEL)',\n",
       " 'Sit down!\\tSetzen Sie sich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1349166 (sacredceltic) & #1791537 (Pfirsichbaeumchen)',\n",
       " 'Sit down.\\tSetz dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1553397 (CK) & #1174265 (MUIRIEL)',\n",
       " 'Speak up!\\tSprich lauter!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #35175 (CK) & #500393 (Espi)',\n",
       " 'Stand up!\\tStehen Sie auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #325429 (CK) & #925574 (Sudajaengi)',\n",
       " 'Stand up!\\tSteht auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #325429 (CK) & #1313708 (al_ex_an_der)',\n",
       " 'Stand up!\\tStehe auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #325429 (CK) & #1923381 (Pfirsichbaeumchen)',\n",
       " 'Stop Tom.\\tHalte Tom auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203866 (CK) & #2203981 (Pfirsichbaeumchen)',\n",
       " 'Stop Tom.\\tHalten Sie Tom auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203866 (CK) & #2203982 (Pfirsichbaeumchen)',\n",
       " 'Stop Tom.\\tHaltet Tom auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203866 (CK) & #2203983 (Pfirsichbaeumchen)',\n",
       " 'Take Tom.\\tNimm Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111478 (CK) & #4734011 (bonny37)',\n",
       " 'Taste it.\\tProbier es mal.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6890476 (Eccles17) & #8227472 (MisterTrouser)',\n",
       " 'Tell Tom.\\tSag es Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111475 (CK) & #4942629 (Hans_Adler)',\n",
       " 'Tell Tom.\\tSagen Sie es Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111475 (CK) & #4942630 (Hans_Adler)',\n",
       " 'Tell Tom.\\tErzähl es Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111475 (CK) & #4942632 (Hans_Adler)',\n",
       " 'Tell Tom.\\tErzählen Sie Tom davon.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111475 (CK) & #4942633 (Hans_Adler)',\n",
       " 'Tell Tom.\\tSagt es Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111475 (CK) & #4942637 (Hans_Adler)',\n",
       " 'Terrific!\\tHervorragend!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52022 (Zifre) & #5362915 (RandomUsername)',\n",
       " 'Terrific!\\tSagenhaft!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52022 (Zifre) & #5362916 (RandomUsername)',\n",
       " 'Terrific!\\tWunderbar!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52022 (Zifre) & #5362918 (RandomUsername)',\n",
       " 'They won.\\tSie haben gewonnen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1618734 (CK) & #4659697 (AC)',\n",
       " 'Tom came.\\tTom kam.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203606 (CK) & #2205389 (Pfirsichbaeumchen)',\n",
       " 'Tom came.\\tTom ist gekommen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203606 (CK) & #2205390 (Pfirsichbaeumchen)',\n",
       " 'Tom died.\\tTom ist gestorben.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005188 (CK) & #2122770 (Pfirsichbaeumchen)',\n",
       " 'Tom died.\\tTom starb.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005188 (CK) & #5830114 (Pfirsichbaeumchen)',\n",
       " 'Tom fell.\\tTom fiel.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203679 (CK) & #2204244 (Pfirsichbaeumchen)',\n",
       " 'Tom fell.\\tTom ist gefallen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203679 (CK) & #2204245 (Pfirsichbaeumchen)',\n",
       " 'Tom knew.\\tTom wusste es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203721 (CK) & #2204189 (Pfirsichbaeumchen)',\n",
       " 'Tom knew.\\tTom hat es gewusst.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203721 (CK) & #2204190 (Pfirsichbaeumchen)',\n",
       " 'Tom knew.\\tTom wusste Bescheid.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203721 (CK) & #3879104 (raggione)',\n",
       " 'Tom left.\\tTom ist gegangen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005178 (CK) & #3768754 (Zaghawa)',\n",
       " 'Tom left.\\tTom ging.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005178 (CK) & #5830112 (Pfirsichbaeumchen)',\n",
       " 'Tom lied.\\tTom log.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005179 (CK) & #4705997 (Esperantostern)',\n",
       " 'Tom lies.\\tTom lügt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203722 (CK) & #1662964 (Pfirsichbaeumchen)',\n",
       " 'Tom lost.\\tTom hat verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005180 (CK) & #4378249 (pullnosemans)',\n",
       " 'Tom paid.\\tTom hat gezahlt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111155 (CK) & #4378251 (pullnosemans)',\n",
       " 'Tom quit.\\tTom hat aufgehört.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005183 (CK) & #5830113 (Pfirsichbaeumchen)',\n",
       " 'Tom swam.\\tTom schwamm.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203845 (CK) & #2204021 (Pfirsichbaeumchen)',\n",
       " 'Tom swam.\\tTom ist geschwommen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203845 (CK) & #2204022 (Pfirsichbaeumchen)',\n",
       " 'Tom wept.\\tTom weinte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1524763 (Spamster) & #1692967 (Pfirsichbaeumchen)',\n",
       " \"Tom's up.\\tTom ist auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107644 (CK) & #6625513 (Felixjp)\",\n",
       " 'Too late.\\tZu spät.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1875 (Swift) & #687 (MUIRIEL)',\n",
       " 'Touch it.\\tFass es an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6049392 (mailohilohi) & #8227466 (MisterTrouser)',\n",
       " 'Trust me.\\tVertraue mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #954991 (FeuDRenais) & #975631 (Esperantostern)',\n",
       " 'Trust me.\\tVertraut mir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #954991 (FeuDRenais) & #1527201 (Pfirsichbaeumchen)',\n",
       " 'Trust me.\\tVertrauen Sie mir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #954991 (FeuDRenais) & #1527202 (Pfirsichbaeumchen)',\n",
       " 'Try hard.\\tVersuch es richtig!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #271901 (Zifre) & #4659651 (AC)',\n",
       " 'Use this.\\tNimm das hier!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954278 (CK) & #2108804 (Pfirsichbaeumchen)',\n",
       " 'Warn Tom.\\tWarnen Sie Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954279 (CK) & #4941624 (Hans_Adler)',\n",
       " 'Warn Tom.\\tWarne Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954279 (CK) & #4941625 (Hans_Adler)',\n",
       " 'Watch me.\\tSchau mir zu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954282 (CK) & #4941628 (Hans_Adler)',\n",
       " 'Watch us.\\tBeobachte uns.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954283 (CK) & #4941626 (Hans_Adler)',\n",
       " 'Watch us.\\tBeobachten Sie uns.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954283 (CK) & #4941627 (Hans_Adler)',\n",
       " 'Watch us.\\tSchau uns zu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954283 (CK) & #7986561 (Hans_Adler)',\n",
       " 'Watch us.\\tSchaut uns zu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954283 (CK) & #7986562 (Hans_Adler)',\n",
       " 'Watch us.\\tSchauen Sie uns zu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954283 (CK) & #7986563 (Hans_Adler)',\n",
       " 'Watch us.\\tBeobachtet uns.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954283 (CK) & #7986564 (Hans_Adler)',\n",
       " 'We agree.\\tWir stimmen zu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #628230 (CK) & #3345900 (freddy1)',\n",
       " 'We agree.\\tWir sind einverstanden.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #628230 (CK) & #3345901 (freddy1)',\n",
       " 'We tried.\\tWir haben es versucht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4488381 (PentheusPuddleglum) & #3213581 (Zaghawa)',\n",
       " 'We tried.\\tWir versuchten es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4488381 (PentheusPuddleglum) & #4942984 (Hans_Adler)',\n",
       " \"We'll go.\\tWir werden gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203688 (CK) & #2204231 (Pfirsichbaeumchen)\",\n",
       " \"We're OK.\\tWir sind in Ordnung.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203135 (CK) & #6624815 (Felixjp)\",\n",
       " 'What for?\\tWozu?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #24867 (CK) & #784579 (al_ex_an_der)',\n",
       " 'What fun!\\tWas für ein Spaß!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #36026 (CM) & #2202284 (Pfirsichbaeumchen)',\n",
       " 'Who am I?\\tWer bin ich?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #259378 (Zifre) & #556318 (samueldora)',\n",
       " 'Who came?\\tWer ist gekommen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203607 (CK) & #2205351 (Pfirsichbaeumchen)',\n",
       " 'Who came?\\tWer kam?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203607 (CK) & #2205353 (Pfirsichbaeumchen)',\n",
       " 'Who died?\\tWer ist gestorben?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005187 (CK) & #2741910 (freddy1)',\n",
       " 'Who fell?\\tWer fiel?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203678 (CK) & #2204242 (Pfirsichbaeumchen)',\n",
       " 'Who fell?\\tWer ist gefallen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203678 (CK) & #2204243 (Pfirsichbaeumchen)',\n",
       " 'Who quit?\\tWer hat aufgehört?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005182 (CK) & #4941612 (Hans_Adler)',\n",
       " 'Who quit?\\tWer ist ausgeschieden?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005182 (CK) & #4941613 (Hans_Adler)',\n",
       " 'Who swam?\\tWer schwamm?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203844 (CK) & #2204019 (Pfirsichbaeumchen)',\n",
       " 'Who swam?\\tWer ist geschwommen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203844 (CK) & #2204020 (Pfirsichbaeumchen)',\n",
       " \"Who's he?\\tWer ist er?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203694 (CK) & #362092 (Wolf)\",\n",
       " 'Write me.\\tSchreib mir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954284 (CK) & #2297034 (freddy1)',\n",
       " 'Write me.\\tSchreiben Sie mir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954284 (CK) & #3291013 (Pfirsichbaeumchen)',\n",
       " 'Write me.\\tSchreibt mir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954284 (CK) & #3291015 (Pfirsichbaeumchen)',\n",
       " 'You lost.\\tSie haben verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6440636 (CK) & #4941722 (Hans_Adler)',\n",
       " 'You lost.\\tDu hast verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6440636 (CK) & #6476834 (raggione)',\n",
       " 'You lost.\\tIhr habt verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6440636 (CK) & #6476835 (raggione)',\n",
       " 'Aim. Fire!\\tZielen. Feuer!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1954262 (CK) & #7277172 (Yorwba)',\n",
       " 'Am I late?\\tBin ich zu spät?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3442146 (CK) & #3610895 (Jan_Schreiber)',\n",
       " 'Answer me.\\tAntworten Sie mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #251060 (Eldad) & #358366 (MUIRIEL)',\n",
       " 'Birds fly.\\tVögel fliegen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #278213 (CK) & #544600 (al_ex_an_der)',\n",
       " 'Bless you.\\tGesundheit.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111946 (CK) & #2235463 (Vortarulo)',\n",
       " 'Call home!\\tRuf zuhause an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1730009 (CK) & #1958628 (Tamy)',\n",
       " 'Call home!\\tRufen Sie zuhause an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1730009 (CK) & #1958630 (Tamy)',\n",
       " 'Calm down!\\tBeruhige dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1123494 (Scott) & #351642 (MUIRIEL)',\n",
       " 'Calm down.\\tBeruhige dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435575 (CK) & #351642 (MUIRIEL)',\n",
       " 'Calm down.\\tBeruhigen Sie sich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435575 (CK) & #1215459 (PeterR)',\n",
       " 'Calm down.\\tBeruhigen Sie sich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435575 (CK) & #1970681 (Zaghawa)',\n",
       " 'Can I eat?\\tKann ich essen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1564744 (kerbear407) & #1611462 (Esperantostern)',\n",
       " 'Can we go?\\tKönnen wir gehen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005176 (CK) & #2005278 (Manfredo)',\n",
       " 'Catch Tom.\\tFang Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203856 (CK) & #2204001 (Pfirsichbaeumchen)',\n",
       " 'Catch Tom.\\tFange Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203856 (CK) & #2204002 (Pfirsichbaeumchen)',\n",
       " 'Catch Tom.\\tFangen Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203856 (CK) & #2204003 (Pfirsichbaeumchen)',\n",
       " 'Catch Tom.\\tFangt Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203856 (CK) & #2204004 (Pfirsichbaeumchen)',\n",
       " 'Chill out.\\tEntspann dich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2951593 (CK) & #3172031 (dinkel_girl)',\n",
       " 'Come back.\\tKomm wieder!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972611 (CK) & #3040044 (roeschter)',\n",
       " 'Come here.\\tKomm hierher.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #39944 (Swift) & #358403 (MUIRIEL)',\n",
       " 'Come here.\\tKomm her!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #39944 (Swift) & #915223 (Esperantostern)',\n",
       " 'Come home.\\tKommt heim!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #413767 (Scott) & #1989265 (Alois)',\n",
       " 'Come home.\\tKomm heim!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #413767 (Scott) & #1989267 (Alois)',\n",
       " 'Come over!\\tKomm hierher!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #472477 (blay_paul) & #1430527 (Esperantostern)',\n",
       " 'Come over.\\tKomm her!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972617 (CK) & #915223 (Esperantostern)',\n",
       " 'Come soon.\\tKomm bald.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972619 (CK) & #4941630 (Hans_Adler)',\n",
       " 'Come soon.\\tKommen Sie bald.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972619 (CK) & #4941631 (Hans_Adler)',\n",
       " 'Cool down.\\tBeruhige dich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1860739 (CK) & #351642 (MUIRIEL)',\n",
       " 'Did I win?\\tHabe ich gewonnen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005189 (CK) & #4422617 (Dokuyaku)',\n",
       " 'Do it now.\\tMach es jetzt!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435763 (CK) & #784305 (Manfredo)',\n",
       " 'Dogs bark.\\tHunde bellen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1357070 (sacredceltic) & #934436 (Fingerhut)',\n",
       " \"Don't ask.\\tFrag nicht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #320080 (CM) & #766800 (dima555)\",\n",
       " \"Don't cry.\\tWeine nicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953247 (CK) & #1324274 (Pfirsichbaeumchen)\",\n",
       " \"Don't cry.\\tWeint nicht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953247 (CK) & #1324275 (al_ex_an_der)\",\n",
       " \"Don't cry.\\tWeinen Sie nicht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953247 (CK) & #1324276 (al_ex_an_der)\",\n",
       " \"Don't cry.\\tWeinen Sie nicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953247 (CK) & #2144520 (Tamy)\",\n",
       " \"Don't cry.\\tWeint nicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953247 (CK) & #2144521 (Tamy)\",\n",
       " \"Don't die.\\tStirb nicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1860499 (CK) & #2109050 (Pfirsichbaeumchen)\",\n",
       " \"Don't lie.\\tLüge nicht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111913 (CK) & #967143 (Esperantostern)\",\n",
       " \"Don't run.\\tLauf nicht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111907 (CK) & #6643290 (Felixjp)\",\n",
       " 'Excuse me.\\tEs tut mir leid.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433504 (CK) & #397871 (MikeMolto)',\n",
       " 'Excuse me.\\tEntschuldigung!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433504 (CK) & #397872 (MikeMolto)',\n",
       " 'Excuse me.\\tEntschuldigung.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433504 (CK) & #410211 (MUIRIEL)',\n",
       " 'Excuse me.\\tEntschuldigen Sie!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433504 (CK) & #499092 (Pfirsichbaeumchen)',\n",
       " 'Fantastic!\\tFantastisch!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433511 (CK) & #366461 (Wolf)',\n",
       " 'Fantastic!\\tGanz toll!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433511 (CK) & #5587659 (raggione)',\n",
       " 'Feel this.\\tFühl mal.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972624 (CK) & #7277176 (Yorwba)',\n",
       " 'Follow me.\\tFolge mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #250135 (CK) & #990067 (Peanutfan)',\n",
       " 'Forget it!\\tVergiss es!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1178694 (CK) & #1009327 (MUIRIEL)',\n",
       " 'Forget it!\\tDaraus wird nichts.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1178694 (CK) & #2144201 (freddy1)',\n",
       " 'Forget it.\\tVergiss es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20398 (Eldad) & #916052 (Fingerhut)',\n",
       " 'Forget it.\\tDas kannst du knicken.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20398 (Eldad) & #1458124 (al_ex_an_der)',\n",
       " 'Go for it.\\tNur zu!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4394847 (CK) & #3331180 (brauchinet)',\n",
       " 'Go for it.\\tTue es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4394847 (CK) & #6625084 (Felixjp)',\n",
       " \"Go get it.\\tHol's dir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972634 (CK) & #2169176 (Vortarulo)\",\n",
       " 'Go inside.\\tGeh rein!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2428954 (CK) & #2431183 (Pfirsichbaeumchen)',\n",
       " 'Go inside.\\tKomm rein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2428954 (CK) & #3040106 (roeschter)',\n",
       " 'Go to bed.\\tGeh schlafen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649250 (CK) & #662117 (stefz)',\n",
       " 'Go to bed.\\tGeh ins Bett!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649250 (CK) & #2436846 (Pfirsichbaeumchen)',\n",
       " 'Go to bed.\\tGeht ins Bett!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649250 (CK) & #2657189 (Pfirsichbaeumchen)',\n",
       " 'Go to bed.\\tLegt euch schlafen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649250 (CK) & #2657190 (Pfirsichbaeumchen)',\n",
       " 'Hands off.\\tHände weg!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #268620 (CM) & #1814451 (Tamy)',\n",
       " 'Have some.\\tNimm dir davon.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111823 (CK) & #4942814 (Hans_Adler)',\n",
       " 'Have some.\\tNehmen Sie davon.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111823 (CK) & #4942816 (Hans_Adler)',\n",
       " 'He is ill.\\tEr ist krank.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #371409 (saeb) & #450240 (al_ex_an_der)',\n",
       " 'He is old.\\tEr ist alt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #302334 (CK) & #364435 (Wolf)',\n",
       " 'He smiled.\\tEr lächelte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1320015 (Eldad) & #7712483 (wolfgangth)',\n",
       " \"He's a DJ.\\tEr ist DJ.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1383444 (CK) & #1386432 (al_ex_an_der)\",\n",
       " \"He's a DJ.\\tEr ist Plattenaufleger.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1383444 (CK) & #1833392 (Pfirsichbaeumchen)\",\n",
       " \"He's fast.\\tEr ist schnell.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #838848 (Scott) & #860311 (Esperantostern)\",\n",
       " \"He's good.\\tEr ist gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1855193 (Spamster) & #1857848 (Pfirsichbaeumchen)\",\n",
       " \"He's lazy.\\tEr ist träge.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2030108 (Spamster) & #2030506 (Pfirsichbaeumchen)\",\n",
       " \"He's rich.\\tEr ist reich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1790802 (Spamster) & #1790810 (Pfirsichbaeumchen)\",\n",
       " 'Here I am.\\tHier bin ich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1540806 (CK) & #3033800 (Pfirsichbaeumchen)',\n",
       " \"Here's $5.\\tHier sind fünf Dollar.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #72539 (CS) & #369772 (MUIRIEL)\",\n",
       " 'Hold fire.\\tNicht schießen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972642 (CK) & #1684247 (Pfirsichbaeumchen)',\n",
       " 'Hold this.\\tHalt das!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972645 (CK) & #4941591 (Hans_Adler)',\n",
       " 'Hold this.\\tHalt das mal.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972645 (CK) & #4941592 (Hans_Adler)',\n",
       " 'Hold this.\\tHalten Sie das!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972645 (CK) & #4941594 (Hans_Adler)',\n",
       " 'Hold this.\\tHaltet das!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1972645 (CK) & #4941596 (Hans_Adler)',\n",
       " 'How awful!\\tSchrecklich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #72077 (CM) & #2940050 (Manfredo)',\n",
       " 'How is it?\\tWie ist die Lage?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1487691 (marshmallowcat) & #2116023 (Pfirsichbaeumchen)',\n",
       " 'How weird!\\tWie seltsam!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4462562 (CK) & #4941572 (Hans_Adler)',\n",
       " 'Humor Tom.\\tLass Tom doch seinen Willen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203861 (CK) & #7141488 (Luiaard)',\n",
       " 'Humor Tom.\\tLasst Tom doch seinen Willen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203861 (CK) & #7141498 (Luiaard)',\n",
       " 'I am busy.\\tIch bin beschäftigt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2425776 (bionicboy) & #351599 (MUIRIEL)',\n",
       " 'I am busy.\\tIch habe zu tun.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2425776 (bionicboy) & #2624978 (Pfirsichbaeumchen)',\n",
       " 'I am fine.\\tMir geht es gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1196240 (waeltken) & #659407 (Esperantostern)',\n",
       " 'I am fine.\\tEs geht mir gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1196240 (waeltken) & #2705321 (Pfirsichbaeumchen)',\n",
       " 'I am here.\\tIch bin hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2778806 (CK) & #2323643 (pne)',\n",
       " 'I am okay.\\tEs geht mir gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #376402 (saeb) & #2705321 (Pfirsichbaeumchen)',\n",
       " 'I am sure.\\tIch bin sicher.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #436433 (lukaszpp) & #399166 (MUIRIEL)',\n",
       " 'I am tall.\\tIch bin groß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #436275 (lukaszpp) & #576296 (Pfirsichbaeumchen)',\n",
       " 'I am weak.\\tIch bin schwach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2681118 (CM) & #2205477 (Pfirsichbaeumchen)',\n",
       " 'I am well.\\tMir geht es gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #574624 (Zac2333) & #659407 (Esperantostern)',\n",
       " 'I am well.\\tEs geht mir gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #574624 (Zac2333) & #2705321 (Pfirsichbaeumchen)',\n",
       " 'I can fly.\\tIch kann fliegen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #774825 (marloncori) & #782641 (Chris)',\n",
       " 'I can run.\\tIch kann laufen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1173661 (CK) & #1074395 (Esperantostern)',\n",
       " 'I can run.\\tIch kann rennen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1173661 (CK) & #1313711 (Pfirsichbaeumchen)',\n",
       " 'I can ski.\\tIch kann Ski fahren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52315 (CK) & #358380 (MUIRIEL)',\n",
       " 'I can win.\\tIch kann gewinnen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6358763 (CK) & #6400838 (Pfirsichbaeumchen)',\n",
       " 'I cheated.\\tIch habe geschummelt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828659 (CK) & #7141500 (Luiaard)',\n",
       " 'I fainted.\\tIch wurde ohnmächtig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20364 (CK) & #407486 (MUIRIEL)',\n",
       " 'I fainted.\\tIch fiel in Ohnmacht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20364 (CK) & #428527 (MUIRIEL)',\n",
       " 'I fear so.\\tIch fürchte, ja.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #245257 (Zifre) & #3062837 (Pfirsichbaeumchen)',\n",
       " 'I gave up.\\tIch habe aufgegeben.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2863162 (Amastan) & #6644373 (Felixjp)',\n",
       " 'I gave up.\\tIch gab auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2863162 (Amastan) & #6644471 (Felixjp)',\n",
       " 'I get you.\\tIch verstehe, was du meinst.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #904789 (Trailsend) & #900088 (Esperantostern)',\n",
       " 'I giggled.\\tIch kicherte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828665 (CK) & #6589453 (Pfirsichbaeumchen)',\n",
       " 'I give in.\\tIch gebe nach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #454503 (FeuDRenais) & #6644456 (Felixjp)',\n",
       " 'I give in.\\tIch füge mich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #454503 (FeuDRenais) & #6644472 (Felixjp)',\n",
       " 'I got mad.\\tIch wurde wütend.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2245795 (CK) & #2393716 (Pfirsichbaeumchen)',\n",
       " 'I got mad.\\tIch wurde böse.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2245795 (CK) & #6842893 (Pfirsichbaeumchen)',\n",
       " 'I had fun.\\tIch hatte Spaß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2245851 (CK) & #2999883 (pne)',\n",
       " 'I had fun.\\tIch habe mich amüsiert.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2245851 (CK) & #3464420 (Zaghawa)',\n",
       " 'I hit Tom.\\tIch habe Tom geschlagen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2235723 (CK) & #5245285 (christian42)',\n",
       " 'I hope so.\\tDas hoffe ich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #51142 (CK) & #340885 (MUIRIEL)',\n",
       " 'I hope so.\\tIch hoffe es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #51142 (CK) & #759593 (Bellinger)',\n",
       " 'I hung up.\\tIch habe aufgelegt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #7768432 (OmkarM) & #2201887 (Esperantostern)',\n",
       " 'I knew it.\\tIch wusste es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1209117 (CK) & #2030483 (Pfirsichbaeumchen)',\n",
       " 'I knew it.\\tIch wusste das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1209117 (CK) & #3879112 (raggione)',\n",
       " 'I laughed.\\tIch lachte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #258713 (CK) & #547385 (Espi)',\n",
       " 'I like it.\\tDas gefällt mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1520245 (ABChessel) & #1358067 (Manfredo)',\n",
       " 'I lost it.\\tIch habe ihn verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2245956 (CK) & #7253037 (raggione)',\n",
       " 'I love it!\\tIch liebe es!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1132072 (CK) & #6644199 (Felixjp)',\n",
       " 'I love it.\\tIch liebe es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3812708 (robbieheslop) & #6644198 (Felixjp)',\n",
       " 'I made it.\\tIch habe es gemacht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828672 (CK) & #700384 (Alois)',\n",
       " 'I made it.\\tIch habe es geschafft.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828672 (CK) & #1223697 (BraveSentry)',\n",
       " 'I may win.\\tVielleicht gewinne ich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6358757 (CK) & #6400839 (Pfirsichbaeumchen)',\n",
       " 'I mean it!\\tIch meine es so!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #322125 (jakov) & #657092 (samueldora)',\n",
       " 'I mean it!\\tIch meine es ernst!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #322125 (jakov) & #2997934 (pne)',\n",
       " 'I mean it.\\tIch meine es ernst.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #250804 (CK) & #1768662 (Pfirsichbaeumchen)',\n",
       " 'I mean it.\\tEs ist ernst gemeint von mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #250804 (CK) & #1806646 (Tamy)',\n",
       " 'I met Tom.\\tIch habe Tom getroffen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6196260 (CK) & #3753519 (Zaghawa)',\n",
       " 'I met Tom.\\tIch traf Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6196260 (CK) & #7592174 (wolfgangth)',\n",
       " 'I met Tom.\\tIch bin Tom begegnet.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6196260 (CK) & #7592175 (wolfgangth)',\n",
       " 'I met him.\\tIch habe ihn getroffen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2771581 (CK) & #3172032 (dinkel_girl)',\n",
       " 'I miss it.\\tIch vermisse es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1337519 (CK) & #6625839 (Felixjp)',\n",
       " 'I miss it.\\tEs fehlt mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1337519 (CK) & #6627179 (raggione)',\n",
       " \"I promise.\\tIch versprech's.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111800 (CK) & #2159648 (Vortarulo)\",\n",
       " 'I promise.\\tIch verspreche es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111800 (CK) & #2159649 (Vortarulo)',\n",
       " 'I said no.\\tIch sagte nein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2405884 (CK) & #6625855 (Felixjp)',\n",
       " 'I saw Tom.\\tIch habe Tom gesehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2244569 (CK) & #3753516 (Zaghawa)',\n",
       " 'I saw him.\\tIch habe ihn gesehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #519963 (darinmex) & #359066 (MUIRIEL)',\n",
       " 'I saw one.\\tIch habe einen gesehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3168241 (CK) & #6611552 (Felixjp)',\n",
       " 'I saw you.\\tIch habe dich gesehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2247402 (CK) & #2527672 (Pfirsichbaeumchen)',\n",
       " 'I saw you.\\tIch habe euch gesehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2247402 (CK) & #2527674 (Pfirsichbaeumchen)',\n",
       " 'I see Tom.\\tIch sehe Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2235735 (CK) & #2559901 (Pfirsichbaeumchen)',\n",
       " 'I took it.\\tIch nahm es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828682 (CK) & #6643992 (Felixjp)',\n",
       " 'I took it.\\tIch habe es genommen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828682 (CK) & #6644347 (Felixjp)',\n",
       " 'I want it.\\tIch möchte es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1148324 (cntrational) & #2296175 (Tamy)',\n",
       " 'I want it.\\tIch will es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1148324 (cntrational) & #6625772 (Felixjp)',\n",
       " 'I was new.\\tIch war neu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3822076 (CK) & #6611521 (Felixjp)',\n",
       " 'I was shy.\\tIch war schüchtern.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4695537 (Hybrid) & #4701236 (dannylow)',\n",
       " 'I will go.\\tIch gehe.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2247573 (CK) & #439366 (Robroy)',\n",
       " 'I will go.\\tIch werde gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2247573 (CK) & #4454179 (rocco_granata)',\n",
       " 'I woke up.\\tIch wachte auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3818967 (CK) & #3940126 (Tamy)',\n",
       " \"I'd do it.\\tIch würde es tun.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3823427 (CK) & #6625751 (Felixjp)\",\n",
       " \"I'd leave.\\tIch würde gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111789 (CK) & #7374105 (Yorwba)\",\n",
       " \"I'll call.\\tIch werde anrufen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111787 (CK) & #7277213 (Yorwba)\",\n",
       " \"I'll come.\\tIch werde kommen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5828685 (CK) & #4776708 (Germic)\",\n",
       " \"I'll cook.\\tIch werde kochen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111783 (CK) & #6643577 (Felixjp)\",\n",
       " \"I'll help.\\tIch helfe.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549831 (CK) & #3303103 (Pfirsichbaeumchen)\",\n",
       " \"I'll live.\\tIch werde leben.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123627 (CK) & #2139191 (Pfirsichbaeumchen)\",\n",
       " \"I'll obey.\\tIch werde gehorchen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111777 (CK) & #7451628 (Yorwba)\",\n",
       " \"I'll quit.\\tIch werde aufhören.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111775 (CK) & #6643669 (Felixjp)\",\n",
       " \"I'll quit.\\tIch werde abbrechen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111775 (CK) & #7374104 (Yorwba)\",\n",
       " \"I'll sing.\\tIch werde singen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111772 (CK) & #3766290 (Jens_Odo)\",\n",
       " \"I'll stay.\\tIch werde bleiben.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111769 (CK) & #2980574 (pne)\",\n",
       " \"I'll stop.\\tIch werde aufhören.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2878242 (CK) & #6643669 (Felixjp)\",\n",
       " \"I'll talk.\\tIch werde reden.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2878243 (CK) & #7281091 (Yorwba)\",\n",
       " \"I'll wait.\\tIch werde warten.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111766 (CK) & #2982909 (pne)\",\n",
       " \"I'll walk.\\tIch gehe zu Fuß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111765 (CK) & #182037 (MUIRIEL)\",\n",
       " \"I'm a man.\\tIch bin ein Mann.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #580774 (FeuDRenais) & #139166 (MUIRIEL)\",\n",
       " \"I'm angry.\\tIch bin sauer.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #507444 (callmath) & #4333608 (Wuzzy)\",\n",
       " \"I'm awake.\\tIch bin wach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #323761 (CK) & #1313714 (Pfirsichbaeumchen)\",\n",
       " \"I'm blind.\\tIch bin blind.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111753 (CK) & #7277209 (Yorwba)\",\n",
       " \"I'm bored.\\tIch bin gelangweilt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #431539 (Clavain) & #520026 (Espi)\",\n",
       " \"I'm bored.\\tMir ist langweilig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #431539 (Clavain) & #534038 (joha2)\",\n",
       " \"I'm broke.\\tIch bin knapp bei Kasse.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #18547 (CK) & #368545 (xtofu80)\",\n",
       " \"I'm broke.\\tIch bin pleite.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #18547 (CK) & #450464 (al_ex_an_der)\",\n",
       " \"I'm cured.\\tIch bin geheilt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2283727 (CK) & #7463705 (Yorwba)\",\n",
       " \"I'm drunk.\\tIch bin betrunken.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #958784 (FeuDRenais) & #370902 (xtofu80)\",\n",
       " \"I'm drunk.\\tIch bin blau.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #958784 (FeuDRenais) & #940616 (Hans07)\",\n",
       " \"I'm dying.\\tIch werde bald sterben.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2051764 (Shishir) & #3836805 (Pfirsichbaeumchen)\",\n",
       " \"I'm early.\\tIch bin früh dran.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111737 (CK) & #7374081 (Yorwba)\",\n",
       " \"I'm first.\\tIch bin Erster.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2202863 (CK) & #7451335 (Yorwba)\",\n",
       " \"I'm first.\\tIch bin Erste.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2202863 (CK) & #7451337 (Yorwba)\",\n",
       " \"I'm going.\\tIch gehe jetzt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #495219 (adjusting) & #360920 (MUIRIEL)\",\n",
       " \"I'm happy.\\tIch bin glücklich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1872056 (Eldad) & #626253 (kolonjano)\",\n",
       " \"I'm happy.\\tIch bin froh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1872056 (Eldad) & #1657039 (Esperantostern)\",\n",
       " \"I'm lying.\\tIch lüge.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111712 (CK) & #7369981 (Yorwba)\",\n",
       " \"I'm needy.\\tIch bin bedürftig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203101 (CK) & #7451368 (Yorwba)\",\n",
       " \"I'm obese.\\tIch bin fett.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3330257 (CK) & #1137134 (Esperantostern)\",\n",
       " \"I'm ready!\\tIch bin soweit!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123619 (CK) & #2139202 (Pfirsichbaeumchen)\",\n",
       " \"I'm ready.\\tIch bin soweit.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1891110 (CK) & #2153181 (Pfirsichbaeumchen)\",\n",
       " \"I'm ready.\\tIch bin so weit.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1891110 (CK) & #7372546 (raggione)\",\n",
       " \"I'm right.\\tIch habe recht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #321343 (CK) & #801040 (jakov)\",\n",
       " \"I'm right.\\tIch habe Recht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #321343 (CK) & #817440 (jakov)\",\n",
       " \"I'm sober.\\tIch bin nüchtern.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2361904 (CK) & #3068308 (Pfirsichbaeumchen)\",\n",
       " \"I'm sorry.\\tEs tut mir leid.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #38387 (CK) & #397871 (MikeMolto)\",\n",
       " \"I'm sorry.\\tEntschuldigung!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #38387 (CK) & #397872 (MikeMolto)\",\n",
       " \"I'm stuck.\\tIch stecke fest.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203412 (CK) & #7413404 (Yorwba)\",\n",
       " \"I'm tired.\\tIch bin müde!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1705 (brauliobezerra) & #372642 (MUIRIEL)\",\n",
       " \"I'm tired.\\tIch bin müde.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1705 (brauliobezerra) & #459131 (Espi)\",\n",
       " \"I'm tough.\\tIch bin zäh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203467 (CK) & #7413439 (Yorwba)\",\n",
       " \"I'm upset.\\tIch bin verärgert.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203558 (CK) & #2205500 (Pfirsichbaeumchen)\",\n",
       " \"I'm upset.\\tIch bin bestürzt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203558 (CK) & #4019285 (raggione)\",\n",
       " \"I'm yours.\\tIch gehöre dir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111665 (CK) & #6625985 (Felixjp)\",\n",
       " \"I've lost.\\tIch habe verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1126967 (CK) & #4659717 (AC)\",\n",
       " 'Ignore it.\\tBeachte es gar nicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649249 (CK) & #2657185 (Pfirsichbaeumchen)',\n",
       " 'Ignore it.\\tBeachten Sie es gar nicht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2649249 (CK) & #2657186 (Pfirsichbaeumchen)',\n",
       " 'Is Tom OK?\\tIst Tom okay?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2244627 (CK) & #6625709 (Felixjp)',\n",
       " 'Is Tom in?\\tIst Tom da?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2244609 (CK) & #3049564 (pne)',\n",
       " 'Is he Tom?\\tIst er Tom?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6196259 (CK) & #6625724 (Felixjp)',\n",
       " 'Is it bad?\\tIst es schlimm?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1665 (CK) & #470 (MUIRIEL)',\n",
       " 'Is it far?\\tIst das weit?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1308427 (CK) & #2078166 (Manfredo)',\n",
       " 'Is it hot?\\tIst es heiß?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #456027 (lukaszpp) & #5363181 (RandomUsername)',\n",
       " 'Is it new?\\tIst es neu?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1910886 (Zaghawa) & #1910884 (Zaghawa)',\n",
       " 'It burned.\\tEs brannte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3825991 (CK) & #4942971 (Hans_Adler)',\n",
       " 'It burned.\\tEs verbrannte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3825991 (CK) & #4942972 (Hans_Adler)',\n",
       " 'It snowed.\\tEs hat geschneit.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1181168 (Eldad) & #1552159 (Zaghawa)',\n",
       " 'It worked.\\tEs hat geklappt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2283736 (CK) & #4514430 (raggione)',\n",
       " 'It worked.\\tDas hat funktioniert.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2283736 (CK) & #7749134 (raggione)',\n",
       " \"It's 3:30.\\tEs ist halb vier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #456175 (lukaszpp) & #1304700 (Manfredo)\",\n",
       " \"It's 7:45.\\tEs ist 7:45 Uhr.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2762784 (CK) & #1602317 (Sudajaengi)\",\n",
       " \"It's 7:45.\\tEs ist sieben Uhr fünfundvierzig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2762784 (CK) & #3172228 (dinkel_girl)\",\n",
       " \"It's 8:30.\\tEs ist 8:30 Uhr.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4636492 (CK) & #6626120 (Felixjp)\",\n",
       " \"It's 9:15.\\tEs ist Viertel nach neun.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2762785 (CK) & #344350 (MUIRIEL)\",\n",
       " \"It's 9:15.\\tEs ist 9.15\\xa0Uhr.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2762785 (CK) & #2763078 (Pfirsichbaeumchen)\",\n",
       " \"It's a TV.\\tDas ist ein Fernseher.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #42698 (CK) & #356174 (MUIRIEL)\",\n",
       " \"It's cold.\\tEs ist kalt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1813 (CK) & #623 (MUIRIEL)\",\n",
       " \"It's cool.\\tEs ist kühl.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123609 (CK) & #2157495 (Pfirsichbaeumchen)\",\n",
       " \"It's dark.\\tEs ist dunkel.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123608 (CK) & #2139215 (Pfirsichbaeumchen)\",\n",
       " \"It's easy.\\tDas ist einfach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #561962 (CK) & #564337 (MUIRIEL)\",\n",
       " \"It's easy.\\tEs ist einfach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #561962 (CK) & #1313715 (Pfirsichbaeumchen)\",\n",
       " \"It's fair.\\tEs ist gerecht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123604 (CK) & #7413206 (Yorwba)\",\n",
       " \"It's free.\\tEs ist frei.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #435782 (CK) & #1210584 (Espi)\",\n",
       " \"It's hard.\\tDas ist hart.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1568774 (Sharaf78) & #1584301 (Esperantostern)\",\n",
       " \"It's here.\\tEs ist hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123598 (CK) & #5284084 (bonny37)\",\n",
       " \"It's late.\\tEs ist spät.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #31195 (CK) & #338438 (Sprachprofi)\",\n",
       " \"It's mine.\\tEs ist meins.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123592 (CK) & #5331782 (bonny37)\",\n",
       " \"It's mine.\\tSie gehört mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123592 (CK) & #7975639 (raggione)\",\n",
       " \"It's mine.\\tEr gehört mir.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123592 (CK) & #7975642 (raggione)\",\n",
       " \"It's okay.\\tEs ist in Ordnung.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123589 (CK) & #6626109 (Felixjp)\",\n",
       " \"It's open.\\tEs ist geöffnet.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2283743 (CK) & #4750028 (bonny37)\",\n",
       " \"It's ours.\\tEs ist unsers.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123588 (CK) & #5331780 (bonny37)\",\n",
       " \"It's over.\\tEs ist vorbei.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1107969 (Scott) & #8087505 (driini)\",\n",
       " \"It's sand.\\tEs ist Sand.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123581 (CK) & #7413201 (Yorwba)\",\n",
       " \"It's true.\\tEs ist wahr.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #54868 (CK) & #396608 (MikeMolto)\",\n",
       " \"It's work.\\tEs ist Arbeit.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433520 (CK) & #547271 (Espi)\",\n",
       " \"It's work.\\tDas ist meine Arbeit.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433520 (CK) & #3771311 (mauersegler)\",\n",
       " 'Jump down.\\tSpring runter!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4356531 (CK) & #4356758 (pullnosemans)',\n",
       " 'Keep away.\\tBleib weg.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111655 (CK) & #4942778 (Hans_Adler)',\n",
       " 'Keep away.\\tBleibt weg.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111655 (CK) & #4942779 (Hans_Adler)',\n",
       " 'Keep away.\\tBleiben Sie weg.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111655 (CK) & #4942780 (Hans_Adler)',\n",
       " 'Keep back.\\tBleiben Sie zurück.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111654 (CK) & #4942777 (Hans_Adler)',\n",
       " 'Keep cool.\\tBleib gelassen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111652 (CK) & #2717264 (Pfirsichbaeumchen)',\n",
       " 'Keep that.\\tBehalt’s!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111634 (CK) & #2717339 (Pfirsichbaeumchen)',\n",
       " 'Keep them.\\tBehalte sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111633 (CK) & #4942770 (Hans_Adler)',\n",
       " 'Keep them.\\tBehalten Sie sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111633 (CK) & #4942771 (Hans_Adler)',\n",
       " 'Keep them.\\tBehaltet sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111633 (CK) & #4942773 (Hans_Adler)',\n",
       " 'Keep them.\\tBehalt sie!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111633 (CK) & #8211339 (raggione)',\n",
       " 'Keep warm.\\tHalt dich warm.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111630 (CK) & #4942764 (Hans_Adler)',\n",
       " 'Keep warm.\\tHaltet euch warm.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111630 (CK) & #4942765 (Hans_Adler)',\n",
       " 'Keep warm.\\tHalten Sie sich warm.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111630 (CK) & #4942766 (Hans_Adler)',\n",
       " 'Leave Tom.\\tVerlasse Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111626 (CK) & #4942755 (Hans_Adler)',\n",
       " 'Leave Tom.\\tVerlassen Sie Tom.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111626 (CK) & #4942756 (Hans_Adler)',\n",
       " 'Leave now.\\tGeh jetzt!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52067 (CM) & #1853898 (Tamy)',\n",
       " 'Leave now.\\tGeht jetzt!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #52067 (CM) & #1853899 (Tamy)',\n",
       " 'Let it be.\\tLass es sein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #764592 (U2FS) & #4943091 (Hans_Adler)',\n",
       " 'Let it be.\\tLass es bleiben.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #764592 (U2FS) & #4943092 (Hans_Adler)',\n",
       " 'Let me be.\\tLass mich in Ruhe.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #660426 (CM) & #358368 (MUIRIEL)',\n",
       " 'Let me go!\\tLassen Sie mich gehen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #250112 (CK) & #582412 (MUIRIEL)',\n",
       " 'Let me go!\\tLass mich gehen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #250112 (CK) & #838639 (Espi)',\n",
       " 'Let me in.\\tLass mich herein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #277466 (CK) & #444002 (al_ex_an_der)',\n",
       " 'Let us go.\\tLass uns gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4945628 (_anna) & #4944498 (Hans_Adler)',\n",
       " 'Let us go.\\tLassen Sie uns gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4945628 (_anna) & #4944500 (Hans_Adler)',\n",
       " \"Let's eat.\\tEssen wir etwas!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1830543 (CK) & #3359873 (Pfirsichbaeumchen)\",\n",
       " \"Let's see.\\tSchauen wir mal.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111618 (CK) & #640939 (samueldora)\",\n",
       " \"Let's try!\\tLasst es uns versuchen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1176908 (bart) & #3779152 (Pfirsichbaeumchen)\",\n",
       " \"Let's try!\\tLass es uns versuchen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1176908 (bart) & #3886105 (Tickler)\",\n",
       " \"Let's try!\\tLass es uns probieren!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1176908 (bart) & #5349743 (RandomUsername)\",\n",
       " 'Lie still.\\tLieg still und beweg dich nicht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1839430 (CK) & #4659721 (AC)',\n",
       " 'Listen up.\\tHör zu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3171862 (CK) & #5337423 (Ganbatte94)',\n",
       " 'Look back!\\tSchau nach hinten!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #240166 (CK) & #631884 (BraveSentry)',\n",
       " 'Look down.\\tGuck runter.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #7466072 (megamanenm) & #8227480 (MisterTrouser)',\n",
       " 'Look here.\\tSchau hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111606 (CK) & #6643120 (Felixjp)',\n",
       " 'Look here.\\tSchau her.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111606 (CK) & #6643121 (Felixjp)',\n",
       " 'Loosen it.\\tDreh es auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111605 (CK) & #2204797 (Manfredo)',\n",
       " 'Loosen up.\\tMach dich locker!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1845492 (CK) & #4659727 (AC)',\n",
       " 'Move over.\\tRutsch mal ein Stück!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1553407 (CK) & #4659686 (AC)',\n",
       " 'Nice shot!\\tGuter Schuss!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1857550 (Spamster) & #1857783 (Pfirsichbaeumchen)',\n",
       " 'Of course!\\tNatürlich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433730 (CK) & #778367 (BraveSentry)',\n",
       " 'Of course!\\tSelbstverständlich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433730 (CK) & #778368 (BraveSentry)',\n",
       " 'Of course!\\tAuf jeden Fall!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433730 (CK) & #778369 (BraveSentry)',\n",
       " 'Of course!\\tNa klar!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433730 (CK) & #778370 (BraveSentry)',\n",
       " 'Of course!\\tAber sicher doch!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433730 (CK) & #778371 (BraveSentry)',\n",
       " 'Please go.\\tBitte geh.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3171854 (CK) & #4942934 (Hans_Adler)',\n",
       " 'Please go.\\tBitte geht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3171854 (CK) & #4942935 (Hans_Adler)',\n",
       " 'Please go.\\tBitte gehen Sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3171854 (CK) & #4942937 (Hans_Adler)',\n",
       " 'Put it on.\\tSetz es auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943236 (Hans_Adler)',\n",
       " 'Put it on.\\tTu es drauf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943237 (Hans_Adler)',\n",
       " 'Put it on.\\tSetz ihn auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943980 (raggione)',\n",
       " 'Put it on.\\tZieh sie an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943982 (raggione)',\n",
       " 'Put it on.\\tSetz sie auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943984 (raggione)',\n",
       " 'Put it on.\\tSetz sie dir auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943986 (raggione)',\n",
       " 'Put it on.\\tZieh sie dir an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943988 (raggione)',\n",
       " 'Put it on.\\tSetz ihn dir auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1231218 (fengli) & #4943990 (raggione)',\n",
       " 'Read this.\\tLies das hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1098659 (cntrational) & #1099265 (MUIRIEL)',\n",
       " 'Say \"aah.\"\\tSag „Ah!“\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2852741 (CK) & #5897067 (Pfirsichbaeumchen)',\n",
       " 'Say hello.\\tSag Hallo.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111563 (CK) & #6625643 (Felixjp)',\n",
       " 'See below.\\tSiehe unten.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #25377 (CM) & #5255405 (Pfirsichbaeumchen)',\n",
       " 'Seize him!\\tFass ihn!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111558 (CK) & #7369937 (Yorwba)',\n",
       " 'Seize him!\\tFassen Sie ihn!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111558 (CK) & #7369939 (Yorwba)',\n",
       " 'Seize him!\\tFasst ihn!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111558 (CK) & #7369941 (Yorwba)',\n",
       " 'Seriously?\\tWirklich?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1327057 (bmaynard87) & #373376 (lilygilder)',\n",
       " 'Seriously?\\tEcht?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1327057 (bmaynard87) & #808911 (Manfredo)',\n",
       " 'Seriously?\\tErnsthaft?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1327057 (bmaynard87) & #937378 (Sudajaengi)',\n",
       " 'Seriously?\\tIm Ernst?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1327057 (bmaynard87) & #937379 (Sudajaengi)',\n",
       " 'She cried.\\tSie weinte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #313193 (CK) & #851915 (Esperantostern)',\n",
       " \"She tried.\\tSie hat's versucht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #309976 (CK) & #924662 (Sudajaengi)\",\n",
       " 'She walks.\\tSie geht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #316925 (CK) & #6402227 (Hannivar)',\n",
       " 'She walks.\\tSie geht zu Fuß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #316925 (CK) & #8067950 (driini)',\n",
       " 'Sign here.\\tUnterschreibe hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1526517 (pauldhunt) & #949228 (Sudajaengi)',\n",
       " 'Sign here.\\tUnterschreiben Sie hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1526517 (pauldhunt) & #3033622 (pne)',\n",
       " 'Sign this.\\tUnterschreiben Sie das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111555 (CK) & #4941778 (Hans_Adler)',\n",
       " 'Sign this.\\tUnterschreib das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111555 (CK) & #4941779 (Hans_Adler)',\n",
       " 'Sit by me.\\tSetz dich zu mir!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2249967 (CK) & #2696100 (Pfirsichbaeumchen)',\n",
       " 'Sit still.\\tSitz still.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111552 (CK) & #4941776 (Hans_Adler)',\n",
       " 'Sit still.\\tSitzen Sie still.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111552 (CK) & #4941777 (Hans_Adler)',\n",
       " 'Sit there.\\tSetz dich dorthin.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111551 (CK) & #1519500 (Espi)',\n",
       " 'Sit tight.\\tHarre aus!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #40196 (CM) & #1012489 (Manfredo)',\n",
       " 'Start now.\\tBeginnen Sie jetzt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111523 (CK) & #4941757 (Hans_Adler)',\n",
       " 'Start now.\\tFang jetzt an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111523 (CK) & #4941759 (Hans_Adler)',\n",
       " 'Stay away.\\tBleiben Sie weg.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111516 (CK) & #4942780 (Hans_Adler)',\n",
       " 'Stay back.\\tBleibt zurück!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1850251 (CK) & #4659728 (AC)',\n",
       " 'Stay calm.\\tBleib ruhig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #788304 (hrin) & #788437 (BraveSentry)',\n",
       " 'Stay calm.\\tBleiben Sie ruhig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #788304 (hrin) & #1076801 (MUIRIEL)',\n",
       " 'Stay cool.\\tBleib ruhig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111514 (CK) & #788437 (BraveSentry)',\n",
       " 'Stay cool.\\tBleibt ruhig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111514 (CK) & #1076800 (MUIRIEL)',\n",
       " 'Stay cool.\\tBleiben Sie ruhig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111514 (CK) & #1076801 (MUIRIEL)',\n",
       " 'Stay cool.\\tBleib cool.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111514 (CK) & #2398607 (Zaghawa)',\n",
       " 'Stay cool.\\tBleibt cool.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111514 (CK) & #2398612 (Zaghawa)',\n",
       " 'Stay down!\\tBleib unten!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1777435 (Spamster) & #4659720 (AC)',\n",
       " 'Stay down.\\tBleib unten!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1841182 (CK) & #4659720 (AC)',\n",
       " 'Stay thin.\\tBleiben Sie dünn.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #275014 (CM) & #4825509 (rumpelstilzchen)',\n",
       " 'Stay thin.\\tBleib dünn.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #275014 (CM) & #4825510 (rumpelstilzchen)',\n",
       " 'Step back.\\tTritt zurück!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1553393 (CK) & #1688328 (Pfirsichbaeumchen)',\n",
       " 'Stop here.\\tHör hier auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111500 (CK) & #6644773 (Felixjp)',\n",
       " 'Stop here.\\tHalte hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111500 (CK) & #6644867 (Felixjp)',\n",
       " 'Stop here.\\tHalte hier an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111500 (CK) & #6644868 (Felixjp)',\n",
       " 'Stop here.\\tBleib hier stehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111500 (CK) & #6645723 (Felixjp)',\n",
       " 'Stop that!\\tLass das!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #651592 (orbpic) & #440925 (Espi)',\n",
       " 'Stop that!\\tHör auf damit!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #651592 (orbpic) & #579371 (kroko)',\n",
       " 'Stop that!\\tHör mal auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #651592 (orbpic) & #1238664 (al_ex_an_der)',\n",
       " 'Stop that!\\tHör mal auf damit!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #651592 (orbpic) & #1238665 (al_ex_an_der)',\n",
       " 'Stop them.\\tStopp sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111491 (CK) & #5480547 (RandomUsername)',\n",
       " 'Stop them.\\tStoppen Sie sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111491 (CK) & #5480548 (RandomUsername)',\n",
       " 'Take care!\\tPass auf dich auf!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #324861 (CK) & #1239683 (Esperantostern)',\n",
       " \"Take care.\\tMach's gut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20362 (sacredceltic) & #2024754 (Vortarulo)\",\n",
       " 'Take mine.\\tNimm meins.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942662 (Hans_Adler)',\n",
       " 'Take mine.\\tNimm meine.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942663 (Hans_Adler)',\n",
       " 'Take mine.\\tNimm meinen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942664 (Hans_Adler)',\n",
       " 'Take mine.\\tNehmt meins.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942666 (Hans_Adler)',\n",
       " 'Take mine.\\tNehmt meine.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942667 (Hans_Adler)',\n",
       " 'Take mine.\\tNehmt meinen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942668 (Hans_Adler)',\n",
       " 'Take mine.\\tNehmen Sie meins.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942669 (Hans_Adler)',\n",
       " 'Take mine.\\tNehmen Sie meinen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942670 (Hans_Adler)',\n",
       " 'Take mine.\\tNehmen Sie meine.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111483 (CK) & #4942671 (Hans_Adler)',\n",
       " 'Take over.\\tÜbernimm du.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111482 (CK) & #4942658 (Hans_Adler)',\n",
       " 'Take over.\\tÜbernehmt ihr.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111482 (CK) & #4942659 (Hans_Adler)',\n",
       " 'Take over.\\tÜbernehmen Sie.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111482 (CK) & #4942660 (Hans_Adler)',\n",
       " 'Take this.\\tNimm das.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111479 (CK) & #6643157 (Felixjp)',\n",
       " 'Thank you.\\tDanke!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1876041 (Asma) & #397489 (MUIRIEL)',\n",
       " \"That's OK.\\tDas ist o.k.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111418 (CK) & #6626295 (Felixjp)\",\n",
       " \"That's it.\\tDas war's.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #270709 (mamat) & #2318384 (al_ex_an_der)\",\n",
       " \"That's me.\\tDas bin ich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1250350 (CK) & #3811992 (Trebalor)\",\n",
       " \"That's me.\\tIch bin jetzt fertig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1250350 (CK) & #4380940 (raggione)\",\n",
       " 'Then what?\\tNa und?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #53035 (CM) & #142 (MUIRIEL)',\n",
       " 'They fell.\\tSie fielen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111371 (CK) & #4941728 (Hans_Adler)',\n",
       " 'They fell.\\tSie stürzten.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111371 (CK) & #4941730 (Hans_Adler)',\n",
       " 'They lied.\\tSie haben gelogen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111367 (CK) & #2169441 (Vortarulo)',\n",
       " 'They lost.\\tSie verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111366 (CK) & #4941721 (Hans_Adler)',\n",
       " 'They lost.\\tSie haben verloren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111366 (CK) & #4941722 (Hans_Adler)',\n",
       " 'They swam.\\tSie schwammen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1963280 (CK) & #503688 (MUIRIEL)',\n",
       " \"Time's up.\\tDie Zeit ist um.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6611926 (Eccles17) & #2435909 (Pfirsichbaeumchen)\",\n",
       " 'Tom bowed.\\tTom verneigte sich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #3721790 (CM) & #4942943 (Hans_Adler)',\n",
       " 'Tom cried.\\tTom weinte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111190 (CK) & #1692967 (Pfirsichbaeumchen)',\n",
       " 'Tom dozed.\\tTom döste.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111188 (CK) & #4941663 (Hans_Adler)',\n",
       " 'Tom drove.\\tTom fuhr.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111185 (CK) & #4941662 (Hans_Adler)',\n",
       " 'Tom froze.\\tTom erstarrte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #7367727 (CM) & #7592277 (wolfgangth)',\n",
       " 'Tom is OK.\\tTom ist in Ordnung.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203136 (CK) & #6625484 (Felixjp)',\n",
       " 'Tom is in.\\tTom ist hier.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2236731 (CK) & #4422550 (Dokuyaku)',\n",
       " 'Tom is up.\\tTom ist auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2236907 (CK) & #6625513 (Felixjp)',\n",
       " 'Tom knits.\\tTom strickt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111161 (CK) & #5480530 (RandomUsername)',\n",
       " 'Tom knows.\\tTom weiß es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111160 (CK) & #3933843 (Zaghawa)',\n",
       " 'Tom moved.\\tTom bewegte sich.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005181 (CK) & #4941603 (Hans_Adler)',\n",
       " 'Tom slept.\\tTom schlief.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #6645778 (CK) & #1691611 (Pfirsichbaeumchen)',\n",
       " 'Tom spoke.\\tTom hat gesprochen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203808 (CK) & #2203989 (Pfirsichbaeumchen)',\n",
       " 'Tom spoke.\\tTom sprach.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203808 (CK) & #2203990 (Pfirsichbaeumchen)',\n",
       " 'Tom swims.\\tTom schwimmt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5190879 (CK) & #1713285 (Pfirsichbaeumchen)',\n",
       " 'Tom tried.\\tTom versuchte es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203870 (CK) & #2203951 (Pfirsichbaeumchen)',\n",
       " 'Tom tried.\\tTom hat es versucht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203870 (CK) & #2203952 (Pfirsichbaeumchen)',\n",
       " 'Tom tries.\\tTom versucht es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203871 (CK) & #2203953 (Pfirsichbaeumchen)',\n",
       " 'Tom voted.\\tTom hat gewählt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203886 (CK) & #2203941 (Pfirsichbaeumchen)',\n",
       " 'Tom voted.\\tTom wählte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203886 (CK) & #2203942 (Pfirsichbaeumchen)',\n",
       " 'Tom walks.\\tTom geht zu Fuß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203890 (CK) & #2203910 (Pfirsichbaeumchen)',\n",
       " 'Tom waved.\\tTom winkte.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2005185 (CK) & #4941616 (Hans_Adler)',\n",
       " 'Tom works.\\tTom arbeitet.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203899 (CK) & #2203905 (Pfirsichbaeumchen)',\n",
       " \"Tom'll go.\\tTom wird gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123528 (CK) & #2204233 (Pfirsichbaeumchen)\",\n",
       " \"Tom's fat.\\tTom ist dick.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107510 (CK) & #2122773 (Pfirsichbaeumchen)\",\n",
       " \"Tom's mad.\\tTom ist wütend.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107565 (CK) & #2007986 (Manfredo)\",\n",
       " \"Tom's sad.\\tTom ist traurig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107586 (CK) & #5830115 (Pfirsichbaeumchen)\",\n",
       " \"Tom's shy.\\tTom ist schüchtern.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107593 (CK) & #5830116 (Pfirsichbaeumchen)\",\n",
       " 'Trust Tom.\\tVertraue Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203867 (CK) & #2203984 (Pfirsichbaeumchen)',\n",
       " 'Trust Tom.\\tVertrauen Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203867 (CK) & #2203985 (Pfirsichbaeumchen)',\n",
       " 'Trust Tom.\\tVertraut Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203867 (CK) & #2203986 (Pfirsichbaeumchen)',\n",
       " 'Try again.\\tVersuchen Sie es noch einmal.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #31723 (CK) & #563346 (MUIRIEL)',\n",
       " 'Try again.\\tVersuch es noch einmal.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #31723 (CK) & #1111038 (MUIRIEL)',\n",
       " 'Try again.\\tVersucht es noch mal.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #31723 (CK) & #1313716 (Esperantostern)',\n",
       " 'Try it on.\\tProbier es an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2253793 (CK) & #2929895 (raggione)',\n",
       " 'Try it on.\\tProbier sie an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2253793 (CK) & #2929896 (raggione)',\n",
       " 'Try it on.\\tProbier ihn an!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2253793 (CK) & #2929897 (raggione)',\n",
       " 'Unlock it.\\tSchließ es auf.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5350432 (mailohilohi) & #8227462 (MisterTrouser)',\n",
       " 'Unlock it.\\tEntsperr es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #5350432 (mailohilohi) & #8227463 (MisterTrouser)',\n",
       " 'Watch Tom.\\tSieh Tom zu!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203868 (CK) & #2203945 (Pfirsichbaeumchen)',\n",
       " 'Watch Tom.\\tSehen Sie Tom zu!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203868 (CK) & #2203949 (Pfirsichbaeumchen)',\n",
       " 'Watch Tom.\\tSeht Tom zu!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203868 (CK) & #2203950 (Pfirsichbaeumchen)',\n",
       " 'We agreed.\\tWir waren uns einig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4054938 (CK) & #4942978 (Hans_Adler)',\n",
       " 'We agreed.\\tWir stimmten zu.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4054938 (CK) & #4942979 (Hans_Adler)',\n",
       " 'We can go.\\tWir können gehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2241036 (CK) & #3065754 (freddy1)',\n",
       " 'We saw it.\\tWir sahen es.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2241438 (CK) & #6047639 (wolfgangth)',\n",
       " 'We saw it.\\tWir haben es gesehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2241438 (CK) & #6047640 (wolfgangth)',\n",
       " 'We talked.\\tWir haben uns unterhalten.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107672 (CK) & #2122758 (Pfirsichbaeumchen)',\n",
       " 'We waited.\\tWir warteten.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107674 (CK) & #4941644 (Hans_Adler)',\n",
       " \"We'll see.\\tWir werden sehen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203796 (CK) & #932897 (Fingerhut)\",\n",
       " \"We'll try.\\tWir werden es versuchen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107682 (CK) & #2110886 (Esperantostern)\",\n",
       " \"We'll win.\\tWir werden siegen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2107684 (CK) & #2110893 (Esperantostern)\",\n",
       " \"We're hot.\\tUns ist heiß.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #464466 (lukaszpp) & #989624 (MUIRIEL)\",\n",
       " \"We're sad.\\tWir sind traurig.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203308 (CK) & #7457188 (Luiaard)\",\n",
       " \"We're shy.\\tWir sind schüchtern.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203343 (CK) & #7289957 (Yorwba)\",\n",
       " \"We've won!\\tWir haben gewonnen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2123500 (CK) & #2818822 (Zaghawa)\",\n",
       " \"What's on?\\tWas ist los?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #621482 (Manfredo) & #360789 (Wolf)\",\n",
       " \"What's up?\\tWas ist los?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #25164 (CK) & #360789 (Wolf)\",\n",
       " \"What's up?\\tNa, wie geht's?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #25164 (CK) & #726120 (Esperantostern)\",\n",
       " 'Who cares?\\tWen kümmert’s?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #40434 (CK) & #2205356 (Pfirsichbaeumchen)',\n",
       " 'Who cares?\\tWen kümmert das schon?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #40434 (CK) & #2205357 (Pfirsichbaeumchen)',\n",
       " 'Who is he?\\tWer ist er?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #347331 (sysko) & #362092 (Wolf)',\n",
       " 'Who spoke?\\tWer hat gesprochen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203807 (CK) & #2203987 (Pfirsichbaeumchen)',\n",
       " 'Who spoke?\\tWer sprach?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203807 (CK) & #2203988 (Pfirsichbaeumchen)',\n",
       " 'Who stood?\\tWer stand?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203827 (CK) & #7466812 (Yorwba)',\n",
       " \"Who'll go?\\tWer geht?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203689 (CK) & #2204229 (Pfirsichbaeumchen)\",\n",
       " \"Who'll go?\\tWer wird gehen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203689 (CK) & #2204232 (Pfirsichbaeumchen)\",\n",
       " \"Who's she?\\tWer ist sie?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203797 (CK) & #357121 (MUIRIEL)\",\n",
       " 'Wonderful!\\tWunderbar!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433510 (CK) & #5362918 (RandomUsername)',\n",
       " 'Wonderful!\\tHerrlich!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #433510 (CK) & #5362921 (RandomUsername)',\n",
       " 'Write Tom.\\tSchreibe Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203869 (CK) & #2203946 (Pfirsichbaeumchen)',\n",
       " 'Write Tom.\\tSchreiben Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203869 (CK) & #2203947 (Pfirsichbaeumchen)',\n",
       " 'Write Tom.\\tSchreibt Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203869 (CK) & #2203948 (Pfirsichbaeumchen)',\n",
       " 'You drive.\\tDu fährst.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549830 (CK) & #4942924 (Hans_Adler)',\n",
       " 'You drive.\\tSie fahren.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549830 (CK) & #4942925 (Hans_Adler)',\n",
       " 'You idiot!\\tDu Idiot!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #418475 (Scott) & #438913 (xtofu80)',\n",
       " 'You start.\\tDu fängst an.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2549829 (CK) & #4606355 (kroko)',\n",
       " 'You stink.\\tDu stinkst.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4834656 (shekitten) & #4942993 (Hans_Adler)',\n",
       " 'You stink.\\tIhr stinkt.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4834656 (shekitten) & #4942994 (Hans_Adler)',\n",
       " 'You stink.\\tSie stinken.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4834656 (shekitten) & #4942996 (Hans_Adler)',\n",
       " \"You tried.\\tDu hast's versucht.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #20119 (CK) & #4659635 (AC)\",\n",
       " \"You're OK.\\tDu bist okay.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203137 (CK) & #6626190 (Felixjp)\",\n",
       " 'Aim higher.\\tZiel höher!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2264156 (sharptoothed) & #7466822 (Yorwba)',\n",
       " 'Aim higher.\\tZielen Sie höher!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2264156 (sharptoothed) & #7466823 (Yorwba)',\n",
       " 'Aim higher.\\tZielt höher!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2264156 (sharptoothed) & #7466824 (Yorwba)',\n",
       " 'All aboard!\\tAlle Mann an Bord!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #22121 (CM) & #4659637 (AC)',\n",
       " 'Am I dying?\\tSterbe ich?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2244886 (CK) & #7451506 (Yorwba)',\n",
       " 'Am I early?\\tBin ich früh dran?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2244887 (CK) & #7451507 (Yorwba)',\n",
       " 'Am I fired?\\tBin ich gefeuert?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2244888 (CK) & #7451509 (Yorwba)',\n",
       " 'Am I hired?\\tBin ich eingestellt?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2244890 (CK) & #7451510 (Yorwba)',\n",
       " 'Am I right?\\tHabe ich recht?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1934796 (Spamster) & #1935227 (Pfirsichbaeumchen)',\n",
       " 'Am I right?\\tLiege ich richtig?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1934796 (Spamster) & #1935228 (Pfirsichbaeumchen)',\n",
       " 'Am I weird?\\tBin ich komisch?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #8101800 (Hybrid) & #8244944 (MisterTrouser)',\n",
       " 'Am I wrong?\\tHab ich nicht Recht?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #410779 (CK) & #357734 (MUIRIEL)',\n",
       " 'Am I wrong?\\tIrre ich mich?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #410779 (CK) & #556823 (cost)',\n",
       " 'Answer Tom.\\tAntworte Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203854 (CK) & #2203994 (Pfirsichbaeumchen)',\n",
       " 'Answer Tom.\\tAntworten Sie Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203854 (CK) & #2203995 (Pfirsichbaeumchen)',\n",
       " 'Answer Tom.\\tAntwortet Tom!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2203854 (CK) & #2203996 (Pfirsichbaeumchen)',\n",
       " 'Are you 18?\\tBist du 18?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #4495350 (CK) & #6626383 (Felixjp)',\n",
       " 'Are you OK?\\tGeht es dir gut?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953106 (CK) & #515856 (MUIRIEL)',\n",
       " 'Are you OK?\\tGeht es euch gut?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953106 (CK) & #1324278 (Pfirsichbaeumchen)',\n",
       " 'Are you OK?\\tGeht es Ihnen gut?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #953106 (CK) & #1324279 (al_ex_an_der)',\n",
       " 'Are you in?\\tBist du dabei?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2908782 (CK) & #6626373 (Felixjp)',\n",
       " 'Are you up?\\tBist du auf?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1841739 (CK) & #6626382 (Felixjp)',\n",
       " 'Ask anyone.\\tFrag irgendwen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2092642 (CK) & #2265065 (Vortarulo)',\n",
       " 'Ask around.\\tFrag herum.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111967 (CK) & #7456425 (Yorwba)',\n",
       " 'Ask around.\\tFragen Sie herum.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111967 (CK) & #7456426 (Yorwba)',\n",
       " 'Ask around.\\tFragt herum.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111967 (CK) & #7456427 (Yorwba)',\n",
       " 'Be careful.\\tSei vorsichtig!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1849392 (CK) & #626167 (kolonjano)',\n",
       " 'Be careful.\\tSieh dich vor!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1849392 (CK) & #1849401 (al_ex_an_der)',\n",
       " 'Be careful.\\tGib Obacht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1849392 (CK) & #1849403 (al_ex_an_der)',\n",
       " 'Be careful.\\tGib Acht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1849392 (CK) & #1849415 (al_ex_an_der)',\n",
       " 'Be careful.\\tGib acht!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1849392 (CK) & #1853920 (Pfirsichbaeumchen)',\n",
       " 'Be patient.\\tSei geduldig!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1486456 (Spamster) & #1486457 (Pfirsichbaeumchen)',\n",
       " 'Be serious.\\tBleib mal auf dem Boden der Tatsachen!\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111954 (CK) & #3033265 (pne)',\n",
       " 'Birds sing.\\tVögel singen.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #278174 (CK) & #547372 (Espi)',\n",
       " 'Bring wine.\\tBring Wein.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2111942 (CK) & #3001031 (pne)',\n",
       " 'Can I come?\\tKann ich kommen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1428567 (CK) & #7354443 (Yorwba)',\n",
       " 'Can I come?\\tDarf ich kommen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #1428567 (CK) & #7354445 (Yorwba)',\n",
       " 'Can I help?\\tKann ich helfen?\\tCC-BY 2.0 (France) Attribution: tatoeba.org #266051 (CK) & #450576 (Pfirsichbaeumchen)',\n",
       " ...]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open('a5_deu-eng/deu.txt',\"rb\") as f:\n",
    "    lines0 = f.read()\n",
    "    lines0 = lines0.decode('utf-8')\n",
    "    lines0 = lines0.split('\\n') ############# list #######\n",
    "print(type(lines0))\n",
    "lines0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #9...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Potzdonner!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Donnerwetter!</td>\n",
       "      <td>CC-BY 2.0 (France) Attribution: tatoeba.org #5...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0              1                                                  2\n",
       "0   Hi.         Hallo!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "1   Hi.     Grüß Gott!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "2  Run!          Lauf!  CC-BY 2.0 (France) Attribution: tatoeba.org #9...\n",
       "3  Wow!    Potzdonner!  CC-BY 2.0 (France) Attribution: tatoeba.org #5...\n",
       "4  Wow!  Donnerwetter!  CC-BY 2.0 (France) Attribution: tatoeba.org #5..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lines_PD = pd.DataFrame(lines0)\n",
    "\n",
    "\n",
    "lines_PD = pd.DataFrame([sub.split(\"\\t\") for sub in lines0])\n",
    "lines_PD.head()\n",
    "#lines_csv = lines_PD.tocsv(lines_PD)\n",
    "#lines_PD.to_csv('G://_SIT_homework/CS584A/Assignment5/deu-eng/deu.csv', sep='\\t', encoding='utf-8')\n",
    "#datasetSentences_df = pd.read_csv(\"G://_SIT_homework/CS584A/Assignment5/deu-eng/deu.csv\",error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertliang/opt/anaconda3/lib/python3.7/site-packages/pandas/core/frame.py:4133: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  errors=errors,\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>English_sentences</th>\n",
       "      <th>German_sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Hallo!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hi.</td>\n",
       "      <td>Grüß Gott!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Run!</td>\n",
       "      <td>Lauf!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Potzdonner!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wow!</td>\n",
       "      <td>Donnerwetter!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200515</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand, der deine Herkunft nicht kennt, s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200516</th>\n",
       "      <td>If someone who doesn't know your background sa...</td>\n",
       "      <td>Wenn jemand Fremdes dir sagt, dass du dich wie...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200517</th>\n",
       "      <td>It may be impossible to get a completely error...</td>\n",
       "      <td>Es ist wohl unmöglich, einen vollkommen fehler...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200518</th>\n",
       "      <td>Doubtless there exists in this world precisely...</td>\n",
       "      <td>Ohne Zweifel findet sich auf dieser Welt zu je...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200519</th>\n",
       "      <td></td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200520 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        English_sentences  \\\n",
       "0                                                     Hi.   \n",
       "1                                                     Hi.   \n",
       "2                                                    Run!   \n",
       "3                                                    Wow!   \n",
       "4                                                    Wow!   \n",
       "...                                                   ...   \n",
       "200515  If someone who doesn't know your background sa...   \n",
       "200516  If someone who doesn't know your background sa...   \n",
       "200517  It may be impossible to get a completely error...   \n",
       "200518  Doubtless there exists in this world precisely...   \n",
       "200519                                                      \n",
       "\n",
       "                                         German_sentences  \n",
       "0                                                  Hallo!  \n",
       "1                                              Grüß Gott!  \n",
       "2                                                   Lauf!  \n",
       "3                                             Potzdonner!  \n",
       "4                                           Donnerwetter!  \n",
       "...                                                   ...  \n",
       "200515  Wenn jemand, der deine Herkunft nicht kennt, s...  \n",
       "200516  Wenn jemand Fremdes dir sagt, dass du dich wie...  \n",
       "200517  Es ist wohl unmöglich, einen vollkommen fehler...  \n",
       "200518  Ohne Zweifel findet sich auf dieser Welt zu je...  \n",
       "200519                                               None  \n",
       "\n",
       "[200520 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eng_Ger_trans_df = lines_PD[[0,1]]\n",
    "Eng_Ger_trans_df.rename(columns={0:'English_sentences',1:'German_sentences',}, inplace=True)\n",
    "Eng_Ger_trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "Eng_Ger_trans_ar = Eng_Ger_trans_df.values ### Convert to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['Hi.', 'Hallo!'],\n",
       "       ['Hi.', 'Grüß Gott!'],\n",
       "       ['Run!', 'Lauf!'],\n",
       "       ...,\n",
       "       ['It may be impossible to get a completely error-free corpus due to the nature of this kind of collaborative effort. However, if we encourage members to contribute sentences in their own languages rather than experiment in languages they are learning, we might be able to minimize errors.',\n",
       "        'Es ist wohl unmöglich, einen vollkommen fehlerfreien Korpus zu erreichen\\xa0— das liegt in der Natur eines solchen Gemeinschaftsprojekts. Doch wenn wir unsere Mitglieder dazu bringen können, nicht mit Sprachen herumzuexperimentieren, die sie gerade lernen, sondern Sätze in ihrer eigenen Muttersprache beizutragen, dann gelingt es uns vielleicht, die Zahl der Fehler klein zu halten.'],\n",
       "       ['Doubtless there exists in this world precisely the right woman for any given man to marry and vice versa; but when you consider that a human being has the opportunity of being acquainted with only a few hundred people, and out of the few hundred that there are but a dozen or less whom he knows intimately, and out of the dozen, one or two friends at most, it will easily be seen, when we remember the number of millions who inhabit this world, that probably, since the earth was created, the right man has never yet met the right woman.',\n",
       "        'Ohne Zweifel findet sich auf dieser Welt zu jedem Mann genau die richtige Ehefrau und umgekehrt; wenn man jedoch in Betracht zieht, dass ein Mensch nur Gelegenheit hat, mit ein paar hundert anderen bekannt zu sein, von denen ihm nur ein Dutzend oder weniger nahesteht, darunter höchstens ein oder zwei Freunde, dann erahnt man eingedenk der Millionen Einwohner dieser Welt\\xa0leicht, dass seit Erschaffung ebenderselben wohl noch nie der richtige Mann der richtigen Frau begegnet ist.'],\n",
       "       ['', None]], dtype=object)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eng_Ger_trans_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i am a human i come from earth\n"
     ]
    }
   ],
   "source": [
    "from string import *\n",
    "import os\n",
    "import string\n",
    "\n",
    "#from nltk.corpus import PlaintextCorpusReader\n",
    "from nltk.corpus import state_union\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "from nltk import sent_tokenize,word_tokenize\n",
    "\n",
    "def data_clean(text):\n",
    "    text=str(text)\n",
    "    text = text.lower()\n",
    "    text_clean = text.translate(text.maketrans(' ',' ', string.punctuation))\n",
    "    \n",
    "#descriptor 'translate' requires a 'str' object but received a 'dict'\n",
    "\n",
    "\n",
    "    return text_clean\n",
    "k = '\"I am a human, I come from earth.\"'\n",
    "\n",
    "print (data_clean(k))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove punctuation\n",
    "Eng_Ger_trans_ar[:,0] = [data_clean(s) for s in Eng_Ger_trans_ar[:,0]]   ###### German sentences\n",
    "Eng_Ger_trans_ar[:,1] = [data_clean(s) for s in Eng_Ger_trans_ar[:,1]]   ###### English sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eng_Ger_trans_ar = Eng_Ger_trans_ar[:1000,:]            \n",
    "len(Eng_Ger_trans_ar)\n",
    "############# I choose 1000 sentences because it cost less time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# empty lists\n",
    "eng_l = []\n",
    "ger_l = []\n",
    "############## just calculate the length of all this phrases\n",
    "# populate the lists with sentence lengths\n",
    "for i in Eng_Ger_trans_ar[:,0]:\n",
    "    eng_l.append(len(i.split()))\n",
    "\n",
    "for i in Eng_Ger_trans_ar[:,1]:\n",
    "    ger_l.append(len(i.split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ plot the length of all the phrases\n",
    "import matplotlib.pyplot as plt \n",
    "#matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "length_df = pd.DataFrame({'eng':eng_l, 'ger':ger_l})\n",
    "length_df.hist(bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "############ plot the length of all the phrases\n",
    "import matplotlib.pyplot as plt \n",
    "#matplotlib inline\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "\n",
    "length_df = pd.DataFrame({'eng':eng_l, 'ger':ger_l})\n",
    "length_df.hist(bins = 20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n",
      "3\n"
     ]
    }
   ],
   "source": [
    "########### Get the longest length\n",
    "max_length_ger = max(length_df['ger'])\n",
    "max_length_eng = max(length_df['eng'])\n",
    "print(max_length_ger)\n",
    "print(max_length_eng)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Tokenizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Corpus Size: 383\n",
      "Germany Corpus Size: 704\n"
     ]
    }
   ],
   "source": [
    "#Tokenizer to make each phrase has key_vakue(sequence) format\n",
    "from nltk import word_tokenize\n",
    "from tensorflow.python.keras.preprocessing.text import Tokenizer\n",
    "########### Word Tokenizer to word_index ###############\n",
    "def tokenizer_word_index(txt):\n",
    "    tokenizer_ = Tokenizer()\n",
    "    s = tokenizer_.fit_on_texts(txt)  ############ None type, output none #################\n",
    "    return tokenizer_.word_index  ########### We should make tokenizer at first\n",
    "Eng_windex_tokenizer = tokenizer_word_index(Eng_Ger_trans_ar[:,0])   ###### Make sequence to each word ##\n",
    "Ger_windex_tokenizer = tokenizer_word_index(Eng_Ger_trans_ar[:,1])\n",
    "eng_corpus_size = len(Eng_windex_tokenizer) + 1\n",
    "ger_corpus_size = len(Ger_windex_tokenizer) + 1\n",
    "\n",
    "print('English Corpus Size: %d' % eng_corpus_size)\n",
    "print('Germany Corpus Size: %d' % ger_corpus_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ich': 1,\n",
       " 'es': 2,\n",
       " 'tom': 3,\n",
       " 'sie': 4,\n",
       " 'bin': 5,\n",
       " 'ist': 6,\n",
       " 'dich': 7,\n",
       " 'habe': 8,\n",
       " 'auf': 9,\n",
       " 'mir': 10,\n",
       " 'das': 11,\n",
       " 'mach': 12,\n",
       " 'wir': 13,\n",
       " 'uns': 14,\n",
       " 'zu': 15,\n",
       " 'geht': 16,\n",
       " 'nicht': 17,\n",
       " 'wer': 18,\n",
       " 'er': 19,\n",
       " 'gehen': 20,\n",
       " 'ab': 21,\n",
       " 'an': 22,\n",
       " 'gut': 23,\n",
       " 'weg': 24,\n",
       " 'mich': 25,\n",
       " 'werde': 26,\n",
       " 'geh': 27,\n",
       " 'hat': 28,\n",
       " 'du': 29,\n",
       " 'hier': 30,\n",
       " 'komm': 31,\n",
       " 'ihn': 32,\n",
       " 'mal': 33,\n",
       " 'lass': 34,\n",
       " 'in': 35,\n",
       " 'verloren': 36,\n",
       " 'haben': 37,\n",
       " 'fort': 38,\n",
       " 'scher': 39,\n",
       " 'kann': 40,\n",
       " 'bleib': 41,\n",
       " 'nimm': 42,\n",
       " 'setz': 43,\n",
       " 'gewonnen': 44,\n",
       " 'zieh': 45,\n",
       " 'die': 46,\n",
       " 'jetzt': 47,\n",
       " 'dir': 48,\n",
       " 'bleiben': 49,\n",
       " 'sei': 50,\n",
       " 'sich': 51,\n",
       " 'versucht': 52,\n",
       " 'meine': 53,\n",
       " 'versuchen': 54,\n",
       " 'hör': 55,\n",
       " 'wie': 56,\n",
       " 'gesehen': 57,\n",
       " 'fiel': 58,\n",
       " 'gehts': 59,\n",
       " 'ernst': 60,\n",
       " 'frag': 61,\n",
       " 'hau': 62,\n",
       " 'verschwinde': 63,\n",
       " 'verdufte': 64,\n",
       " 'leine': 65,\n",
       " 'vom': 66,\n",
       " 'acker': 67,\n",
       " 'verzieh': 68,\n",
       " 'verkrümele': 69,\n",
       " 'troll': 70,\n",
       " 'zisch': 71,\n",
       " 'pack': 72,\n",
       " '’ne': 73,\n",
       " 'fliege': 74,\n",
       " 'schwirr': 75,\n",
       " 'sause': 76,\n",
       " 'nehmen': 77,\n",
       " 'ein': 78,\n",
       " 'euch': 79,\n",
       " 'runter': 80,\n",
       " 'kommen': 81,\n",
       " 'was': 82,\n",
       " 'heiß': 83,\n",
       " 'schau': 84,\n",
       " 'sind': 85,\n",
       " 'ruhig': 86,\n",
       " 'hallo': 87,\n",
       " 'rannte': 88,\n",
       " 'wach': 89,\n",
       " 'doch': 90,\n",
       " 'herein': 91,\n",
       " 'schon': 92,\n",
       " 'raus': 93,\n",
       " 'nach': 94,\n",
       " 'kam': 95,\n",
       " 'kurz': 96,\n",
       " 'weinte': 97,\n",
       " 'gehe': 98,\n",
       " 'dick': 99,\n",
       " 'wurde': 100,\n",
       " 'neu': 101,\n",
       " 'traurig': 102,\n",
       " 'schüchtern': 103,\n",
       " 'ordnung': 104,\n",
       " 'behalt': 105,\n",
       " 'behalten': 106,\n",
       " 'behaltet': 107,\n",
       " 'nehmt': 108,\n",
       " 'gefallen': 109,\n",
       " 'tun': 110,\n",
       " 'komme': 111,\n",
       " 'zurück': 112,\n",
       " 'spät': 113,\n",
       " 'sicher': 114,\n",
       " 'lasst': 115,\n",
       " 'probier': 116,\n",
       " 'wusste': 117,\n",
       " 'werden': 118,\n",
       " 'recht': 119,\n",
       " 'meins': 120,\n",
       " 'bleibt': 121,\n",
       " 'bist': 122,\n",
       " 'verstehe': 123,\n",
       " 'einverstanden': 124,\n",
       " 'drück': 125,\n",
       " 'den': 126,\n",
       " 'alt': 127,\n",
       " 'kommt': 128,\n",
       " 'ruf': 129,\n",
       " 'verpiss': 130,\n",
       " 'heim': 131,\n",
       " 'hilf': 132,\n",
       " 'helfen': 133,\n",
       " 'warten': 134,\n",
       " 'getroffen': 135,\n",
       " 'schließ': 136,\n",
       " 'dran': 137,\n",
       " 'sehen': 138,\n",
       " 'mann': 139,\n",
       " 'tapfer': 140,\n",
       " 'fass': 141,\n",
       " 'spaß': 142,\n",
       " 'sprach': 143,\n",
       " 'dünn': 144,\n",
       " 'tut': 145,\n",
       " 'lassen': 146,\n",
       " 'halte': 147,\n",
       " 'halten': 148,\n",
       " 'haltet': 149,\n",
       " 'sag': 150,\n",
       " 'davon': 151,\n",
       " 'ihr': 152,\n",
       " 'beruhige': 153,\n",
       " 'her': 154,\n",
       " 'bald': 155,\n",
       " 'entschuldigung': 156,\n",
       " 'wird': 157,\n",
       " 'halt': 158,\n",
       " 'fuß': 159,\n",
       " 'uhr': 160,\n",
       " 'warm': 161,\n",
       " 'still': 162,\n",
       " 'na': 163,\n",
       " 'bitte': 164,\n",
       " 'unten': 165,\n",
       " 'meinen': 166,\n",
       " 'noch': 167,\n",
       " 'höher': 168,\n",
       " 'herum': 169,\n",
       " 'gib': 170,\n",
       " 'lauf': 171,\n",
       " 'feuer': 172,\n",
       " 'stopp': 173,\n",
       " 'warte': 174,\n",
       " 'weiter': 175,\n",
       " 'hab': 176,\n",
       " 'wohl': 177,\n",
       " 'verstanden': 178,\n",
       " 'mit': 179,\n",
       " 'umarme': 180,\n",
       " 'hin': 181,\n",
       " 'weiß': 182,\n",
       " 'gelogen': 183,\n",
       " '19': 184,\n",
       " 'wirklich': 185,\n",
       " 'echt': 186,\n",
       " 'im': 187,\n",
       " 'danke': 188,\n",
       " 'fragen': 189,\n",
       " 'fragt': 190,\n",
       " 'fantastisch': 191,\n",
       " 'entspann': 192,\n",
       " 'nett': 193,\n",
       " 'seien': 194,\n",
       " 'hol': 195,\n",
       " 'hause': 196,\n",
       " 'rennt': 197,\n",
       " 'helft': 198,\n",
       " 'fett': 199,\n",
       " 'bins': 200,\n",
       " 'küsst': 201,\n",
       " 'drauf': 202,\n",
       " 'aß': 203,\n",
       " 'gegessen': 204,\n",
       " 'gerannt': 205,\n",
       " 'wachen': 206,\n",
       " 'laufen': 207,\n",
       " 'hast': 208,\n",
       " 'fassen': 209,\n",
       " 'fasst': 210,\n",
       " 'rufen': 211,\n",
       " 'ihm': 212,\n",
       " 'handschellen': 213,\n",
       " 'ernsthaft': 214,\n",
       " 'nur': 215,\n",
       " 'viel': 216,\n",
       " 'schön': 217,\n",
       " 'süß': 218,\n",
       " 'tu': 219,\n",
       " 'beeil': 220,\n",
       " 'geschafft': 221,\n",
       " 'habs': 222,\n",
       " 'gemacht': 223,\n",
       " 'klar': 224,\n",
       " 'lächelte': 225,\n",
       " 'wieder': 226,\n",
       " 'da': 227,\n",
       " 'beschäftigt': 228,\n",
       " 'gerecht': 229,\n",
       " 'frei': 230,\n",
       " 'dabei': 231,\n",
       " 'der': 232,\n",
       " 'reich': 233,\n",
       " 'krank': 234,\n",
       " 'groß': 235,\n",
       " 'schwach': 236,\n",
       " 'funktioniert': 237,\n",
       " 'allein': 238,\n",
       " 'losgehen': 239,\n",
       " 'darf': 240,\n",
       " 'log': 241,\n",
       " 'stehen': 242,\n",
       " 'wunderbar': 243,\n",
       " 'gekommen': 244,\n",
       " 'gestorben': 245,\n",
       " 'aufgehört': 246,\n",
       " 'schwamm': 247,\n",
       " 'geschwommen': 248,\n",
       " 'vertraue': 249,\n",
       " 'vertraut': 250,\n",
       " 'vertrauen': 251,\n",
       " 'versuch': 252,\n",
       " 'richtig': 253,\n",
       " 'schauen': 254,\n",
       " 'schreiben': 255,\n",
       " 'schreibt': 256,\n",
       " 'zielen': 257,\n",
       " 'antworten': 258,\n",
       " 'vögel': 259,\n",
       " 'fliegen': 260,\n",
       " 'zuhause': 261,\n",
       " 'beruhigen': 262,\n",
       " 'essen': 263,\n",
       " 'können': 264,\n",
       " 'fang': 265,\n",
       " 'hierher': 266,\n",
       " 'weint': 267,\n",
       " 'weinen': 268,\n",
       " 'lüge': 269,\n",
       " 'leid': 270,\n",
       " 'vergiss': 271,\n",
       " 'rein': 272,\n",
       " 'schlafen': 273,\n",
       " 'ins': 274,\n",
       " 'bett': 275,\n",
       " 'seinen': 276,\n",
       " 'willen': 277,\n",
       " 'fahren': 278,\n",
       " 'wütend': 279,\n",
       " 'hoffe': 280,\n",
       " 'liebe': 281,\n",
       " 'so': 282,\n",
       " 'war': 283,\n",
       " 'würde': 284,\n",
       " 'aufhören': 285,\n",
       " 'singen': 286,\n",
       " 'früh': 287,\n",
       " 'soweit': 288,\n",
       " 'weit': 289,\n",
       " 'müde': 290,\n",
       " 'gar': 291,\n",
       " 'okay': 292,\n",
       " 'einfach': 293,\n",
       " 'gehört': 294,\n",
       " 'arbeit': 295,\n",
       " 'und': 296,\n",
       " 'unterschreiben': 297,\n",
       " 'cool': 298,\n",
       " 'damit': 299,\n",
       " 'gesprochen': 300,\n",
       " 'einmal': 301,\n",
       " 'sieh': 302,\n",
       " 'los': 303,\n",
       " 'wen': 304,\n",
       " 'acht': 305,\n",
       " 'grüß': 306,\n",
       " 'gott': 307,\n",
       " 'potzdonner': 308,\n",
       " 'donnerwetter': 309,\n",
       " 'hilfe': 310,\n",
       " 'hülf': 311,\n",
       " 'aha': 312,\n",
       " 'probiere': 313,\n",
       " 'lächeln': 314,\n",
       " 'zum': 315,\n",
       " 'iss': 316,\n",
       " 'keine': 317,\n",
       " 'bewegung': 318,\n",
       " 'stehenbleiben': 319,\n",
       " 'kapiert': 320,\n",
       " 'lief': 321,\n",
       " 'arm': 322,\n",
       " 'stürzte': 323,\n",
       " 'hingefallen': 324,\n",
       " 'gestürzt': 325,\n",
       " 'bezahlt': 326,\n",
       " 'zahlte': 327,\n",
       " 'sang': 328,\n",
       " 'schwimme': 329,\n",
       " 'jahre': 330,\n",
       " 'unmöglich': 331,\n",
       " 'frage': 332,\n",
       " 'gibt’s': 333,\n",
       " 'ausgeschlossen': 334,\n",
       " 'keinster': 335,\n",
       " 'weise': 336,\n",
       " 'versuch’s': 337,\n",
       " 'warum': 338,\n",
       " 'ungerecht': 339,\n",
       " 'fair': 340,\n",
       " 'macht': 341,\n",
       " 'wiedersehen': 342,\n",
       " 'leb': 343,\n",
       " 'tschüss': 344,\n",
       " 'nachlassen': 345,\n",
       " 'läuft': 346,\n",
       " 'schlage': 347,\n",
       " 'schlagt': 348,\n",
       " 'schlagen': 349,\n",
       " 'umarmen': 350,\n",
       " 'umarmt': 351,\n",
       " 'drückt': 352,\n",
       " 'drücken': 353,\n",
       " 'geweint': 354,\n",
       " 'schnarche': 355,\n",
       " 'schlecht': 356,\n",
       " 'nass': 357,\n",
       " 'schließe': 358,\n",
       " 'perfekt': 359,\n",
       " 'zeigs': 360,\n",
       " 'halts': 361,\n",
       " 'maul': 362,\n",
       " 'bis': 363,\n",
       " 'später': 364,\n",
       " 'wartet': 365,\n",
       " 'willkommen': 366,\n",
       " 'läufst': 367,\n",
       " 'näher': 368,\n",
       " 'seid': 369,\n",
       " 'rufe': 370,\n",
       " 'ruft': 371,\n",
       " 'kopf': 372,\n",
       " 'hoch': 373,\n",
       " 'reg': 374,\n",
       " 'leg': 375,\n",
       " 'legen': 376,\n",
       " 'finde': 377,\n",
       " 'findet': 378,\n",
       " 'finden': 379,\n",
       " 'beheben': 380,\n",
       " 'behebe': 381,\n",
       " 'repariere': 382,\n",
       " 'reparieren': 383,\n",
       " 'hinlegen': 384,\n",
       " 'deckung': 385,\n",
       " 'greif': 386,\n",
       " 'vergnügen': 387,\n",
       " 'feier': 388,\n",
       " 'tief': 389,\n",
       " 'zurecht': 390,\n",
       " 'über': 391,\n",
       " 'runden': 392,\n",
       " 'bekommen': 393,\n",
       " 'half': 394,\n",
       " 'geholfen': 395,\n",
       " 'gesprungen': 396,\n",
       " 'weigere': 397,\n",
       " 'trete': 398,\n",
       " 'rasierte': 399,\n",
       " 'rasiert': 400,\n",
       " 'blieb': 401,\n",
       " 'dageblieben': 402,\n",
       " 'benutze': 403,\n",
       " 'gewartet': 404,\n",
       " 'gähnte': 405,\n",
       " 'gegähnt': 406,\n",
       " 'zahlen': 407,\n",
       " 'glatzköpfig': 408,\n",
       " 'eine': 409,\n",
       " 'glatze': 410,\n",
       " 'taub': 411,\n",
       " 'satt': 412,\n",
       " 'mache': 413,\n",
       " 'verirrt': 414,\n",
       " 'gemein': 415,\n",
       " 'nächste': 416,\n",
       " 'ordentlich': 417,\n",
       " 'hässlich': 418,\n",
       " 'hilft': 419,\n",
       " 'weh': 420,\n",
       " 'schmerzt': 421,\n",
       " 'klappt': 422,\n",
       " 'seins': 423,\n",
       " 'eintritt': 424,\n",
       " 'verboten': 425,\n",
       " 'kein': 426,\n",
       " 'zutritt': 427,\n",
       " 'küsse': 428,\n",
       " 'küssen': 429,\n",
       " 'vorsicht': 430,\n",
       " 'heirate': 431,\n",
       " 'rette': 432,\n",
       " 'rettet': 433,\n",
       " 'retten': 434,\n",
       " 'setzen': 435,\n",
       " 'sprich': 436,\n",
       " 'lauter': 437,\n",
       " 'steht': 438,\n",
       " 'stehe': 439,\n",
       " 'sagen': 440,\n",
       " 'erzähl': 441,\n",
       " 'erzählen': 442,\n",
       " 'sagt': 443,\n",
       " 'hervorragend': 444,\n",
       " 'sagenhaft': 445,\n",
       " 'starb': 446,\n",
       " 'gewusst': 447,\n",
       " 'bescheid': 448,\n",
       " 'gegangen': 449,\n",
       " 'ging': 450,\n",
       " 'lügt': 451,\n",
       " 'gezahlt': 452,\n",
       " 'warnen': 453,\n",
       " 'warne': 454,\n",
       " 'beobachte': 455,\n",
       " 'beobachten': 456,\n",
       " 'schaut': 457,\n",
       " 'beobachtet': 458,\n",
       " 'stimmen': 459,\n",
       " 'versuchten': 460,\n",
       " 'wozu': 461,\n",
       " 'für': 462,\n",
       " 'ausgeschieden': 463,\n",
       " 'schreib': 464,\n",
       " 'habt': 465,\n",
       " 'gesundheit': 466,\n",
       " 'fange': 467,\n",
       " 'fangen': 468,\n",
       " 'fangt': 469,\n",
       " 'hunde': 470,\n",
       " 'bellen': 471,\n",
       " 'weine': 472,\n",
       " 'stirb': 473,\n",
       " 'entschuldigen': 474,\n",
       " 'ganz': 475,\n",
       " 'toll': 476,\n",
       " 'fühl': 477,\n",
       " 'folge': 478,\n",
       " 'daraus': 479,\n",
       " 'nichts': 480,\n",
       " 'kannst': 481,\n",
       " 'knicken': 482,\n",
       " 'tue': 483,\n",
       " 'hols': 484,\n",
       " 'legt': 485,\n",
       " 'hände': 486,\n",
       " 'dj': 487,\n",
       " 'plattenaufleger': 488,\n",
       " 'schnell': 489,\n",
       " 'träge': 490,\n",
       " 'fünf': 491,\n",
       " 'dollar': 492,\n",
       " 'schießen': 493,\n",
       " 'schrecklich': 494,\n",
       " 'lage': 495,\n",
       " 'seltsam': 496,\n",
       " 'rennen': 497,\n",
       " 'ski': 498,\n",
       " 'gewinnen': 499,\n",
       " 'geschummelt': 500,\n",
       " 'ohnmächtig': 501,\n",
       " 'ohnmacht': 502,\n",
       " 'fürchte': 503,\n",
       " 'ja': 504,\n",
       " 'aufgegeben': 505,\n",
       " 'gab': 506,\n",
       " 'meinst': 507,\n",
       " 'kicherte': 508,\n",
       " 'gebe': 509,\n",
       " 'füge': 510,\n",
       " 'böse': 511,\n",
       " 'hatte': 512,\n",
       " 'amüsiert': 513,\n",
       " 'geschlagen': 514,\n",
       " 'aufgelegt': 515,\n",
       " 'lachte': 516,\n",
       " 'gefällt': 517,\n",
       " 'vielleicht': 518,\n",
       " 'gewinne': 519,\n",
       " 'gemeint': 520,\n",
       " 'von': 521,\n",
       " 'traf': 522,\n",
       " 'begegnet': 523,\n",
       " 'vermisse': 524,\n",
       " 'fehlt': 525,\n",
       " 'versprechs': 526,\n",
       " 'verspreche': 527,\n",
       " 'sagte': 528,\n",
       " 'nein': 529,\n",
       " 'einen': 530,\n",
       " 'sehe': 531,\n",
       " 'nahm': 532,\n",
       " 'genommen': 533,\n",
       " 'möchte': 534,\n",
       " 'will': 535,\n",
       " 'wachte': 536,\n",
       " 'anrufen': 537,\n",
       " 'kochen': 538,\n",
       " 'helfe': 539,\n",
       " 'leben': 540,\n",
       " 'gehorchen': 541,\n",
       " 'abbrechen': 542,\n",
       " 'reden': 543,\n",
       " 'sauer': 544,\n",
       " 'blind': 545,\n",
       " 'gelangweilt': 546,\n",
       " 'langweilig': 547,\n",
       " 'knapp': 548,\n",
       " 'bei': 549,\n",
       " 'kasse': 550,\n",
       " 'pleite': 551,\n",
       " 'geheilt': 552,\n",
       " 'betrunken': 553,\n",
       " 'blau': 554,\n",
       " 'sterben': 555,\n",
       " 'erster': 556,\n",
       " 'erste': 557,\n",
       " 'glücklich': 558,\n",
       " 'froh': 559,\n",
       " 'bedürftig': 560,\n",
       " 'nüchtern': 561,\n",
       " 'stecke': 562,\n",
       " 'fest': 563,\n",
       " 'zäh': 564,\n",
       " 'verärgert': 565,\n",
       " 'bestürzt': 566,\n",
       " 'gehöre': 567,\n",
       " 'beachte': 568,\n",
       " 'beachten': 569,\n",
       " 'schlimm': 570,\n",
       " 'brannte': 571,\n",
       " 'verbrannte': 572,\n",
       " 'geschneit': 573,\n",
       " 'geklappt': 574,\n",
       " 'halb': 575,\n",
       " 'vier': 576,\n",
       " '745': 577,\n",
       " 'sieben': 578,\n",
       " 'fünfundvierzig': 579,\n",
       " '830': 580,\n",
       " 'viertel': 581,\n",
       " 'neun': 582,\n",
       " '915\\xa0uhr': 583,\n",
       " 'fernseher': 584,\n",
       " 'kalt': 585,\n",
       " 'kühl': 586,\n",
       " 'dunkel': 587,\n",
       " 'hart': 588,\n",
       " 'geöffnet': 589,\n",
       " 'unsers': 590,\n",
       " 'vorbei': 591,\n",
       " 'sand': 592,\n",
       " 'wahr': 593,\n",
       " 'spring': 594,\n",
       " 'gelassen': 595,\n",
       " 'behalt’s': 596,\n",
       " 'behalte': 597,\n",
       " 'verlasse': 598,\n",
       " 'verlassen': 599,\n",
       " 'sein': 600,\n",
       " 'ruhe': 601,\n",
       " 'etwas': 602,\n",
       " 'probieren': 603,\n",
       " 'lieg': 604,\n",
       " 'beweg': 605,\n",
       " 'hinten': 606,\n",
       " 'guck': 607,\n",
       " 'dreh': 608,\n",
       " 'locker': 609,\n",
       " 'rutsch': 610,\n",
       " 'stück': 611,\n",
       " 'guter': 612,\n",
       " 'schuss': 613,\n",
       " 'natürlich': 614,\n",
       " 'selbstverständlich': 615,\n",
       " 'jeden': 616,\n",
       " 'fall': 617,\n",
       " 'aber': 618,\n",
       " 'lies': 619,\n",
       " '„ah“': 620,\n",
       " 'siehe': 621,\n",
       " 'hats': 622,\n",
       " 'unterschreibe': 623,\n",
       " 'unterschreib': 624,\n",
       " 'sitz': 625,\n",
       " 'sitzen': 626,\n",
       " 'dorthin': 627,\n",
       " 'harre': 628,\n",
       " 'aus': 629,\n",
       " 'beginnen': 630,\n",
       " 'tritt': 631,\n",
       " 'stoppen': 632,\n",
       " 'pass': 633,\n",
       " 'machs': 634,\n",
       " 'übernimm': 635,\n",
       " 'übernehmt': 636,\n",
       " 'übernehmen': 637,\n",
       " 'ok': 638,\n",
       " 'wars': 639,\n",
       " 'fertig': 640,\n",
       " 'fielen': 641,\n",
       " 'stürzten': 642,\n",
       " 'schwammen': 643,\n",
       " 'zeit': 644,\n",
       " 'um': 645,\n",
       " 'verneigte': 646,\n",
       " 'döste': 647,\n",
       " 'fuhr': 648,\n",
       " 'erstarrte': 649,\n",
       " 'strickt': 650,\n",
       " 'bewegte': 651,\n",
       " 'schlief': 652,\n",
       " 'schwimmt': 653,\n",
       " 'versuchte': 654,\n",
       " 'gewählt': 655,\n",
       " 'wählte': 656,\n",
       " 'winkte': 657,\n",
       " 'arbeitet': 658,\n",
       " 'entsperr': 659,\n",
       " 'seht': 660,\n",
       " 'waren': 661,\n",
       " 'einig': 662,\n",
       " 'stimmten': 663,\n",
       " 'sahen': 664,\n",
       " 'unterhalten': 665,\n",
       " 'warteten': 666,\n",
       " 'siegen': 667,\n",
       " 'kümmert’s': 668,\n",
       " 'kümmert': 669,\n",
       " 'stand': 670,\n",
       " 'herrlich': 671,\n",
       " 'schreibe': 672,\n",
       " 'fährst': 673,\n",
       " 'idiot': 674,\n",
       " 'fängst': 675,\n",
       " 'stinkst': 676,\n",
       " 'stinkt': 677,\n",
       " 'stinken': 678,\n",
       " 'hasts': 679,\n",
       " 'ziel': 680,\n",
       " 'zielt': 681,\n",
       " 'alle': 682,\n",
       " 'bord': 683,\n",
       " 'sterbe': 684,\n",
       " 'gefeuert': 685,\n",
       " 'eingestellt': 686,\n",
       " 'liege': 687,\n",
       " 'komisch': 688,\n",
       " 'irre': 689,\n",
       " 'antworte': 690,\n",
       " 'antwortet': 691,\n",
       " '18': 692,\n",
       " 'ihnen': 693,\n",
       " 'irgendwen': 694,\n",
       " 'vorsichtig': 695,\n",
       " 'vor': 696,\n",
       " 'obacht': 697,\n",
       " 'geduldig': 698,\n",
       " 'dem': 699,\n",
       " 'boden': 700,\n",
       " 'tatsachen': 701,\n",
       " 'bring': 702,\n",
       " 'wein': 703}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Ger_windex_tokenizer\n",
    "#Eng_Ger_trans_ar[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequence "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Padding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "def pad(x, max_length):\n",
    "    \"\"\"\n",
    "    Pad x\n",
    "    :param x: List of sequences.\n",
    "    :param length: Length to pad the sequence to.  If None, use length of longest sequence in x.\n",
    "    :return: Padded numpy array of sequences\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    return pad_sequences(x,max_length , padding='post')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to build a tokenizer\n",
    "def tokenization(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English Vocabulary Size: 383\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "eng_tokenizer = tokenization(Eng_Ger_trans_ar[:, 0])\n",
    "eng_vocab_size = len(eng_tokenizer.word_index) + 1\n",
    "#eng_length = 8\n",
    "print('English Vocabulary Size: %d' % eng_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Germany Vocabulary Size: 704\n"
     ]
    }
   ],
   "source": [
    "# prepare english tokenizer\n",
    "ger_tokenizer = tokenization(Eng_Ger_trans_ar[:, 1])\n",
    "ger_vocab_size = len(ger_tokenizer.word_index) + 1\n",
    "\n",
    "#eng_length = 8\n",
    "print('Germany Vocabulary Size: %d' % ger_vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode and pad sequences\n",
    "#from tensorflow.python.keras.preprocessing.sequence import pad_sequences\n",
    "#def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    #seq = tokenizer.texts_to_sequences(lines)\n",
    "    \n",
    "    # pad sequences with 0 values\n",
    "    #seqq = pad(seq, length)\n",
    "    #return seq\n",
    "def encode_sequences(tokenizer, length, lines):\n",
    "    # integer encode sequences\n",
    "    seq = tokenizer.texts_to_sequences(lines)\n",
    "    # pad sequences with 0 values\n",
    "    #seq = pad_sequences(seq, maxlen=length, padding='post')\n",
    "    seq = pad(seq, length)\n",
    "    return seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(Eng_Ger_trans_ar, test_size=0.2, random_state = 2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    " #train_test_split?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training data\n",
    "trainX = encode_sequences(ger_tokenizer, max_length_ger, train[:, 1])\n",
    "trainY = encode_sequences(eng_tokenizer, max_length_eng, train[:, 0])\n",
    "# prepare validation data\n",
    "testX = encode_sequences(ger_tokenizer, max_length_ger, test[:, 1])\n",
    "testY = encode_sequences(eng_tokenizer, max_length_eng, test[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  1,   5, 114, ...,   0,   0,   0],\n",
       "       [  2, 145,  10, ...,   0,   0,   0],\n",
       "       [  8,   1, 119, ...,   0,   0,   0],\n",
       "       ...,\n",
       "       [352,   3,   0, ...,   0,   0,   0],\n",
       "       [  1,   5, 418, ...,   0,   0,   0],\n",
       "       [ 27,  24,   0, ...,   0,   0,   0]], dtype=int32)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['hi', 'hallo'],\n",
       "       ['hi', 'grüß gott'],\n",
       "       ['run', 'lauf'],\n",
       "       ...,\n",
       "       ['can i come', 'kann ich kommen'],\n",
       "       ['can i come', 'darf ich kommen'],\n",
       "       ['can i help', 'kann ich helfen']], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Eng_Ger_trans_ar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Invalid keyword argument(s) in `compile`: {'lr'}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-b241bda712a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#rms = optimizers.RMSprop(lr=0.001)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m#model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mmodel_attention\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sparse_categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'accuracy'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, optimizer, loss, metrics, loss_weights, weighted_metrics, run_eagerly, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'compile'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 538\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_compile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmetrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    539\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_eagerly\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_eagerly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    540\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_validate_compile\u001b[0;34m(self, optimizer, metrics, **kwargs)\u001b[0m\n\u001b[1;32m   2495\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0minvalid_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2496\u001b[0m       raise TypeError('Invalid keyword argument(s) in `compile`: %s' %\n\u001b[0;32m-> 2497\u001b[0;31m                       (invalid_kwargs,))\n\u001b[0m\u001b[1;32m   2498\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2499\u001b[0m     \u001b[0;31m# Model must be created and compiled with the same DistStrat.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Invalid keyword argument(s) in `compile`: {'lr'}"
     ]
    }
   ],
   "source": [
    "#import tensorflow \n",
    "#import tensorflow.compat.v2.keras.layers.Attention\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Embedding, LSTM, Dense, GRU,RepeatVector, TimeDistributed, Dropout\n",
    "from nltk import FreqDist\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.layers import Attention\n",
    "#model.add(LSTM(150, input_shape=(n_timesteps_in, n_features), return_sequences=True))\n",
    "\n",
    "def build_model_attention(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "   # model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "######################## Attention Layer ######################\n",
    "    model.add(Attention(in_vocab))\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model\n",
    "\n",
    "#from keras import optimizers\n",
    "\n",
    "model_attention = build_model_attention(trainX.all(), eng_vocab_size, max_length_ger, max_length_eng, 32)\n",
    "#rms = optimizers.RMSprop(lr=0.001)\n",
    "#model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "model_attention.compile(loss='sparse_categorical_crossentropy',optimizer='adam',lr = 1e-3,metrics=['accuracy'])\n",
    "\n",
    "#model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "You must compile your model before training/testing. Use `model.compile(optimizer, loss)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-a2d0c2130efc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;31m#####################If more epochs here, the result would be better !!!\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m           \u001b[0mvalidation_split\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m            verbose=1)\n\u001b[0m",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    106\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1029\u001b[0m     \u001b[0;31m# Legacy graph support is contained in `training_v1.Model`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0mversion_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisallow_legacy_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Model'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1031\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_compile_was_called\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1032\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_call_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1033\u001b[0m     \u001b[0m_disallow_inside_tf_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fit'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_assert_compile_was_called\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2567\u001b[0m     \u001b[0;31m# (i.e. whether the model is built and its inputs/outputs are set).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2568\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_compiled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2569\u001b[0;31m       raise RuntimeError('You must compile your model before '\n\u001b[0m\u001b[1;32m   2570\u001b[0m                          \u001b[0;34m'training/testing. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2571\u001b[0m                          'Use `model.compile(optimizer, loss)`.')\n",
      "\u001b[0;31mRuntimeError\u001b[0m: You must compile your model before training/testing. Use `model.compile(optimizer, loss)`."
     ]
    }
   ],
   "source": [
    "## trainX.shape (800, 7) trainY.shape (800, 3) \n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "filename = 'model.h1.1119'\n",
    "#checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "#history_attention = model_attention.fit(trainX.all(), trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "history_attention = model_attention.fit(trainX.all(), Eng_windex_tokenizer, \n",
    "          epochs=20, batch_size=32, #####################If more epochs here, the result would be better !!!\n",
    "          validation_split = 0.2,\n",
    "           verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build NMT model\n",
    "def build_model(in_vocab, out_vocab, in_timesteps, out_timesteps, units):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(in_vocab, units, input_length=in_timesteps, mask_zero=True))\n",
    "    ############## Encoder ##########################\n",
    "    model.add(LSTM(units))\n",
    "    model.add(RepeatVector(out_timesteps))\n",
    "    ############## Decoder #########################\n",
    "    model.add(LSTM(units, return_sequences=True))\n",
    "    model.add(Dense(out_vocab, activation='softmax'))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#from keras import optimizers\n",
    "\n",
    "model = build_model(ger_vocab_size, eng_vocab_size, max_length_ger, max_length_eng, 32)\n",
    "#rms = optimizers.RMSprop(lr=0.001)\n",
    "#model.compile(optimizer=rms, loss='sparse_categorical_crossentropy')\n",
    "model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every 200 iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/20 [===========================>..] - ETA: 0s - loss: 5.9342 - accuracy: 0.2566  \n",
      "Epoch 00001: val_loss improved from inf to 5.91083, saving model to model.h1.1120\n",
      "WARNING:tensorflow:From /Users/albertliang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "WARNING:tensorflow:From /Users/albertliang/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/training/tracking/tracking.py:111: Layer.updates (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n",
      "INFO:tensorflow:Assets written to: model.h1.1120/assets\n",
      "20/20 [==============================] - 4s 219ms/step - loss: 5.9332 - accuracy: 0.2589 - val_loss: 5.9108 - val_accuracy: 0.2937\n",
      "step: 0, loss: [5.933244228363037]\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.8596 - accuracy: 0.2927\n",
      "Epoch 00001: val_loss improved from 5.91083 to 5.76114, saving model to model.h1.1120\n",
      "INFO:tensorflow:Assets written to: model.h1.1120/assets\n",
      "20/20 [==============================] - 4s 203ms/step - loss: 5.8596 - accuracy: 0.2927 - val_loss: 5.7611 - val_accuracy: 0.2937\n",
      "step: 200, loss: [5.859603404998779]\n",
      "20/20 [==============================] - ETA: 0s - loss: 5.4542 - accuracy: 0.2927\n",
      "Epoch 00001: val_loss improved from 5.76114 to 4.90862, saving model to model.h1.1120\n",
      "INFO:tensorflow:Assets written to: model.h1.1120/assets\n",
      "20/20 [==============================] - 4s 208ms/step - loss: 5.4542 - accuracy: 0.2927 - val_loss: 4.9086 - val_accuracy: 0.2937\n",
      "step: 400, loss: [5.4541707038879395]\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.4165 - accuracy: 0.2927\n",
      "Epoch 00001: val_loss improved from 4.90862 to 4.20729, saving model to model.h1.1120\n",
      "INFO:tensorflow:Assets written to: model.h1.1120/assets\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 4.4165 - accuracy: 0.2927 - val_loss: 4.2073 - val_accuracy: 0.2937\n",
      "step: 600, loss: [4.416512489318848]\n"
     ]
    }
   ],
   "source": [
    "## trainX.shape (800, 7) trainY.shape (800, 3) \n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "filename = 'model.h1.1120'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "'''\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "          epochs=200, batch_size=32, #####################If more epochs here, the result would be better !!!\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)'''\n",
    "steps = int(trainX.shape[0]*0.8)\n",
    "losses = []\n",
    "for iteration in range(steps):\n",
    "    #feed = feed_dict(X_train, Y_train)\n",
    "            \n",
    "    #backward_step(sess, feed)\n",
    "        \n",
    "    if iteration % 200 == 0 or iteration == 0:\n",
    "        \n",
    "        history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "          epochs=1, batch_size=32, #####################If more epochs here, the result would be better !!!\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)\n",
    "        loss_value = history.history['loss']\n",
    "        print ('step: {}, loss: {}'.format(iteration, loss_value))\n",
    "        losses.append(loss_value)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD6CAYAAACvZ4z8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deVyU5f7/8ddHRJFFRSRFUNEyFwyRcFdyKXMpzbTS0yJWx8ol/frTylN9XU+ZaWmmx5avW2qLZuWpTM0ls9JEQw8uaZkCacoiigoucP3+YPQgDjLowD0Mn+fjMQ9m7vu6Z96XU5/7nuvexBiDUkop91XO6gBKKaWKlxZ6pZRyc1rolVLKzWmhV0opN6eFXiml3JwWeqWUcnMOFXoRqSoiy0Vkn4jsFZE2+eaLiLwlIr+JyC4Ricwzb6CIHLA9Bjq7A0oppa6tvIPtZgLfGGP6iUgFwDvf/O5AA9ujFfAvoJWIVAPGAVGAAbaLyEpjzIlrfVj16tVNaGio471QSqkybvv27SnGmEB78wot9CJSGYgGYgCMMeeB8/ma9QYWmdyzr7bYfgEEAR2BtcaYNNt7rQW6AR9e6zNDQ0OJjY0tLJpSSikbETlc0DxHhm7qA8nAfBH5RUTeFxGffG2CgcQ8r5Ns0wqarpRSqoQ4UujLA5HAv4wxzYEzwAv52oid5cw1pl9FRAaLSKyIxCYnJzsQSymllCMcKfRJQJIxZqvt9XJyC3/+NrXzvA4Bjlxj+lWMMe8aY6KMMVGBgXaHmZRSSl2HQsfojTF/iUiiiDQ0xvwKdAH25Gu2EhgmIh+RuzP2pDHmqIisBl4REX9bu67AWCfmV0q5uAsXLpCUlERWVpbVUdyCl5cXISEheHp6OryMo0fdDAeW2I64OQgMEpGnAYwxc4GvgR7Ab8BZYJBtXpqITAK22d5n4qUds0qpsiEpKQk/Pz9CQ0MRsTeaqxxljCE1NZWkpCTq1avn8HIOFXpjTBy5h0jmNTfPfAMMLWDZecA8hxMppdxKVlaWFnknERECAgIo6n5MPTNWKVXstMg7z/X8Wzo6dFMqzN05Fy8PL/y9/PH38ifAK+Dy80rlK1kdTymlLOE2hd4Yw7z4eWRezLQ7v1L5SlTzqoZ/xdzCX82rWu5rr6tfV/OqpisGpdxEeno6S5cuZciQIUVarkePHixdupSqVasWU7KS4zaFXkTY+retnLlwhhNZJ0jNSuVE1glOnDtBWlYaaVlpua+zTpCSmcL+E/s5kXWC8zn5T/LNVal8pStWCvZWBv4V/alWKfevt2f+q0IopVxBeno6c+bMuarQZ2dn4+HhUeByX3/9dXFHKzFuU+ght9j7VvDFt4IvtSvXLrS9MYazF8+SlplG2rncFUH+lUJaVhopmSkcSD9AWmZagSuGS0NG9lYK/hXt/2LQcUulit8LL7zA77//TkREBJ6envj6+hIUFERcXBx79uzhvvvuIzExkaysLEaMGMHgwYOB/16K5fTp03Tv3p327dvz448/EhwczBdffEGlSqXnV79bFfqiEhF8PH3w8fShNkVYMWT9d6WQf+WQdi73+W/pv3Ei6wTnss/Zfa+8+xIu708oYFgpwCtAVwzKLUz49272HDnl1PdsUqsy4+4NK3D+lClTiI+PJy4ujo0bN9KzZ0/i4+MvH544b948qlWrRmZmJi1atKBv374EBARc8R4HDhzgww8/5L333uPBBx/k008/5ZFHHnFqP4pTmS70RXXFisHPsRVD5sXM/w4j5f/FkGdY6WD6QdKy0gpcMVT0qHjlL4aKBQwn2f56l/fWFYNSdrRs2fKKY9DfeustPvvsMwASExM5cODAVYW+Xr16REREAHD77bdz6NChEsvrDFroi5GI4O3pjbend5FWDHlXBKmZqZw4d/VK4o/0P0jLSiMr2/7ZhhXKVbi8/+CaO54rVqNaJV0xqJJxrS3vkuLj899rMm7cuJFvv/2Wn376CW9vbzp27Gj3DN6KFStefu7h4UFmpv2DPlyVFnoXknfFEOIX4tAyZy+ctbsiyL+v4Y+Tf3Di3IkCj0qqUK7CtY9GqnjlIas+nj66YlClgp+fHxkZGXbnnTx5En9/f7y9vdm3bx9btmwp4XQlQwt9KXdpxRDs69jVn/OvGK76e+4EaZlpHDp1iLSstAJXDJ7lPK86VyH/SuHS60DvQD1cVVkmICCAdu3a0bRpUypVqkSNGjUuz+vWrRtz584lPDychg0b0rp1awuTFh/JvXqBa4mKijJ64xHXkHkx0+7+hbRzaaRlpl31a8LeisHLw4s+DfrwWJPHHP6lotzH3r17ady4sdUx3Iq9f1MR2W6MyX+pGkC36FUhKpWvRCXfStTyreVQ+6yLWVeuFM6d4OejP7Ns/zI+/vVj7qp7F4PCBhFW3fqxWqXKCi30yqm8ynsR5BtEkG/Q5Wm9bu7F8ObDWbJvCct+XcbqQ6tpUbMFMWExdAjuoGP9ShUzvaiZKhE1fGow6vZRrO23ltFRo0k4lcDQdUO5f+X9fPHbF1zIvmB1RKXclhZ6VaJ8K/gyMGwgq+5fxSvtX0FEeOmHl+i2ohvz4+eTcd7+0RFKqeunhV5ZwtPDk3tvvpdP7/2Uf935L+pVrscb29+g6/KuTI+dzl9n/rI6olJuQ8folaVEhPbB7Wkf3J7dqbtZGL+QRXsWsXjvYnrU60FMWAwN/BtYHVOpUk236JXLCAsIY+odU/mqz1c8eOuDrD28lvtX3s8z3z7Dz0d/xhUPBVbux9fXF4AjR47Qr18/u206duxIYYeAz5gxg7Nnz15+3aNHD9LT050XtAi00CuXE+IXwthWY1nTdw3DIoaxJ3UPT6x5gv5f9eebP77hYs5FqyOqMqBWrVosX778upfPX+i//vpry65tr4VeuayqXlV5qtlTrO67mv9t87+cvXCWMZvGcM9n97B071LOXjhb+JuoMu/5559nzpw5l1+PHz+eCRMm0KVLFyIjI7ntttv44osvrlru0KFDNG3aFIDMzEz69+9PeHg4Dz300BXXunnmmWeIiooiLCyMcePGAbkXSjty5AidOnWiU6dOQO5lj1NSUgB44403aNq0KU2bNmXGjBmXP69x48b8/e9/JywsjK5duzrtmjoOnRkrIoeADCAbuJj/7CsRGQM8bHtZHmgMBBpj0gpb1h49M1bZk2Ny2JC4gfnx89mZvJMqFavQv2F/BjQaQEClgMLfQFniirM4V70Af/3HuR9Q8zboPqXA2b/88gsjR47ku+++A6BJkyZ88803VK1alcqVK5OSkkLr1q05cOBA7j0tfH05ffo0hw4d4p577iE+Pp433niD+Ph45s2bx65du4iMjGTLli1ERUWRlpZGtWrVyM7OpkuXLrz11luEh4dfvp599erVgf9e3/7w4cPExMSwZcsWjDG0atWKxYsX4+/vzy233EJsbCwRERE8+OCD9OrVy+7lkIt6ZmxRtug7GWMi7L2RMeZ127wIYCzwnTEmzZFllXJUOSlHlzpdWNxjMYu6LyLypkje2fUOd396NxN/msjhU4etjqhcUPPmzTl+/DhHjhxh586d+Pv7ExQUxD/+8Q/Cw8O58847+fPPPzl27FiB77Fp06bLBTc8PJzw8PDL8z755BMiIyNp3rw5u3fvZs+ePdfMs3nzZvr06YOPjw++vr7cf//9fP/990DxXQ65OI66GQB8WAzvq9RlzW9qTvPOzTl48iCLdi/i898+Z/n+5XSp04WYpjE0C2xmdURlzzW2vItTv379WL58OX/99Rf9+/dnyZIlJCcns337djw9PQkNDbV7eeK87J3B/ccffzBt2jS2bduGv78/MTExhb7PtUZRiutyyI5u0RtgjYhsF5HBBTUSEW+gG/DpdSw7WERiRSQ2OTnZwViqrKtfpT7j245nTb81PHnbk2z9ayuPfP0IA1cNZEPCBnJMjtURlQvo378/H330EcuXL6dfv36cPHmSm266CU9PTzZs2MDhw9f+NRgdHc2SJUsAiI+PZ9euXQCcOnUKHx8fqlSpwrFjx1i1atXlZQq6PHJ0dDSff/45Z8+e5cyZM3z22Wd06NDBib29mqNb9O2MMUdE5CZgrYjsM8ZsstPuXuCHfMM2Di1rjHkXeBdyx+iL2A9VxlWvVJ1nI5/lydueZMWBFSzas4hnNzxLvSr1iAmL4Z7691DBo4LVMZVFwsLCyMjIIDg4mKCgIB5++GHuvfdeoqKiiIiIoFGjRtdc/plnnmHQoEGEh4cTERFBy5YtAWjWrBnNmzcnLCyM+vXr065du8vLDB48mO7duxMUFMSGDRsuT4+MjCQmJubyezz55JM0b968WO9aVeTLFIvIeOC0MWaanXmfAcuMMUuLumxeujNW3agLORdYc2gNC3YvYF/aPqpXqs7DjR/mgVsfoErFKlbHK1P0MsXO5/SdsSLiIyJ+l54DXYF4O+2qAHcAXxR1WaWczbOcJz3r9+STez7h3bve5Vb/W5m5YyZdl3fltZ9f4+jpo1ZHVKrEODJ0UwP4zLYjojyw1BjzjYg8DWCMmWtr1wdYY4w5U9iyzgqvVGFEhDa12tCmVht+TfuVBbsX8OG+D/lw34d0q9eNQWGDaFitodUxlSpWeocpVeYcPX2UD/Z+wKf7P+XsxbO0CWpDTNMY2gS10WvjFwMdunG+4jyOXim3EOQbxHMtnmNNvzWMiBzBgfQDPLX2KR788kG+PPglF3L02vjKvWihV2VWlYpVePK2J1nddzUT207kXPY5xn4/lp4revLBng/0EgvKbWihV2VeBY8K9GnQh897f86szrMI8gli6rap3Ln8TmbumElKZorVEZW6IVrolbIpJ+XoWLsjC7svZEmPJbQOas3//ef/6Lq8K+N+HMfBkwetjqiu06VLD5dVeuMRpewIDwznjY5vcPjUYRbtXsQXv3/BigMr6Fi7I4PCBtH8pua641aVGrpFr9Q11K1cl5fbvMzqvqt5utnTxB2PY+A3A3lk1SN8e/hbsnOyrY6oisAYw5gxY2jatCm33XYbH3/8MQBHjx4lOjqaiIgImjZtyvfff092djYxMTGX27755psA/P7773Tr1o3bb7+dDh06sG/fPgCWLVtG06ZNadasGdHR0Zb10R7dolfKAQGVAhgaMZRBYYP44vcvWLR7Ef+z8X+oW7kujzV5jF4398KrvJfVMV3eaz+/xr60fU59z0bVGvF8y+cdartixQri4uLYuXMnKSkptGjRgujoaJYuXcrdd9/Niy++SHZ2NmfPniUuLo4///yT+Pjcczwv3R1q8ODBzJ07lwYNGrB161aGDBnC+vXrmThxIqtXryY4ONiyO0kVRAu9UkXg7enNgEYDeODWB/g24VsWxC9g0pZJzI6bzYBGA+jfsD9Vvay5i5Aq3ObNmxkwYAAeHh7UqFGDO+64g23bttGiRQsef/xxLly4wH333UdERAT169fn4MGDDB8+nJ49e9K1a1dOnz7Njz/+yAMPPHD5Pc+dOwdAu3btiImJ4cEHH+T++++3qot2aaFX6jqUL1eebqHduLvu3cQei2V+/Hxmx81mXvw87rvlPh5r8hghfiFWx3Q5jm55F5eCThCNjo5m06ZNfPXVVzz66KOMGTOGxx57jJ07d7J69Wpmz57NJ598wowZM6hatSpxcXFXvcfcuXPZunUrX331FREREcTFxREQ4Bo3xNExeqVugIjQomYL5tw5hxW9VnBX3btYtn8ZPT/ryZjvxrA7dbfVEVUe0dHRfPzxx2RnZ5OcnMymTZto2bIlhw8f5qabbuLvf/87TzzxBDt27CAlJYWcnBz69u3LpEmT2LFjB5UrV6ZevXosW7YMyF1x7Ny5E8gdu2/VqhUTJ06kevXqJCYmWtnVK+gWvVJO0sC/Af9s/0+ebf4sS/YuYdn+ZXxz6Bta1mxJTFgM7YPb65E6FuvTpw8//fQTzZo1Q0SYOnUqNWvWZOHChbz++ut4enri6+vLokWL+PPPPxk0aBA5Obn3NHj11VcBWLJkCc888wyTJ0/mwoUL9O/fn2bNmjFmzBgOHDiAMYYuXbrQrJnr3PxGr3WjVDHJOJ/Bp/s/5YO9H3D87HFuqXoLMWEx9KjXA08PT6vjlRi91o3z6bVulHIRfhX8iGkawzf3f8PkdpMBeOmHl+i2ohsL4hdw+vxpixOqskILvVLFzNPDk9639GZFrxXM6TKH0MqhTN8+nbuW38UbsW9w7EzBN6VWyhl0jF6pEiIidAjpQIeQDuxO2c2C3QtYuGchH+z9gB71ehATFkMD/wZWxywWxhjdP+Ek1zPcrlv0SlkgrHoYr9/xOl/2+ZIHbn2ANYfWcP/K+xny7RC2/bXtuv5ndlVeXl6kpqa6VZ+sYowhNTUVL6+inZynO2OVcgHpWel89OtHfLjvQ9Ky0ggLCCOmaQx31rmT8uVK9w/vCxcukJSURFZWltVR3IKXlxchISF4el65Q/9aO2O10CvlQrIuZrHy95Us3L2QhIwEgn2DeazJY9x3y314e3pbHU+5MC30SpUy2TnZbEjcwPzd89mVvIuqFavSv1F/BjQaQDWvalbHUy7ohgu9iBwCMoBs4GL+NxORjsAXwB+2SSuMMRNt87oBMwEP4H1jzJTCPk8LvVK5jDH8cvwX5u+ez8bEjVT0qEjvm3szMGwgdSrXsTqeciHXKvRFGfzrZIy51q12vjfG3JPvgz2A2cBdQBKwTURWGmP2FOFzlSqzRITIGpFE1ojkYPpBFu5ZyGe/fcay/cvoUqcLg5oOIjww3OqYysUV91E3LYHfjDEHjTHngY+A3sX8mUq5pfpV6zOh7QRW913NE7c9wda/tvLw1w8zcNVANiZuJMfkWB1RuShHC70B1ojIdhEZXECbNiKyU0RWiUiYbVowkPfKPkm2aUqp6xToHciIyBGs7beW51o8x9EzRxm+fjh9vujDigMrOJ993uqIysU4WujbGWMige7AUBHJf/uUHUBdY0wzYBbwuW26vTMk7O4UEJHBIhIrIrHJyckOxlKq7PLx9OHRJo/y1f1f8WqHV/Es58m4H8dx96d38/5/3ufU+VNWR1QuoshH3YjIeOC0MWbaNdocAqKABsB4Y8zdtuljAYwxr17rM3RnrFJFZ4zhp6M/sSB+AT8d/Qnv8t70vbUvjzZ+lCDfIKvjqWJ2QztjRcQHKGeMybA97wpMzNemJnDMGGNEpCW5vxRSgXSggYjUA/4E+gN/u6HeKKXsEhHa1mpL21pt2Zu6lwW7F7B071I+3Pshd9e7m0Fhg2hYraHVMZUFCt2iF5H6wGe2l+WBpcaYf4rI0wDGmLkiMgx4BrgIZAKjjDE/2pbvAcwg9/DKecaYfxYWSrfolXKOI6eP8MGeD/j0wKdkXsykba22xITF0DqotV57xs3oCVNKlXEnz51k2f5lLN6zmNSsVBpVa8TEthNpHKDXiXcXej16pcq4KhWr8ORtT7K632rGtxlPWlYaQ9YN4ejpo1ZHUyVAC71SZUhFj4r0vbUv7971LlkXsxiybojeAKUM0EKvVBl0c9Wbmd5xOn+c/IPRm0ZzMeei1ZFUMdJCr1QZ1bZWW15s/SI//PkDU36eoteLd2Ol+0LXSqkb8sCtD5BwKoEFuxcQWjmUR5o8YnUkVQy00CtVxo2MHEnCqQSmbptKiF8IHWt3tDqScjIdulGqjPMo58GrHV6lcUBjntv0HHtT91odSTmZFnqlFN6e3rzd+W0qV6jMsHXDOHbmmNWRlBNpoVdKAblXxZzdZTanL5xm+PrhnL1w1upIykm00CulLmtYrSGv3/E6v574lec3PU92TrbVkZQTaKFXSl0hOiSa51s8z8akjUzfPt3qOMoJ9KgbpdRV/tb4byRkJPDBng+o61eXhxo9ZHUkdQO00Cul7BoTNYbEjERe/flVgv2CaR/c3upI6jrp0I1Syi6Pch5MjZ7KLVVvYfR3o9l/Yr/VkdR10kKvlCqQj6cPb3d5G+/y3gxbN4yUzBSrI6nroIVeKXVNNX1qMqvLLNLPpTN83XAyL2ZaHUkVkRZ6pVShwgLCmNJhCrtTd/Pi5hfJMTlWR1JFoIVeKeWQznU68/+i/h9rD69l5o6ZVsdRRaBH3SilHPZYk8dIOJXAvPh51PGrQ99b+1odSTlAC71SymEiwgutXiDpdBKTt0wm2C+Y1kGtrY6lCuHQ0I2IHBKR/4hInIhcddduEXlYRHbZHj+KSDNHl1VKlS6e5TyZdsc0QquEMmrDKA6mH7Q6kipEUcboOxljIgq4y/gfwB3GmHBgEvBuEZZVSpUyfhX8eLvL23h6eDJk3RDSstKsjqSuwSk7Y40xPxpjTthebgFCnPG+SinXFewbzKzOs0jJTGHE+hGcyz5ndSRVAEcLvQHWiMh2ERlcSNsngFVFXVZEBotIrIjEJicnOxhLKWWl8MBwXmn/CnHJcby8+WW976yLcrTQtzPGRALdgaEiEm2vkYh0IrfQP1/UZY0x7xpjoowxUYGBgY73QCllqa6hXRkROYJVh1YxO2621XGUHQ4VemPMEdvf48BnQMv8bUQkHHgf6G2MSS3Kskqp0u2Jpk/Q55Y+vLPrHVb+vtLqOCqfQgu9iPiIiN+l50BXID5fmzrACuBRY8z+oiyrlCr9RISXW79My5otGffjOGL/0gPsXIkjW/Q1gM0ishP4GfjKGPONiDwtIk/b2vwvEADMyXcYpd1lndwHpZQL8PTw5I2ObxDiG8LIjSM5fOqw1ZGUjbjizpOoqCgTG6tbBEqVRomnEnn464epXLEyi7svpqpXVasjlQkisr2gQ9j1WjdKKaeqXbk2MzvP5MjpI4zcOJLz2eetjlTmaaFXSjld85uaM6ndJLYf286EnyboYZcW02vdKKWKRc/6PUnISGBO3Bzq+NXhqWZPWR2pzNJCr5QqNk+HP03CqQTejnubOpXr0L1ed6sjlUk6dKOUKjYiwoS2E4i8KZKXNr9E3PE4qyOVSVrolVLFqoJHBWZ0mkENnxqM2DCCxIxEqyOVOVrolVLFzt/Lnzld5nAx5yJD1w3l1PlTVkcqU7TQK6VKRGiVUGZ0mkFiRiKjNo7iQs4FqyOVGVrolVIlpkXNFoxvM56tR7cyectkPeyyhOhRN0qpEtX7lt4cPnWY9/7zHnUr1+Xxpo9bHcntaaFXSpW4Yc2HkZiRyJvb36SOXx3urHun1ZHcmg7dKKVKXDkpx6R2kwgPDGfs92OJT9GL2hYnLfRKKUt4lffirU5vEVApgGHrhnH09FGrI7ktLfRKKcsEVApgdpfZnMs+x5B1Qzh9/rTVkdySFnqllKVurnoz0ztO54+TfzB602gu5ly0OpLb0UKvlLJc21ptean1S/zw5w9M+XmKHnbpZHrUjVLKJfS7tR8JpxKYv3s+oZVDeaTJI1ZHchta6JVSLmPk7SNJyEhg6raphPiF0LF2R6sjuQUdulFKuYxyUo5XO7xKk4AmPLfpOfam7rU6klvQQq+UcimVyldiVudZVKlYhWHrhnHszDGrI5V6DhV6ETkkIv8RkTgRuequ3ZLrLRH5TUR2iUhknnkDReSA7THQmeGVUu4p0DuQtzu/zekLpxm+fjhnL5y1OlKpVpQt+k7GmIgC7jLeHWhgewwG/gUgItWAcUAroCUwTkT8byyyUqosaFitIa/f8Tq/nviV5zc9T3ZOttWRSi1nDd30BhaZXFuAqiISBNwNrDXGpBljTgBrgW5O+kyllJuLDonmhZYvsDFpI9Nip1kdp9RytNAbYI2IbBeRwXbmBwN5bxuTZJtW0PSriMhgEYkVkdjk5GQHYyml3N2ARgN4uPHDLN67mI/2fWR1nFLJ0ULfzhgTSe4QzVARic43X+wsY64x/eqJxrxrjIkyxkQFBgY6GEspVRaMiRrDHSF3MOXnKWz+c7PVcUodhwq9MeaI7e9x4DNyx9vzSgJq53kdAhy5xnSllHKYRzkPpkZPpYF/A0Z/N5r9J/ZbHalUKbTQi4iPiPhdeg50BfJfU3Ql8Jjt6JvWwEljzFFgNdBVRPxtO2G72qYppVSReHt6M6vzLHzK+zBs3TBSMlOsjlRqOLJFXwPYLCI7gZ+Br4wx34jI0yLytK3N18BB4DfgPWAIgDEmDZgEbLM9JtqmKaVUkdX0qcmsLrNIP5fO8HXDybyYaXWkUkFc8eJBUVFRJjb2qsP1lVIKgPUJ6xm5YSR31r2TaXdMo5zouZ8isr2Aw9/1zFilVOnTuU5nRkeNZu3htczcMdPqOC5PL2qmlCqVHm3yKIdPHWZe/Dzq+NWh7619rY7ksrTQK6VKJRFhbKuxJJ1OYvKWyQT7BdM6qLXVsVySDt0opUqt8uXKM+2OaYRWCWXUhlEcTD9odSSXpIVeKVWq+VXwY3aX2Xh6eDJk3RBSM1OtjuRytNArpUq9Wr61mNV5FimZKYzYMIJz2eesjuRStNArpdxCeGA4r7R/hZ3JO3l588vkmByrI7kMLfRKKbfRNbQrIyNHsurQKubEzbE6jsvQo26UUm7l8aaPk5CRwDu73qFO5Tr0urmX1ZEsp4VeKeVWRISXWr3Enxl/Mu7HcdTyqUVUTbsnjJYZOnSjlHI7nh6eTO84ndp+tRm5cSSHTx22OpKltNArpdxSlYpVmN15NuUox9B1Q0nPSrc6kmW00Cul3FbtyrWZ2XkmR04fYeTGkZzPPm91JEtooVdKubXmNzVncrvJbD+2nQk/TcAVr9hb3HRnrFLK7fWo34PDGYeZEzeHOn51eKrZU1ZHKlFa6JVSZcLT4U+TeCqRt+PeprZfbXrU72F1pBKjQzdKqTJBRBjfdjyRN0Xy8g8vE3c8zupIJUYLvVKqzKjgUYGZnWZS06cmz65/lsSMRKsjlQgt9EqpMqWqV1Vmd5lNtslm6LqhnDp/yupIxc7hQi8iHiLyi4h8aWfemyISZ3vsF5H0PPOy88xb6azgSil1vUKrhDKj0wwSMxIZtXEUF3IuWB2pWBVli34EsNfeDGPM/xhjIowxEcAsYEWe2ZmX5hlj9KITSimX0KJmC8a3Gc/Wo1uZvGWyWx926VChF5EQoCfwvgPNBwAf3kgopZQqCb1v6aWG+Y0AAAxySURBVM3fb/s7Kw6sYP7u+VbHKTaObtHPAJ4DrnmBZxGpC9QD1ueZ7CUisSKyRUTuu76YSilVPIY1H0a30G68uf1Nvj38rdVxikWhhV5E7gGOG2O2O/B+/YHlxpjsPNPqGGOigL8BM0Tk5gI+Z7BthRCbnJzsSHallLph5aQck9pNIjwwnLHfjyU+Jd7qSE7nyBZ9O6CXiBwCPgI6i8jiAtr2J9+wjTHmiO3vQWAj0NzegsaYd40xUcaYqMDAQMfSK6WUE3iV9+KtTm8RUCmAYeuGcfT0UasjOVWhhd4YM9YYE2KMCSW3kK83xjySv52INAT8gZ/yTPMXkYq259XJXWnscVJ2pZRymoBKAczuMpvz2ecZsm4Ip8+ftjqS01z3cfQiMlFE8h5FMwD4yFy567oxECsiO4ENwBRjjBZ6pZRLurnqzUzvOJ1DJw8xetNoLuZctDqSU4grHlIUFRVlYmNjrY6hlCqjlu9fzoSfJvBQw4d4sdWLiIjVkQolIttt+0Ovohc1U0qpfPrd2o+EUwnM3z2fupXr8miTR62OdEO00CullB0jbx9JYkYir297ndp+telYu6PVka6bXutGKaXsKCfleKXDKzQJaMJzm55jb6rdCwOUClrolVKqAJXKV2JW51lUqViFYeuGcezMMasjXRct9EopdQ2B3oG83fltzlw8w/D1wzl74azVkYpMC71SShWiYbWGvB79Or+e+JXnNz1Pdk524Qu5EC30SinlgA4hHXih5QtsTNrItNhpVscpEj3qRimlHDSg0QASTiWweO9i6lauS/9G/a2O5BAt9EopVQSjo0aTmJHIlJ+nEOIXQvvg9lZHKpQO3SilVBF4lPNgavRUGvg3YPR3o9l/Yr/VkQqlhV4ppYrI29ObWZ1n4VPeh2HrhpGSmWJ1pGvSQq+UUtehpk9NZnWZRfq5dIavG07mxUyrIxVIC71SSl2nJgFNeK3Da+xO3c0/vv8HOeaaN+GzjBZ6pZS6AZ3qdGJ01Gi+TfiWGTtmWB3HLj3qRimlbtCjTR4lISOB+fHzqetXl7639rU60hW00Cul1A0SEV5o+QJJGUlM3jKZYL9gWge1tjrWZTp0o5RSTlC+XHmm3TGN0CqhjNowioPpB62OdJkWeqWUchLfCr7M7jKbCh4VGLJuCKmZqVZHArTQK6WUU9XyrcWszrNIzUxlxIYRnMs+Z3UkLfRKKeVstwXexisdXmFn8k5e3vyy5YddOlzoRcRDRH4RkS/tzIsRkWQRibM9nswzb6CIHLA9BjoruFJKubK76t7FyMiRrDq0ijlxcyzNUpSjbkYAe4HKBcz/2BgzLO8EEakGjAOiAANsF5GVxpgT1xNWKaVKk8ebPk5CRgLv7HqHOpXr0OvmXpbkcGiLXkRCgJ7A+0V8/7uBtcaYNFtxXwt0K+J7KKVUqSQivNT6JVrVbMW4H8ex7a9tluRwdOhmBvAccK2Bpr4isktElotIbdu0YCAxT5sk27SriMhgEYkVkdjk5GQHYymllGvzLOfJ9I7Tqe1Xm5EbRnLo5KESz1BooReRe4Djxpjt12j2byDUGBMOfAssvLS4nbbG3hsYY941xkQZY6ICAwMLi6WUUqVGlYpVmN1lNh7iwdB1Q0nPSi/Rz3dki74d0EtEDgEfAZ1FZHHeBsaYVGPMpWOI3gNutz1PAmrnaRoCHLmhxEopVQrV9qvNW53f4q8zfzFiwwjOZ58vsc8utNAbY8YaY0KMMaFAf2C9MeaRvG1EJCjPy17k7rQFWA10FRF/EfEHutqmKaVUmRNxUwST2k1ix/EdjP9xPMbYHeBwuuu+1o2ITARijTErgWdFpBdwEUgDYgCMMWkiMgm4tAdiojEm7cYiK6VU6dWjfg8SMhKYHTebupXr8lSzp4r9M6Wk1ihFERUVZWJjY62OoZRSxcIYw4ubX+TfB//Nax1eo0f9Hjf8niKy3RgTZW+enhmrlFIlTEQY33Y8t9e4nZd/eJm443HF+nla6JVSygIVPCowo+MMavrU5Nn1z5KYkVj4QtdJC71SSlmkqldVZneZTQ45DF03lFPnTxXL52ihV0opC4VWCeXNjm+SmJHIqI2juJBzwemfoYVeKaUs1qJmCya0nUD1StWL5ZBLvZWgUkq5gF439+Le+vciYu+CAjdGt+iVUspFFEeRBy30Sinl9rTQK6WUm9NCr5RSbk4LvVJKuTkt9Eop5ea00CullJvTQq+UUm5OC71SSrk5LfRKKeXmtNArpZSb00KvlFJuTgu9Ukq5OS30Sinl5hwu9CLiISK/iMiXduaNEpE9IrJLRNaJSN0887JFJM72WOms4EoppRxTlOvRjwD2ApXtzPsFiDLGnBWRZ4CpwEO2eZnGmIgbi6mUUup6ObRFLyIhQE/gfXvzjTEbjDFnbS+3ACHOiaeUUupGOTp0MwN4DshxoO0TwKo8r71EJFZEtojIfQUtJCKDbe1ik5OTHYyllFKqMIUWehG5BzhujNnuQNtHgCjg9TyT6xhjooC/ATNE5GZ7yxpj3jXGRBljogIDAx1Lr5RSqlCObNG3A3qJyCHgI6CziCzO30hE7gReBHoZY85dmm6MOWL7exDYCDS/8dhKKaUcVWihN8aMNcaEGGNCgf7AemPMI3nbiEhz4B1yi/zxPNP9RaSi7Xl1clcae5yYXymlVCGKctTNFURkIhBrjFlJ7lCNL7DMdnPbBGNML6Ax8I6I5JC7UplijNFCr5RSJUiMMVZnuEpUVJSJjY21OoZSSpUaIrLdtj/0KnpmrFJKuTkt9Eop5ea00CullJvTQq+UUm5OC71SSrk5LfRKKeXmtNArpZSb00KvlFJuTgu9Ukq5OZc8M1ZEkoHD17l4dSDFiXGs5C59cZd+gPbFFblLP+DG+lLXGGP30r8uWehvhIjEFnQacGnjLn1xl36A9sUVuUs/oPj6okM3Sinl5rTQK6WUm3PHQv+u1QGcyF364i79AO2LK3KXfkAx9cXtxuiVUkpdyR236JVSSuVRagu9iHQTkV9F5DcRecHO/Ioi8rFt/lYRCS35lIVzoB8xIpIsInG2x5NW5CyMiMwTkeMiEl/AfBGRt2z93CUikSWd0VEO9KWjiJzM8538b0lndJSI1BaRDSKyV0R2i8gIO21c/rtxsB+l4nsRES8R+VlEdtr6MsFOG+fWL2NMqXsAHsDvQH2gArATaJKvzRBgru15f+Bjq3NfZz9igLetzupAX6KBSCC+gPk9gFWAAK2BrVZnvoG+dAS+tDqng30JAiJtz/2A/Xb+G3P578bBfpSK78X27+xre+4JbAVa52vj1PpVWrfoWwK/GWMOGmPOAx8BvfO16Q0stD1fDnQR2w1tXYgj/SgVjDGbgLRrNOkNLDK5tgBVRSSoZNIVjQN9KTWMMUeNMTtszzOAvUBwvmYu/9042I9SwfbvfNr20tP2yL+z1Kn1q7QW+mAgMc/rJK7+0i+3McZcBE4CASWSznGO9AOgr+0n9XIRqV0y0ZzO0b6WFm1sP71XiUiY1WEcYfv535zcLci8StV3c41+QCn5XkTEQ0TigOPAWmNMgd+JM+pXaS309tZs+deIjrSxmiMZ/w2EGmPCgW/571q+tCkN34ejdpB7unkzYBbwucV5CiUivsCnwEhjzKn8s+0s4pLfTSH9KDXfizEm2xgTAYQALUWkab4mTv1OSmuhTwLybtmGAEcKaiMi5YEquN7P8UL7YYxJNcacs718D7i9hLI5myPfWalgjDl16ae3MeZrwFNEqlscq0Ai4klucVxijFlhp0mp+G4K60dp+14AjDHpwEagW75ZTq1fpbXQbwMaiEg9EalA7s6KlfnarAQG2p73A9Yb254NF1JoP/KNlfYid2yyNFoJPGY7wqM1cNIYc9TqUNdDRGpeGi8VkZbk/n+Uam0q+2w5/w/Ya4x5o4BmLv/dONKP0vK9iEigiFS1Pa8E3Ansy9fMqfWr/PUuaCVjzEURGQasJvfIlXnGmN0iMhGINcasJPc/ig9E5Ddy14T9rUtsn4P9eFZEegEXye1HjGWBr0FEPiT3qIfqIpIEjCN3JxPGmLnA1+Qe3fEbcBYYZE3SwjnQl37AMyJyEcgE+rvgRsQl7YBHgf/YxoQB/gHUgVL13TjSj9LyvQQBC0XEg9yV0SfGmC+Ls37pmbFKKeXmSuvQjVJKKQdpoVdKKTenhV4ppdycFnqllHJzWuiVUsrNaaFXSik3p4VeKaXcnBZ6pZRyc/8fw+n0ONtnsv8AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.plot(losses)\n",
    "plt.legend(['train','validation', 'losses'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(800, 3, 1)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainX.shape\n",
    "#type(trainY.shape)\n",
    "aa = trainY.reshape(trainY.shape[0], trainY.shape[1], 1)\n",
    "aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "####testY.shapm     (200,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another epoch measure(normal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.0820 - accuracy: 0.2927\n",
      "Epoch 00001: val_loss improved from inf to 4.18176, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 219ms/step - loss: 4.0820 - accuracy: 0.2927 - val_loss: 4.1818 - val_accuracy: 0.2937\n",
      "Epoch 2/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 4.0321 - accuracy: 0.2927\n",
      "Epoch 00002: val_loss did not improve from 4.18176\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 4.0321 - accuracy: 0.2927 - val_loss: 4.1830 - val_accuracy: 0.2937\n",
      "Epoch 3/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 4.0296 - accuracy: 0.2850\n",
      "Epoch 00003: val_loss did not improve from 4.18176\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 4.0005 - accuracy: 0.2927 - val_loss: 4.1852 - val_accuracy: 0.2937\n",
      "Epoch 4/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.9746 - accuracy: 0.2927\n",
      "Epoch 00004: val_loss did not improve from 4.18176\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.9746 - accuracy: 0.2927 - val_loss: 4.1832 - val_accuracy: 0.2937\n",
      "Epoch 5/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.9631 - accuracy: 0.2896\n",
      "Epoch 00005: val_loss improved from 4.18176 to 4.17489, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 218ms/step - loss: 3.9496 - accuracy: 0.2927 - val_loss: 4.1749 - val_accuracy: 0.2937\n",
      "Epoch 6/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.9213 - accuracy: 0.2927\n",
      "Epoch 00006: val_loss improved from 4.17489 to 4.15805, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 3.9213 - accuracy: 0.2927 - val_loss: 4.1581 - val_accuracy: 0.2937\n",
      "Epoch 7/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.8788 - accuracy: 0.2936\n",
      "Epoch 00007: val_loss improved from 4.15805 to 4.13589, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 208ms/step - loss: 3.8909 - accuracy: 0.2927 - val_loss: 4.1359 - val_accuracy: 0.2937\n",
      "Epoch 8/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.8280 - accuracy: 0.2958\n",
      "Epoch 00008: val_loss improved from 4.13589 to 4.11016, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 3.8541 - accuracy: 0.2927 - val_loss: 4.1102 - val_accuracy: 0.2937\n",
      "Epoch 9/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.8109 - accuracy: 0.2937\n",
      "Epoch 00009: val_loss improved from 4.11016 to 4.09149, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 3.8174 - accuracy: 0.2927 - val_loss: 4.0915 - val_accuracy: 0.2937\n",
      "Epoch 10/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.7760 - accuracy: 0.2927\n",
      "Epoch 00010: val_loss improved from 4.09149 to 4.07197, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 183ms/step - loss: 3.7862 - accuracy: 0.2927 - val_loss: 4.0720 - val_accuracy: 0.2937\n",
      "Epoch 11/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.7655 - accuracy: 0.2896\n",
      "Epoch 00011: val_loss improved from 4.07197 to 4.05307, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 214ms/step - loss: 3.7604 - accuracy: 0.2927 - val_loss: 4.0531 - val_accuracy: 0.2937\n",
      "Epoch 12/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 3.7332 - accuracy: 0.2927\n",
      "Epoch 00012: val_loss improved from 4.05307 to 4.03913, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 3.7332 - accuracy: 0.2927 - val_loss: 4.0391 - val_accuracy: 0.2937\n",
      "Epoch 13/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.7128 - accuracy: 0.2979\n",
      "Epoch 00013: val_loss improved from 4.03913 to 4.02165, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 3.7074 - accuracy: 0.2969 - val_loss: 4.0216 - val_accuracy: 0.3333\n",
      "Epoch 14/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.6895 - accuracy: 0.3313\n",
      "Epoch 00014: val_loss improved from 4.02165 to 4.01079, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.6850 - accuracy: 0.3286 - val_loss: 4.0108 - val_accuracy: 0.3396\n",
      "Epoch 15/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 3.6172 - accuracy: 0.3385\n",
      "Epoch 00015: val_loss improved from 4.01079 to 3.99822, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 3.6651 - accuracy: 0.3286 - val_loss: 3.9982 - val_accuracy: 0.3417\n",
      "Epoch 16/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.6547 - accuracy: 0.3305\n",
      "Epoch 00016: val_loss improved from 3.99822 to 3.99309, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 3.6468 - accuracy: 0.3286 - val_loss: 3.9931 - val_accuracy: 0.3438\n",
      "Epoch 17/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.6024 - accuracy: 0.3371\n",
      "Epoch 00017: val_loss improved from 3.99309 to 3.98486, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 3.6281 - accuracy: 0.3286 - val_loss: 3.9849 - val_accuracy: 0.3438\n",
      "Epoch 18/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.6069 - accuracy: 0.3305\n",
      "Epoch 00018: val_loss improved from 3.98486 to 3.98012, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 3.6110 - accuracy: 0.3286 - val_loss: 3.9801 - val_accuracy: 0.3438\n",
      "Epoch 19/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.6511 - accuracy: 0.3267\n",
      "Epoch 00019: val_loss improved from 3.98012 to 3.97187, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.5967 - accuracy: 0.3302 - val_loss: 3.9719 - val_accuracy: 0.3438\n",
      "Epoch 20/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.5688 - accuracy: 0.3343\n",
      "Epoch 00020: val_loss did not improve from 3.97187\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.5788 - accuracy: 0.3313 - val_loss: 3.9725 - val_accuracy: 0.3458\n",
      "Epoch 21/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.5840 - accuracy: 0.3324\n",
      "Epoch 00021: val_loss improved from 3.97187 to 3.96277, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 3.5655 - accuracy: 0.3339 - val_loss: 3.9628 - val_accuracy: 0.3458\n",
      "Epoch 22/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.5136 - accuracy: 0.3390\n",
      "Epoch 00022: val_loss did not improve from 3.96277\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.5484 - accuracy: 0.3339 - val_loss: 3.9689 - val_accuracy: 0.3458\n",
      "Epoch 23/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.5550 - accuracy: 0.3286\n",
      "Epoch 00023: val_loss improved from 3.96277 to 3.95265, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 3.5334 - accuracy: 0.3359 - val_loss: 3.9526 - val_accuracy: 0.3458\n",
      "Epoch 24/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.5287 - accuracy: 0.3314\n",
      "Epoch 00024: val_loss did not improve from 3.95265\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.5195 - accuracy: 0.3354 - val_loss: 3.9545 - val_accuracy: 0.3458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.5204 - accuracy: 0.3342\n",
      "Epoch 00025: val_loss improved from 3.95265 to 3.94434, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 3.5075 - accuracy: 0.3365 - val_loss: 3.9443 - val_accuracy: 0.3458\n",
      "Epoch 26/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.4895 - accuracy: 0.3390\n",
      "Epoch 00026: val_loss did not improve from 3.94434\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.4953 - accuracy: 0.3359 - val_loss: 3.9460 - val_accuracy: 0.3458\n",
      "Epoch 27/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.4255 - accuracy: 0.3455\n",
      "Epoch 00027: val_loss improved from 3.94434 to 3.94026, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.4825 - accuracy: 0.3365 - val_loss: 3.9403 - val_accuracy: 0.3458\n",
      "Epoch 28/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.5461 - accuracy: 0.3286\n",
      "Epoch 00028: val_loss did not improve from 3.94026\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.4714 - accuracy: 0.3365 - val_loss: 3.9404 - val_accuracy: 0.3458\n",
      "Epoch 29/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.4807 - accuracy: 0.3290\n",
      "Epoch 00029: val_loss improved from 3.94026 to 3.93551, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 3.4599 - accuracy: 0.3370 - val_loss: 3.9355 - val_accuracy: 0.3458\n",
      "Epoch 30/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.4405 - accuracy: 0.3420\n",
      "Epoch 00030: val_loss improved from 3.93551 to 3.93501, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.4491 - accuracy: 0.3375 - val_loss: 3.9350 - val_accuracy: 0.3458\n",
      "Epoch 31/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.4295 - accuracy: 0.3381\n",
      "Epoch 00031: val_loss improved from 3.93501 to 3.93391, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 205ms/step - loss: 3.4381 - accuracy: 0.3370 - val_loss: 3.9339 - val_accuracy: 0.3458\n",
      "Epoch 32/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.4206 - accuracy: 0.3324\n",
      "Epoch 00032: val_loss improved from 3.93391 to 3.93007, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 3.4284 - accuracy: 0.3375 - val_loss: 3.9301 - val_accuracy: 0.3458\n",
      "Epoch 33/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3944 - accuracy: 0.3419\n",
      "Epoch 00033: val_loss did not improve from 3.93007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.4188 - accuracy: 0.3375 - val_loss: 3.9331 - val_accuracy: 0.3458\n",
      "Epoch 34/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.3803 - accuracy: 0.3429\n",
      "Epoch 00034: val_loss did not improve from 3.93007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.4080 - accuracy: 0.3375 - val_loss: 3.9332 - val_accuracy: 0.3458\n",
      "Epoch 35/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.3928 - accuracy: 0.3368\n",
      "Epoch 00035: val_loss did not improve from 3.93007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3973 - accuracy: 0.3370 - val_loss: 3.9319 - val_accuracy: 0.3458\n",
      "Epoch 36/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3618 - accuracy: 0.3419\n",
      "Epoch 00036: val_loss did not improve from 3.93007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3869 - accuracy: 0.3375 - val_loss: 3.9336 - val_accuracy: 0.3479\n",
      "Epoch 37/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.3535 - accuracy: 0.3411\n",
      "Epoch 00037: val_loss did not improve from 3.93007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 3.3765 - accuracy: 0.3380 - val_loss: 3.9303 - val_accuracy: 0.3479\n",
      "Epoch 38/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.4061 - accuracy: 0.3368\n",
      "Epoch 00038: val_loss improved from 3.93007 to 3.92887, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 3.3639 - accuracy: 0.3385 - val_loss: 3.9289 - val_accuracy: 0.3479\n",
      "Epoch 39/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3474 - accuracy: 0.3343\n",
      "Epoch 00039: val_loss improved from 3.92887 to 3.92084, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 3.3524 - accuracy: 0.3370 - val_loss: 3.9208 - val_accuracy: 0.3479\n",
      "Epoch 40/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3420 - accuracy: 0.3400\n",
      "Epoch 00040: val_loss improved from 3.92084 to 3.91582, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.3381 - accuracy: 0.3375 - val_loss: 3.9158 - val_accuracy: 0.3458\n",
      "Epoch 41/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3029 - accuracy: 0.3381\n",
      "Epoch 00041: val_loss improved from 3.91582 to 3.91025, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 3.3222 - accuracy: 0.3391 - val_loss: 3.9103 - val_accuracy: 0.3479\n",
      "Epoch 42/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3048 - accuracy: 0.3400\n",
      "Epoch 00042: val_loss improved from 3.91025 to 3.90372, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 3.3092 - accuracy: 0.3385 - val_loss: 3.9037 - val_accuracy: 0.3417\n",
      "Epoch 43/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3359 - accuracy: 0.3333\n",
      "Epoch 00043: val_loss improved from 3.90372 to 3.89757, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 3.2944 - accuracy: 0.3391 - val_loss: 3.8976 - val_accuracy: 0.3458\n",
      "Epoch 44/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.3149 - accuracy: 0.3390\n",
      "Epoch 00044: val_loss improved from 3.89757 to 3.89211, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.2796 - accuracy: 0.3401 - val_loss: 3.8921 - val_accuracy: 0.3438\n",
      "Epoch 45/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.2490 - accuracy: 0.3447\n",
      "Epoch 00045: val_loss improved from 3.89211 to 3.88099, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 3.2635 - accuracy: 0.3427 - val_loss: 3.8810 - val_accuracy: 0.3458\n",
      "Epoch 46/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.2323 - accuracy: 0.3494\n",
      "Epoch 00046: val_loss improved from 3.88099 to 3.85695, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 3.2423 - accuracy: 0.3453 - val_loss: 3.8569 - val_accuracy: 0.3500\n",
      "Epoch 47/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.2416 - accuracy: 0.3438\n",
      "Epoch 00047: val_loss improved from 3.85695 to 3.84858, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 183ms/step - loss: 3.2174 - accuracy: 0.3510 - val_loss: 3.8486 - val_accuracy: 0.3562\n",
      "Epoch 48/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.1730 - accuracy: 0.3580\n",
      "Epoch 00048: val_loss improved from 3.84858 to 3.83492, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 3.1957 - accuracy: 0.3505 - val_loss: 3.8349 - val_accuracy: 0.3521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.1299 - accuracy: 0.3665\n",
      "Epoch 00049: val_loss improved from 3.83492 to 3.82683, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 3.1766 - accuracy: 0.3609 - val_loss: 3.8268 - val_accuracy: 0.3646\n",
      "Epoch 50/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.1413 - accuracy: 0.3703\n",
      "Epoch 00050: val_loss improved from 3.82683 to 3.81752, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 3.1539 - accuracy: 0.3672 - val_loss: 3.8175 - val_accuracy: 0.3646\n",
      "Epoch 51/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.1550 - accuracy: 0.3715\n",
      "Epoch 00051: val_loss improved from 3.81752 to 3.79820, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.1322 - accuracy: 0.3714 - val_loss: 3.7982 - val_accuracy: 0.3667\n",
      "Epoch 52/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.1142 - accuracy: 0.3731\n",
      "Epoch 00052: val_loss improved from 3.79820 to 3.78632, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 3.1092 - accuracy: 0.3776 - val_loss: 3.7863 - val_accuracy: 0.3708\n",
      "Epoch 53/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.1014 - accuracy: 0.3731\n",
      "Epoch 00053: val_loss improved from 3.78632 to 3.77644, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.0859 - accuracy: 0.3776 - val_loss: 3.7764 - val_accuracy: 0.3729\n",
      "Epoch 54/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.0267 - accuracy: 0.3892\n",
      "Epoch 00054: val_loss improved from 3.77644 to 3.75853, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 3.0602 - accuracy: 0.3812 - val_loss: 3.7585 - val_accuracy: 0.3729\n",
      "Epoch 55/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.0446 - accuracy: 0.3826\n",
      "Epoch 00055: val_loss improved from 3.75853 to 3.74968, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 3.0352 - accuracy: 0.3885 - val_loss: 3.7497 - val_accuracy: 0.3729\n",
      "Epoch 56/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 3.0072 - accuracy: 0.3889\n",
      "Epoch 00056: val_loss improved from 3.74968 to 3.74324, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 3.0108 - accuracy: 0.3901 - val_loss: 3.7432 - val_accuracy: 0.3771\n",
      "Epoch 57/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.9872 - accuracy: 0.3932\n",
      "Epoch 00057: val_loss improved from 3.74324 to 3.73793, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 2.9857 - accuracy: 0.3906 - val_loss: 3.7379 - val_accuracy: 0.3708\n",
      "Epoch 58/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 3.0532 - accuracy: 0.3731\n",
      "Epoch 00058: val_loss improved from 3.73793 to 3.72967, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.9629 - accuracy: 0.3938 - val_loss: 3.7297 - val_accuracy: 0.3771\n",
      "Epoch 59/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.9276 - accuracy: 0.3930\n",
      "Epoch 00059: val_loss improved from 3.72967 to 3.71881, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 2.9390 - accuracy: 0.3953 - val_loss: 3.7188 - val_accuracy: 0.3792\n",
      "Epoch 60/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.8728 - accuracy: 0.3987\n",
      "Epoch 00060: val_loss did not improve from 3.71881\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.9155 - accuracy: 0.3974 - val_loss: 3.7202 - val_accuracy: 0.3708\n",
      "Epoch 61/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.8780 - accuracy: 0.4123\n",
      "Epoch 00061: val_loss improved from 3.71881 to 3.70395, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.8912 - accuracy: 0.4047 - val_loss: 3.7040 - val_accuracy: 0.3792\n",
      "Epoch 62/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.8851 - accuracy: 0.4138\n",
      "Epoch 00062: val_loss improved from 3.70395 to 3.70026, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 2.8715 - accuracy: 0.4125 - val_loss: 3.7003 - val_accuracy: 0.3771\n",
      "Epoch 63/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.7802 - accuracy: 0.4186\n",
      "Epoch 00063: val_loss improved from 3.70026 to 3.69523, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.8478 - accuracy: 0.4104 - val_loss: 3.6952 - val_accuracy: 0.3833\n",
      "Epoch 64/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.8049 - accuracy: 0.4233\n",
      "Epoch 00064: val_loss improved from 3.69523 to 3.68880, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 2.8282 - accuracy: 0.4214 - val_loss: 3.6888 - val_accuracy: 0.3792\n",
      "Epoch 65/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.8253 - accuracy: 0.4318\n",
      "Epoch 00065: val_loss did not improve from 3.68880\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.8072 - accuracy: 0.4234 - val_loss: 3.6934 - val_accuracy: 0.3875\n",
      "Epoch 66/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.7873 - accuracy: 0.4245\n",
      "Epoch 00066: val_loss improved from 3.68880 to 3.67970, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 2.7873 - accuracy: 0.4224 - val_loss: 3.6797 - val_accuracy: 0.3833\n",
      "Epoch 67/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.7340 - accuracy: 0.4148\n",
      "Epoch 00067: val_loss did not improve from 3.67970\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.7677 - accuracy: 0.4281 - val_loss: 3.6799 - val_accuracy: 0.3771\n",
      "Epoch 68/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.7366 - accuracy: 0.4358\n",
      "Epoch 00068: val_loss improved from 3.67970 to 3.66979, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.7487 - accuracy: 0.4313 - val_loss: 3.6698 - val_accuracy: 0.3854\n",
      "Epoch 69/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.7516 - accuracy: 0.4356\n",
      "Epoch 00069: val_loss improved from 3.66979 to 3.66753, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 2.7339 - accuracy: 0.4406 - val_loss: 3.6675 - val_accuracy: 0.3854\n",
      "Epoch 70/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.7177 - accuracy: 0.4441\n",
      "Epoch 00070: val_loss did not improve from 3.66753\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.7153 - accuracy: 0.4365 - val_loss: 3.6682 - val_accuracy: 0.3812\n",
      "Epoch 71/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.6904 - accuracy: 0.4347\n",
      "Epoch 00071: val_loss improved from 3.66753 to 3.66292, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.6973 - accuracy: 0.4396 - val_loss: 3.6629 - val_accuracy: 0.3896\n",
      "Epoch 72/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 0s - loss: 2.6810 - accuracy: 0.4323\n",
      "Epoch 00072: val_loss improved from 3.66292 to 3.66177, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 215ms/step - loss: 2.6810 - accuracy: 0.4323 - val_loss: 3.6618 - val_accuracy: 0.4042\n",
      "Epoch 73/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.6562 - accuracy: 0.4498\n",
      "Epoch 00073: val_loss improved from 3.66177 to 3.65248, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 2.6650 - accuracy: 0.4365 - val_loss: 3.6525 - val_accuracy: 0.3917\n",
      "Epoch 74/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.6845 - accuracy: 0.4328\n",
      "Epoch 00074: val_loss improved from 3.65248 to 3.64186, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 216ms/step - loss: 2.6478 - accuracy: 0.4401 - val_loss: 3.6419 - val_accuracy: 0.3938\n",
      "Epoch 75/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 2.6109 - accuracy: 0.4594\n",
      "Epoch 00075: val_loss did not improve from 3.64186\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.6322 - accuracy: 0.4458 - val_loss: 3.6430 - val_accuracy: 0.4062\n",
      "Epoch 76/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.6004 - accuracy: 0.4375\n",
      "Epoch 00076: val_loss improved from 3.64186 to 3.63152, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 2.6160 - accuracy: 0.4406 - val_loss: 3.6315 - val_accuracy: 0.4062\n",
      "Epoch 77/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.6023 - accuracy: 0.4488\n",
      "Epoch 00077: val_loss improved from 3.63152 to 3.62510, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 206ms/step - loss: 2.5991 - accuracy: 0.4448 - val_loss: 3.6251 - val_accuracy: 0.4062\n",
      "Epoch 78/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.5529 - accuracy: 0.4470\n",
      "Epoch 00078: val_loss improved from 3.62510 to 3.61570, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 2.5849 - accuracy: 0.4464 - val_loss: 3.6157 - val_accuracy: 0.4042\n",
      "Epoch 79/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.5448 - accuracy: 0.4413\n",
      "Epoch 00079: val_loss improved from 3.61570 to 3.61285, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 2.5706 - accuracy: 0.4479 - val_loss: 3.6129 - val_accuracy: 0.4021\n",
      "Epoch 80/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4909 - accuracy: 0.4621\n",
      "Epoch 00080: val_loss improved from 3.61285 to 3.60746, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 2.5527 - accuracy: 0.4453 - val_loss: 3.6075 - val_accuracy: 0.4146\n",
      "Epoch 81/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.5135 - accuracy: 0.4601\n",
      "Epoch 00081: val_loss improved from 3.60746 to 3.59650, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.5412 - accuracy: 0.4557 - val_loss: 3.5965 - val_accuracy: 0.4125\n",
      "Epoch 82/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.5008 - accuracy: 0.4612\n",
      "Epoch 00082: val_loss improved from 3.59650 to 3.59264, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 212ms/step - loss: 2.5245 - accuracy: 0.4552 - val_loss: 3.5926 - val_accuracy: 0.4125\n",
      "Epoch 83/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.5423 - accuracy: 0.4505\n",
      "Epoch 00083: val_loss did not improve from 3.59264\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.5097 - accuracy: 0.4615 - val_loss: 3.5970 - val_accuracy: 0.4187\n",
      "Epoch 84/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.4755 - accuracy: 0.4661\n",
      "Epoch 00084: val_loss improved from 3.59264 to 3.58747, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.4940 - accuracy: 0.4578 - val_loss: 3.5875 - val_accuracy: 0.4187\n",
      "Epoch 85/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4439 - accuracy: 0.4744\n",
      "Epoch 00085: val_loss improved from 3.58747 to 3.58085, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 2.4795 - accuracy: 0.4635 - val_loss: 3.5809 - val_accuracy: 0.4146\n",
      "Epoch 86/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4486 - accuracy: 0.4697\n",
      "Epoch 00086: val_loss improved from 3.58085 to 3.57423, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 2.4653 - accuracy: 0.4625 - val_loss: 3.5742 - val_accuracy: 0.4187\n",
      "Epoch 87/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4634 - accuracy: 0.4508\n",
      "Epoch 00087: val_loss improved from 3.57423 to 3.56540, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.4508 - accuracy: 0.4615 - val_loss: 3.5654 - val_accuracy: 0.4187\n",
      "Epoch 88/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4391 - accuracy: 0.4650\n",
      "Epoch 00088: val_loss did not improve from 3.56540\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.4366 - accuracy: 0.4630 - val_loss: 3.5724 - val_accuracy: 0.4208\n",
      "Epoch 89/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.4071 - accuracy: 0.4688\n",
      "Epoch 00089: val_loss improved from 3.56540 to 3.56266, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 2.4221 - accuracy: 0.4672 - val_loss: 3.5627 - val_accuracy: 0.4229\n",
      "Epoch 90/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4429 - accuracy: 0.4706\n",
      "Epoch 00090: val_loss did not improve from 3.56266\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.4094 - accuracy: 0.4688 - val_loss: 3.5639 - val_accuracy: 0.4271\n",
      "Epoch 91/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4066 - accuracy: 0.4640\n",
      "Epoch 00091: val_loss improved from 3.56266 to 3.55039, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.3943 - accuracy: 0.4703 - val_loss: 3.5504 - val_accuracy: 0.4292\n",
      "Epoch 92/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.4292 - accuracy: 0.4564\n",
      "Epoch 00092: val_loss improved from 3.55039 to 3.54982, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 2.3803 - accuracy: 0.4724 - val_loss: 3.5498 - val_accuracy: 0.4313\n",
      "Epoch 93/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.3824 - accuracy: 0.4773\n",
      "Epoch 00093: val_loss improved from 3.54982 to 3.54934, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 2.3699 - accuracy: 0.4776 - val_loss: 3.5493 - val_accuracy: 0.4271\n",
      "Epoch 94/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.3716 - accuracy: 0.4809\n",
      "Epoch 00094: val_loss improved from 3.54934 to 3.53509, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 2.3568 - accuracy: 0.4776 - val_loss: 3.5351 - val_accuracy: 0.4250\n",
      "Epoch 95/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11/20 [===============>..............] - ETA: 0s - loss: 2.3301 - accuracy: 0.4773\n",
      "Epoch 00095: val_loss did not improve from 3.53509\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.3425 - accuracy: 0.4839 - val_loss: 3.5403 - val_accuracy: 0.4313\n",
      "Epoch 96/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.2954 - accuracy: 0.4852\n",
      "Epoch 00096: val_loss improved from 3.53509 to 3.52586, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 2.3275 - accuracy: 0.4823 - val_loss: 3.5259 - val_accuracy: 0.4271\n",
      "Epoch 97/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.3564 - accuracy: 0.4714\n",
      "Epoch 00097: val_loss improved from 3.52586 to 3.52183, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 2.3157 - accuracy: 0.4828 - val_loss: 3.5218 - val_accuracy: 0.4333\n",
      "Epoch 98/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.3335 - accuracy: 0.4826\n",
      "Epoch 00098: val_loss did not improve from 3.52183\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.3031 - accuracy: 0.4839 - val_loss: 3.5273 - val_accuracy: 0.4250\n",
      "Epoch 99/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.3099 - accuracy: 0.4740\n",
      "Epoch 00099: val_loss did not improve from 3.52183\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.2905 - accuracy: 0.4839 - val_loss: 3.5227 - val_accuracy: 0.4313\n",
      "Epoch 100/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.3261 - accuracy: 0.4800\n",
      "Epoch 00100: val_loss improved from 3.52183 to 3.50886, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 201ms/step - loss: 2.2772 - accuracy: 0.4859 - val_loss: 3.5089 - val_accuracy: 0.4396\n",
      "Epoch 101/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.2934 - accuracy: 0.4839\n",
      "Epoch 00101: val_loss improved from 3.50886 to 3.50048, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.2644 - accuracy: 0.4917 - val_loss: 3.5005 - val_accuracy: 0.4333\n",
      "Epoch 102/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.2408 - accuracy: 0.4924\n",
      "Epoch 00102: val_loss did not improve from 3.50048\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.2541 - accuracy: 0.4943 - val_loss: 3.5005 - val_accuracy: 0.4375\n",
      "Epoch 103/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.2517 - accuracy: 0.4974\n",
      "Epoch 00103: val_loss improved from 3.50048 to 3.49499, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 2.2421 - accuracy: 0.4984 - val_loss: 3.4950 - val_accuracy: 0.4396\n",
      "Epoch 104/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.2209 - accuracy: 0.5057\n",
      "Epoch 00104: val_loss did not improve from 3.49499\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.2309 - accuracy: 0.5010 - val_loss: 3.5009 - val_accuracy: 0.4375\n",
      "Epoch 105/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.2132 - accuracy: 0.4965\n",
      "Epoch 00105: val_loss did not improve from 3.49499\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.2166 - accuracy: 0.5000 - val_loss: 3.4990 - val_accuracy: 0.4354\n",
      "Epoch 106/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.1116 - accuracy: 0.5170\n",
      "Epoch 00106: val_loss did not improve from 3.49499\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.2013 - accuracy: 0.5021 - val_loss: 3.4976 - val_accuracy: 0.4333\n",
      "Epoch 107/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.1909 - accuracy: 0.5000\n",
      "Epoch 00107: val_loss improved from 3.49499 to 3.48521, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 2.1875 - accuracy: 0.5016 - val_loss: 3.4852 - val_accuracy: 0.4292\n",
      "Epoch 108/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.1404 - accuracy: 0.5191\n",
      "Epoch 00108: val_loss did not improve from 3.48521\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.1770 - accuracy: 0.5016 - val_loss: 3.4853 - val_accuracy: 0.4333\n",
      "Epoch 109/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.1762 - accuracy: 0.5009\n",
      "Epoch 00109: val_loss did not improve from 3.48521\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.1644 - accuracy: 0.5057 - val_loss: 3.4893 - val_accuracy: 0.4313\n",
      "Epoch 110/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.1946 - accuracy: 0.5000\n",
      "Epoch 00110: val_loss improved from 3.48521 to 3.48387, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 2.1493 - accuracy: 0.5135 - val_loss: 3.4839 - val_accuracy: 0.4375\n",
      "Epoch 111/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.1177 - accuracy: 0.5180\n",
      "Epoch 00111: val_loss improved from 3.48387 to 3.47854, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 182ms/step - loss: 2.1337 - accuracy: 0.5135 - val_loss: 3.4785 - val_accuracy: 0.4333\n",
      "Epoch 112/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.1383 - accuracy: 0.5047\n",
      "Epoch 00112: val_loss improved from 3.47854 to 3.47499, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 2.1204 - accuracy: 0.5125 - val_loss: 3.4750 - val_accuracy: 0.4333\n",
      "Epoch 113/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.0896 - accuracy: 0.5341\n",
      "Epoch 00113: val_loss did not improve from 3.47499\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.1067 - accuracy: 0.5172 - val_loss: 3.4781 - val_accuracy: 0.4333\n",
      "Epoch 114/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.1051 - accuracy: 0.5148\n",
      "Epoch 00114: val_loss improved from 3.47499 to 3.47326, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.0932 - accuracy: 0.5193 - val_loss: 3.4733 - val_accuracy: 0.4313\n",
      "Epoch 115/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.0888 - accuracy: 0.5417\n",
      "Epoch 00115: val_loss improved from 3.47326 to 3.46979, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 204ms/step - loss: 2.0798 - accuracy: 0.5380 - val_loss: 3.4698 - val_accuracy: 0.4292\n",
      "Epoch 116/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.0116 - accuracy: 0.5521\n",
      "Epoch 00116: val_loss did not improve from 3.46979\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.0694 - accuracy: 0.5432 - val_loss: 3.4707 - val_accuracy: 0.4271\n",
      "Epoch 117/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.0596 - accuracy: 0.5391\n",
      "Epoch 00117: val_loss did not improve from 3.46979\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.0547 - accuracy: 0.5437 - val_loss: 3.4751 - val_accuracy: 0.4271\n",
      "Epoch 118/200\n",
      "20/20 [==============================] - ETA: 0s - loss: 2.0421 - accuracy: 0.5479\n",
      "Epoch 00118: val_loss improved from 3.46979 to 3.46276, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 202ms/step - loss: 2.0421 - accuracy: 0.5479 - val_loss: 3.4628 - val_accuracy: 0.4271\n",
      "Epoch 119/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 2.0171 - accuracy: 0.5530\n",
      "Epoch 00119: val_loss did not improve from 3.46276\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.0300 - accuracy: 0.5479 - val_loss: 3.4678 - val_accuracy: 0.4292\n",
      "Epoch 120/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.0251 - accuracy: 0.5530\n",
      "Epoch 00120: val_loss improved from 3.46276 to 3.45300, saving model to model.h1.1121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 2.0193 - accuracy: 0.5495 - val_loss: 3.4530 - val_accuracy: 0.4292\n",
      "Epoch 121/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.9970 - accuracy: 0.5417\n",
      "Epoch 00121: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 2.0082 - accuracy: 0.5495 - val_loss: 3.4550 - val_accuracy: 0.4292\n",
      "Epoch 122/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.0187 - accuracy: 0.5434\n",
      "Epoch 00122: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9949 - accuracy: 0.5557 - val_loss: 3.4573 - val_accuracy: 0.4313\n",
      "Epoch 123/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 2.0189 - accuracy: 0.5425\n",
      "Epoch 00123: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9815 - accuracy: 0.5573 - val_loss: 3.4600 - val_accuracy: 0.4313\n",
      "Epoch 124/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.9524 - accuracy: 0.5608\n",
      "Epoch 00124: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9717 - accuracy: 0.5542 - val_loss: 3.4575 - val_accuracy: 0.4271\n",
      "Epoch 125/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.9619 - accuracy: 0.5642\n",
      "Epoch 00125: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9589 - accuracy: 0.5578 - val_loss: 3.4624 - val_accuracy: 0.4396\n",
      "Epoch 126/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.9755 - accuracy: 0.5599\n",
      "Epoch 00126: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9465 - accuracy: 0.5609 - val_loss: 3.4575 - val_accuracy: 0.4396\n",
      "Epoch 127/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.9169 - accuracy: 0.5660\n",
      "Epoch 00127: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9352 - accuracy: 0.5630 - val_loss: 3.4665 - val_accuracy: 0.4333\n",
      "Epoch 128/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.9239 - accuracy: 0.5597\n",
      "Epoch 00128: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9221 - accuracy: 0.5641 - val_loss: 3.4634 - val_accuracy: 0.4375\n",
      "Epoch 129/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.9190 - accuracy: 0.5556\n",
      "Epoch 00129: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9134 - accuracy: 0.5635 - val_loss: 3.4614 - val_accuracy: 0.4333\n",
      "Epoch 130/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.8616 - accuracy: 0.5738\n",
      "Epoch 00130: val_loss did not improve from 3.45300\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.9031 - accuracy: 0.5672 - val_loss: 3.4655 - val_accuracy: 0.4375\n",
      "Epoch 131/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.8174 - accuracy: 0.5852\n",
      "Epoch 00131: val_loss improved from 3.45300 to 3.44332, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 209ms/step - loss: 1.8919 - accuracy: 0.5693 - val_loss: 3.4433 - val_accuracy: 0.4375\n",
      "Epoch 132/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.9072 - accuracy: 0.5653\n",
      "Epoch 00132: val_loss improved from 3.44332 to 3.43751, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 180ms/step - loss: 1.8772 - accuracy: 0.5682 - val_loss: 3.4375 - val_accuracy: 0.4354\n",
      "Epoch 133/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.8550 - accuracy: 0.5843\n",
      "Epoch 00133: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.8658 - accuracy: 0.5708 - val_loss: 3.4498 - val_accuracy: 0.4333\n",
      "Epoch 134/200\n",
      "10/20 [==============>...............] - ETA: 0s - loss: 1.8455 - accuracy: 0.5792\n",
      "Epoch 00134: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.8538 - accuracy: 0.5729 - val_loss: 3.4392 - val_accuracy: 0.4417\n",
      "Epoch 135/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.8406 - accuracy: 0.5764\n",
      "Epoch 00135: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.8435 - accuracy: 0.5719 - val_loss: 3.4583 - val_accuracy: 0.4437\n",
      "Epoch 136/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.8551 - accuracy: 0.5755\n",
      "Epoch 00136: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.8304 - accuracy: 0.5792 - val_loss: 3.4472 - val_accuracy: 0.4396\n",
      "Epoch 137/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.7904 - accuracy: 0.5868\n",
      "Epoch 00137: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.8209 - accuracy: 0.5771 - val_loss: 3.4417 - val_accuracy: 0.4437\n",
      "Epoch 138/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.8060 - accuracy: 0.5755\n",
      "Epoch 00138: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.8100 - accuracy: 0.5792 - val_loss: 3.4401 - val_accuracy: 0.4437\n",
      "Epoch 139/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.8189 - accuracy: 0.5885\n",
      "Epoch 00139: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7997 - accuracy: 0.5839 - val_loss: 3.4492 - val_accuracy: 0.4417\n",
      "Epoch 140/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.7995 - accuracy: 0.5851\n",
      "Epoch 00140: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7868 - accuracy: 0.5865 - val_loss: 3.4522 - val_accuracy: 0.4437\n",
      "Epoch 141/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.8416 - accuracy: 0.5767\n",
      "Epoch 00141: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7742 - accuracy: 0.5854 - val_loss: 3.4459 - val_accuracy: 0.4458\n",
      "Epoch 142/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.7404 - accuracy: 0.5956\n",
      "Epoch 00142: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7632 - accuracy: 0.5901 - val_loss: 3.4437 - val_accuracy: 0.4521\n",
      "Epoch 143/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.7488 - accuracy: 0.5911\n",
      "Epoch 00143: val_loss did not improve from 3.43751\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7524 - accuracy: 0.5911 - val_loss: 3.4425 - val_accuracy: 0.4458\n",
      "Epoch 144/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.7330 - accuracy: 0.5877\n",
      "Epoch 00144: val_loss improved from 3.43751 to 3.42007, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 211ms/step - loss: 1.7422 - accuracy: 0.5938 - val_loss: 3.4201 - val_accuracy: 0.4437\n",
      "Epoch 145/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.7438 - accuracy: 0.5928\n",
      "Epoch 00145: val_loss did not improve from 3.42007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7318 - accuracy: 0.5953 - val_loss: 3.4516 - val_accuracy: 0.4500\n",
      "Epoch 146/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.7144 - accuracy: 0.6016\n",
      "Epoch 00146: val_loss did not improve from 3.42007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7232 - accuracy: 0.5995 - val_loss: 3.4352 - val_accuracy: 0.4500\n",
      "Epoch 147/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.7407 - accuracy: 0.5938\n",
      "Epoch 00147: val_loss did not improve from 3.42007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7113 - accuracy: 0.6005 - val_loss: 3.4350 - val_accuracy: 0.4542\n",
      "Epoch 148/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6765 - accuracy: 0.6111\n",
      "Epoch 00148: val_loss did not improve from 3.42007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.7006 - accuracy: 0.6073 - val_loss: 3.4224 - val_accuracy: 0.4521\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.7323 - accuracy: 0.6032\n",
      "Epoch 00149: val_loss did not improve from 3.42007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6910 - accuracy: 0.6099 - val_loss: 3.4222 - val_accuracy: 0.4583\n",
      "Epoch 150/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6483 - accuracy: 0.6181\n",
      "Epoch 00150: val_loss did not improve from 3.42007\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6782 - accuracy: 0.6073 - val_loss: 3.4256 - val_accuracy: 0.4542\n",
      "Epoch 151/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6609 - accuracy: 0.6076\n",
      "Epoch 00151: val_loss improved from 3.42007 to 3.41484, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 181ms/step - loss: 1.6668 - accuracy: 0.6130 - val_loss: 3.4148 - val_accuracy: 0.4563\n",
      "Epoch 152/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.6165 - accuracy: 0.6297\n",
      "Epoch 00152: val_loss did not improve from 3.41484\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6566 - accuracy: 0.6151 - val_loss: 3.4190 - val_accuracy: 0.4563\n",
      "Epoch 153/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6034 - accuracy: 0.6311\n",
      "Epoch 00153: val_loss did not improve from 3.41484\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6446 - accuracy: 0.6151 - val_loss: 3.4217 - val_accuracy: 0.4542\n",
      "Epoch 154/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6067 - accuracy: 0.6259\n",
      "Epoch 00154: val_loss did not improve from 3.41484\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6335 - accuracy: 0.6198 - val_loss: 3.4364 - val_accuracy: 0.4625\n",
      "Epoch 155/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6135 - accuracy: 0.6267\n",
      "Epoch 00155: val_loss improved from 3.41484 to 3.41028, saving model to model.h1.1121\n",
      "INFO:tensorflow:Assets written to: model.h1.1121/assets\n",
      "20/20 [==============================] - 4s 210ms/step - loss: 1.6242 - accuracy: 0.6219 - val_loss: 3.4103 - val_accuracy: 0.4563\n",
      "Epoch 156/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.6076 - accuracy: 0.6193\n",
      "Epoch 00156: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6130 - accuracy: 0.6240 - val_loss: 3.4176 - val_accuracy: 0.4625\n",
      "Epoch 157/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5959 - accuracy: 0.6389\n",
      "Epoch 00157: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.6023 - accuracy: 0.6328 - val_loss: 3.4184 - val_accuracy: 0.4542\n",
      "Epoch 158/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6031 - accuracy: 0.6241\n",
      "Epoch 00158: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5959 - accuracy: 0.6250 - val_loss: 3.4206 - val_accuracy: 0.4563\n",
      "Epoch 159/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.6183 - accuracy: 0.6311\n",
      "Epoch 00159: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5847 - accuracy: 0.6307 - val_loss: 3.4142 - val_accuracy: 0.4500\n",
      "Epoch 160/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5666 - accuracy: 0.6345\n",
      "Epoch 00160: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5745 - accuracy: 0.6323 - val_loss: 3.4453 - val_accuracy: 0.4521\n",
      "Epoch 161/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5826 - accuracy: 0.6241\n",
      "Epoch 00161: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5641 - accuracy: 0.6354 - val_loss: 3.4345 - val_accuracy: 0.4583\n",
      "Epoch 162/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.4932 - accuracy: 0.6493\n",
      "Epoch 00162: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5548 - accuracy: 0.6365 - val_loss: 3.4272 - val_accuracy: 0.4604\n",
      "Epoch 163/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5401 - accuracy: 0.6458\n",
      "Epoch 00163: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5453 - accuracy: 0.6406 - val_loss: 3.4300 - val_accuracy: 0.4542\n",
      "Epoch 164/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5436 - accuracy: 0.6285\n",
      "Epoch 00164: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5348 - accuracy: 0.6448 - val_loss: 3.4234 - val_accuracy: 0.4604\n",
      "Epoch 165/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.5358 - accuracy: 0.6506\n",
      "Epoch 00165: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5238 - accuracy: 0.6432 - val_loss: 3.4265 - val_accuracy: 0.4625\n",
      "Epoch 166/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5012 - accuracy: 0.6458\n",
      "Epoch 00166: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5133 - accuracy: 0.6490 - val_loss: 3.4268 - val_accuracy: 0.4604\n",
      "Epoch 167/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.4993 - accuracy: 0.6476\n",
      "Epoch 00167: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.5036 - accuracy: 0.6469 - val_loss: 3.4191 - val_accuracy: 0.4583\n",
      "Epoch 168/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.4831 - accuracy: 0.6484\n",
      "Epoch 00168: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4941 - accuracy: 0.6484 - val_loss: 3.4275 - val_accuracy: 0.4542\n",
      "Epoch 169/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5069 - accuracy: 0.6528\n",
      "Epoch 00169: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4828 - accuracy: 0.6495 - val_loss: 3.4229 - val_accuracy: 0.4563\n",
      "Epoch 170/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.4525 - accuracy: 0.6554\n",
      "Epoch 00170: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4751 - accuracy: 0.6521 - val_loss: 3.4329 - val_accuracy: 0.4583\n",
      "Epoch 171/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.4340 - accuracy: 0.6581\n",
      "Epoch 00171: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4653 - accuracy: 0.6526 - val_loss: 3.4192 - val_accuracy: 0.4604\n",
      "Epoch 172/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.4775 - accuracy: 0.6562\n",
      "Epoch 00172: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4584 - accuracy: 0.6562 - val_loss: 3.4204 - val_accuracy: 0.4542\n",
      "Epoch 173/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.5013 - accuracy: 0.6424\n",
      "Epoch 00173: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4494 - accuracy: 0.6542 - val_loss: 3.4405 - val_accuracy: 0.4583\n",
      "Epoch 174/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.4215 - accuracy: 0.6606\n",
      "Epoch 00174: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4393 - accuracy: 0.6609 - val_loss: 3.4246 - val_accuracy: 0.4583\n",
      "Epoch 175/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.4310 - accuracy: 0.6623\n",
      "Epoch 00175: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4324 - accuracy: 0.6578 - val_loss: 3.4127 - val_accuracy: 0.4563\n",
      "Epoch 176/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.4303 - accuracy: 0.6638\n",
      "Epoch 00176: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4258 - accuracy: 0.6604 - val_loss: 3.4172 - val_accuracy: 0.4563\n",
      "Epoch 177/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.3841 - accuracy: 0.6809\n",
      "Epoch 00177: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4146 - accuracy: 0.6620 - val_loss: 3.4459 - val_accuracy: 0.4583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 178/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3754 - accuracy: 0.6684\n",
      "Epoch 00178: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.4046 - accuracy: 0.6641 - val_loss: 3.4179 - val_accuracy: 0.4604\n",
      "Epoch 179/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3936 - accuracy: 0.6753\n",
      "Epoch 00179: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3949 - accuracy: 0.6651 - val_loss: 3.4240 - val_accuracy: 0.4604\n",
      "Epoch 180/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3838 - accuracy: 0.6684\n",
      "Epoch 00180: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3853 - accuracy: 0.6693 - val_loss: 3.4165 - val_accuracy: 0.4583\n",
      "Epoch 181/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3849 - accuracy: 0.6780\n",
      "Epoch 00181: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3765 - accuracy: 0.6724 - val_loss: 3.4178 - val_accuracy: 0.4604\n",
      "Epoch 182/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3769 - accuracy: 0.6658\n",
      "Epoch 00182: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3698 - accuracy: 0.6714 - val_loss: 3.4285 - val_accuracy: 0.4604\n",
      "Epoch 183/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3599 - accuracy: 0.6788\n",
      "Epoch 00183: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3591 - accuracy: 0.6760 - val_loss: 3.4388 - val_accuracy: 0.4583\n",
      "Epoch 184/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3276 - accuracy: 0.6858\n",
      "Epoch 00184: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3510 - accuracy: 0.6766 - val_loss: 3.4395 - val_accuracy: 0.4625\n",
      "Epoch 185/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3697 - accuracy: 0.6753\n",
      "Epoch 00185: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3420 - accuracy: 0.6802 - val_loss: 3.4405 - val_accuracy: 0.4625\n",
      "Epoch 186/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3271 - accuracy: 0.6840\n",
      "Epoch 00186: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3343 - accuracy: 0.6802 - val_loss: 3.4255 - val_accuracy: 0.4667\n",
      "Epoch 187/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3259 - accuracy: 0.6892\n",
      "Epoch 00187: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3280 - accuracy: 0.6865 - val_loss: 3.4243 - val_accuracy: 0.4667\n",
      "Epoch 188/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3108 - accuracy: 0.6832\n",
      "Epoch 00188: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3213 - accuracy: 0.6854 - val_loss: 3.4367 - val_accuracy: 0.4646\n",
      "Epoch 189/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3053 - accuracy: 0.6858\n",
      "Epoch 00189: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3150 - accuracy: 0.6849 - val_loss: 3.4195 - val_accuracy: 0.4625\n",
      "Epoch 190/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2865 - accuracy: 0.6979\n",
      "Epoch 00190: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.3051 - accuracy: 0.6901 - val_loss: 3.4406 - val_accuracy: 0.4625\n",
      "Epoch 191/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3240 - accuracy: 0.6884\n",
      "Epoch 00191: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2987 - accuracy: 0.6901 - val_loss: 3.4304 - val_accuracy: 0.4604\n",
      "Epoch 192/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.3016 - accuracy: 0.6866\n",
      "Epoch 00192: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2888 - accuracy: 0.6906 - val_loss: 3.4353 - val_accuracy: 0.4563\n",
      "Epoch 193/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2756 - accuracy: 0.6901\n",
      "Epoch 00193: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2800 - accuracy: 0.6948 - val_loss: 3.4285 - val_accuracy: 0.4604\n",
      "Epoch 194/200\n",
      "11/20 [===============>..............] - ETA: 0s - loss: 1.2713 - accuracy: 0.6989\n",
      "Epoch 00194: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2712 - accuracy: 0.6979 - val_loss: 3.4353 - val_accuracy: 0.4604\n",
      "Epoch 195/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2668 - accuracy: 0.7066\n",
      "Epoch 00195: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2635 - accuracy: 0.7000 - val_loss: 3.4341 - val_accuracy: 0.4625\n",
      "Epoch 196/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2787 - accuracy: 0.6979\n",
      "Epoch 00196: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2544 - accuracy: 0.7000 - val_loss: 3.4381 - val_accuracy: 0.4604\n",
      "Epoch 197/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2304 - accuracy: 0.7109\n",
      "Epoch 00197: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2459 - accuracy: 0.7010 - val_loss: 3.4411 - val_accuracy: 0.4583\n",
      "Epoch 198/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2636 - accuracy: 0.6910\n",
      "Epoch 00198: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2387 - accuracy: 0.7016 - val_loss: 3.4377 - val_accuracy: 0.4583\n",
      "Epoch 199/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2391 - accuracy: 0.7005\n",
      "Epoch 00199: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2297 - accuracy: 0.7031 - val_loss: 3.4493 - val_accuracy: 0.4583\n",
      "Epoch 200/200\n",
      "12/20 [=================>............] - ETA: 0s - loss: 1.2662 - accuracy: 0.6970\n",
      "Epoch 00200: val_loss did not improve from 3.41028\n",
      "20/20 [==============================] - 0s 6ms/step - loss: 1.2214 - accuracy: 0.7104 - val_loss: 3.4562 - val_accuracy: 0.4583\n"
     ]
    }
   ],
   "source": [
    "## trainX.shape (800, 7) trainY.shape (800, 3) \n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint\n",
    "filename = 'model.h1.1121'\n",
    "checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "history = model.fit(trainX, trainY.reshape(trainY.shape[0], trainY.shape[1], 1), \n",
    "          epochs=200, batch_size=32, #####################If more epochs here, the result would be better !!!\n",
    "          validation_split = 0.2,\n",
    "          callbacks=[checkpoint], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['train','validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#plt.plot(history.history['loss'])\n",
    "#plt.plot(history.history['val_loss'])\n",
    "plt.plot(losses)\n",
    "plt.legend(['train','validation', 'losses'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-42-5c93d5b25dcc>:3: Sequential.predict_classes (from tensorflow.python.keras.engine.sequential) is deprecated and will be removed after 2021-01-01.\n",
      "Instructions for updating:\n",
      "Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.keras.models import load_model\n",
    "#model = load_model('model.h1.24_jan_19')\n",
    "preds = model.predict_classes(testX.reshape((testX.shape[0],testX.shape[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[313, 333, 333],\n",
       "       [  0,   0,   0],\n",
       "       [333, 333, 333],\n",
       "       [183, 183, 183],\n",
       "       [203, 203, 355],\n",
       "       [200, 200, 200],\n",
       "       [316, 316, 316],\n",
       "       [  5,   5,   5],\n",
       "       [314, 314, 314],\n",
       "       [261, 233, 260],\n",
       "       [101, 101, 101],\n",
       "       [151, 355, 355],\n",
       "       [291, 291,  98],\n",
       "       [211, 211, 211],\n",
       "       [263, 263, 340],\n",
       "       [261, 261, 261],\n",
       "       [169, 169, 169],\n",
       "       [232, 232, 232],\n",
       "       [  2,   2,   2],\n",
       "       [  0,  40,  40],\n",
       "       [138, 138, 138],\n",
       "       [191, 191, 191],\n",
       "       [ 11,  11,  11],\n",
       "       [  2,   2,   2],\n",
       "       [183, 183, 183],\n",
       "       [254, 254, 254],\n",
       "       [ 68,  68,  68],\n",
       "       [254, 254, 254],\n",
       "       [314, 314, 314],\n",
       "       [377, 377, 377],\n",
       "       [355, 355, 355],\n",
       "       [137, 137, 137],\n",
       "       [321, 321, 321],\n",
       "       [ 97,  97,  97],\n",
       "       [ 97,  97,  97],\n",
       "       [332, 332, 332],\n",
       "       [138, 138, 138],\n",
       "       [117, 117, 117],\n",
       "       [ 29, 247, 247],\n",
       "       [ 27,  27,  27],\n",
       "       [193, 193, 193],\n",
       "       [  7,   7,   7],\n",
       "       [337, 337, 337],\n",
       "       [352, 352, 352],\n",
       "       [117, 117, 117],\n",
       "       [ 29,  29,  29],\n",
       "       [239, 239, 239],\n",
       "       [321, 321, 321],\n",
       "       [289, 289, 289],\n",
       "       [  4,   4,   4],\n",
       "       [263, 263, 263],\n",
       "       [358, 358, 358],\n",
       "       [ 73,  73,  73],\n",
       "       [193, 193, 193],\n",
       "       [  2,   2,  43],\n",
       "       [  0,   0,   0],\n",
       "       [184, 184, 352],\n",
       "       [263, 263, 263],\n",
       "       [243, 243, 243],\n",
       "       [180, 180, 180],\n",
       "       [209, 209, 209],\n",
       "       [332, 332, 332],\n",
       "       [283, 283, 283],\n",
       "       [217, 217, 217],\n",
       "       [117, 117, 117],\n",
       "       [301, 301, 301],\n",
       "       [180, 180, 180],\n",
       "       [125, 115, 115],\n",
       "       [187, 187, 187],\n",
       "       [193, 352, 352],\n",
       "       [ 35,  35,  35],\n",
       "       [175, 175, 175],\n",
       "       [292, 292, 292],\n",
       "       [101,   4,   4],\n",
       "       [166, 166, 166],\n",
       "       [ 29, 247, 247],\n",
       "       [179, 179, 179],\n",
       "       [147, 147, 147],\n",
       "       [169, 169, 169],\n",
       "       [265, 265, 265],\n",
       "       [276, 276, 358],\n",
       "       [ 68,  68,  68],\n",
       "       [  4, 217, 217],\n",
       "       [321, 321, 321],\n",
       "       [130, 130, 270],\n",
       "       [149, 149, 149],\n",
       "       [198, 198, 198],\n",
       "       [117, 355, 355],\n",
       "       [285, 285, 285],\n",
       "       [332, 332, 321],\n",
       "       [147, 147, 147],\n",
       "       [ 97,  97,  97],\n",
       "       [125, 222, 222],\n",
       "       [147, 147,  74],\n",
       "       [368, 368, 368],\n",
       "       [291, 291, 291],\n",
       "       [117, 117, 117],\n",
       "       [  4,   4,   4],\n",
       "       [267, 267, 267],\n",
       "       [169, 169, 169],\n",
       "       [241, 241, 241],\n",
       "       [291, 291, 291],\n",
       "       [285, 285, 285],\n",
       "       [ 47,  47,  47],\n",
       "       [263, 263, 263],\n",
       "       [149, 149, 149],\n",
       "       [ 66,  66,  66],\n",
       "       [117, 117, 117],\n",
       "       [215, 215, 215],\n",
       "       [ 97,  97,  97],\n",
       "       [286, 286, 286],\n",
       "       [  5,   5,   5],\n",
       "       [217, 217, 217],\n",
       "       [ 48,  48,  48],\n",
       "       [219, 219, 219],\n",
       "       [ 29,  29,  29],\n",
       "       [  7,   7,   7],\n",
       "       [125, 230, 230],\n",
       "       [382, 269, 269],\n",
       "       [247, 247, 247],\n",
       "       [163, 163, 335],\n",
       "       [358, 358, 358],\n",
       "       [209, 209, 209],\n",
       "       [125, 125, 125],\n",
       "       [210, 210, 210],\n",
       "       [111, 111, 111],\n",
       "       [ 97,  45,  45],\n",
       "       [335, 335, 335],\n",
       "       [117, 117, 117],\n",
       "       [211, 352, 352],\n",
       "       [ 73,  73,  73],\n",
       "       [314, 209, 209],\n",
       "       [289, 289, 289],\n",
       "       [370, 370, 370],\n",
       "       [324, 324, 324],\n",
       "       [118, 118, 118],\n",
       "       [377, 377, 377],\n",
       "       [116, 258, 258],\n",
       "       [247, 247, 247],\n",
       "       [123, 123, 123],\n",
       "       [ 97,  97,  97],\n",
       "       [  7,   7,   7],\n",
       "       [116, 258, 258],\n",
       "       [321, 321, 321],\n",
       "       [147, 147, 147],\n",
       "       [274,   0,   0],\n",
       "       [378, 378, 378],\n",
       "       [263, 263, 263],\n",
       "       [274, 274, 274],\n",
       "       [  7,   7,   7],\n",
       "       [361, 361, 361],\n",
       "       [233, 233, 233],\n",
       "       [156, 156, 156],\n",
       "       [201, 201, 188],\n",
       "       [ 29,  29,  29],\n",
       "       [289, 289, 289],\n",
       "       [149, 149, 149],\n",
       "       [291,  40,  40],\n",
       "       [156, 156, 156],\n",
       "       [326, 326, 326],\n",
       "       [169, 169, 169],\n",
       "       [287, 332, 332],\n",
       "       [334, 334, 334],\n",
       "       [  7,   7,   7],\n",
       "       [ 14,  14,  14],\n",
       "       [195, 195, 195],\n",
       "       [115, 115, 115],\n",
       "       [149, 149, 149],\n",
       "       [ 29, 247, 247],\n",
       "       [321, 321, 321],\n",
       "       [356,  35,  35],\n",
       "       [  0,  40,  40],\n",
       "       [138, 138, 138],\n",
       "       [314, 314, 314],\n",
       "       [169, 373, 373],\n",
       "       [ 45,  45,  45],\n",
       "       [  4,   4,   4],\n",
       "       [  4, 217, 217],\n",
       "       [311, 311, 311],\n",
       "       [263, 263, 263],\n",
       "       [ 21,  21,  21],\n",
       "       [130, 262, 262],\n",
       "       [ 89,  89,  89],\n",
       "       [263, 263, 263],\n",
       "       [118, 118, 118],\n",
       "       [343, 343, 343],\n",
       "       [ 97, 192, 192],\n",
       "       [324, 324, 324],\n",
       "       [235,   7,   7],\n",
       "       [ 97,  97,  97],\n",
       "       [321, 321, 321],\n",
       "       [267, 232, 232],\n",
       "       [174, 174, 174],\n",
       "       [ 77,  77,  77],\n",
       "       [182, 182, 182],\n",
       "       [355, 355, 119],\n",
       "       [258, 258, 258],\n",
       "       [ 56,  56,  56],\n",
       "       [ 26,  26,  26],\n",
       "       [ 94, 374, 374]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds ############# Preds is the vectors about the translation word #############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_word(n, tokenizer):\n",
    "    for word, index in tokenizer.word_index.items():\n",
    "        if index == n: \n",
    "    ###############Using Key-Value to search in tokenizer.word_index dictionary\n",
    "            return word\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#eng_tokenizer.word_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert predictions into text (English)\n",
    "preds_text = []\n",
    "for i in preds:\n",
    "    temp = []\n",
    "    for j in range(len(i)):\n",
    "        t = get_word(i[j], eng_tokenizer)\n",
    "        if j > 0:\n",
    "            if (t == get_word(i[j-1], eng_tokenizer)) or (t == None): \n",
    "                ############# The same meaning words or can not translate to English words\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t)\n",
    "             \n",
    "        else:\n",
    "            if(t == None):\n",
    "                temp.append('')\n",
    "            else:\n",
    "                temp.append(t) \n",
    "                ############# This English word can not be translated to Germany word ###########           \n",
    "        \n",
    "    preds_text.append(' '.join(temp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal epoches do better in translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['one far ',\n",
       " '  ',\n",
       " 'far  ',\n",
       " 'bald  ',\n",
       " 'inside  then',\n",
       " 'do  ',\n",
       " 'live  ',\n",
       " 'go  ',\n",
       " 'woke  ',\n",
       " 'wet burned snore',\n",
       " 'new  ',\n",
       " 'mad then ',\n",
       " 'touch  fair',\n",
       " 'hope  ',\n",
       " 'pull  open',\n",
       " 'wet  ',\n",
       " 'old  ',\n",
       " 'ignore  ',\n",
       " 'tom  ',\n",
       " ' watch ',\n",
       " 'brief  ',\n",
       " 'hurts  ',\n",
       " 'lost  ',\n",
       " 'tom  ',\n",
       " 'bald  ',\n",
       " 'sang  ',\n",
       " 'fun  ',\n",
       " 'sang  ',\n",
       " 'woke  ',\n",
       " '18  ',\n",
       " 'then  ',\n",
       " 'brave  ',\n",
       " 'awake  ',\n",
       " 'lied  ',\n",
       " 'lied  ',\n",
       " 'ive  ',\n",
       " 'brief  ',\n",
       " 'win  ',\n",
       " 'is wonderful ',\n",
       " 'this  ',\n",
       " 'left  ',\n",
       " 'get  ',\n",
       " 'tv  ',\n",
       " 'tight  ',\n",
       " 'win  ',\n",
       " 'is  ',\n",
       " 'loosen  ',\n",
       " 'awake  ',\n",
       " 'wept  ',\n",
       " 'im  ',\n",
       " 'pull  ',\n",
       " 'dozed  ',\n",
       " 'hes  ',\n",
       " 'left  ',\n",
       " 'tom  ask',\n",
       " '  ',\n",
       " 'free  tight',\n",
       " 'pull  ',\n",
       " 'unlock  ',\n",
       " 'use  ',\n",
       " 'give  ',\n",
       " 'ive  ',\n",
       " 'helps  ',\n",
       " 'want  ',\n",
       " 'win  ',\n",
       " 'lazy  ',\n",
       " 'use  ',\n",
       " 'sign answer ',\n",
       " 'rich  ',\n",
       " 'left tight ',\n",
       " 'help  ',\n",
       " 'cute  ',\n",
       " 'bless  ',\n",
       " 'new im ',\n",
       " 'freeze  ',\n",
       " 'is wonderful ',\n",
       " 'stayed  ',\n",
       " 'terrific  ',\n",
       " 'old  ',\n",
       " 'show  ',\n",
       " 'deaf  dozed',\n",
       " 'fun  ',\n",
       " 'im want ',\n",
       " 'awake  ',\n",
       " 'paid  real',\n",
       " 'what  ',\n",
       " 'fly  ',\n",
       " 'win then ',\n",
       " 'marry  ',\n",
       " 'ive  awake',\n",
       " 'terrific  ',\n",
       " 'lied  ',\n",
       " 'sign bored ',\n",
       " 'terrific  are',\n",
       " 'talked  ',\n",
       " 'touch  ',\n",
       " 'win  ',\n",
       " 'im  ',\n",
       " 'long  ',\n",
       " 'old  ',\n",
       " 'care  ',\n",
       " 'touch  ',\n",
       " 'marry  ',\n",
       " 'call  ',\n",
       " 'pull  ',\n",
       " 'what  ',\n",
       " 'came  ',\n",
       " 'win  ',\n",
       " 'promise  ',\n",
       " 'lied  ',\n",
       " 'speak  ',\n",
       " 'go  ',\n",
       " 'want  ',\n",
       " 'him  ',\n",
       " 'will  ',\n",
       " 'is  ',\n",
       " 'get  ',\n",
       " 'sign tired ',\n",
       " 'wine cheer ',\n",
       " 'wonderful  ',\n",
       " 'around  330',\n",
       " 'dozed  ',\n",
       " 'give  ',\n",
       " 'sign  ',\n",
       " 'had  ',\n",
       " 'busy  ',\n",
       " 'lied leave ',\n",
       " '330  ',\n",
       " 'win  ',\n",
       " 'hope tight ',\n",
       " 'hes  ',\n",
       " 'woke give ',\n",
       " 'wept  ',\n",
       " 'stood  ',\n",
       " 'going  ',\n",
       " 'excuse  ',\n",
       " '18  ',\n",
       " 'catch awesome ',\n",
       " 'wonderful  ',\n",
       " 'right  ',\n",
       " 'lied  ',\n",
       " 'get  ',\n",
       " 'catch awesome ',\n",
       " 'awake  ',\n",
       " 'terrific  ',\n",
       " 'resign  ',\n",
       " 'anyone  ',\n",
       " 'pull  ',\n",
       " 'resign  ',\n",
       " 'get  ',\n",
       " 'knits  ',\n",
       " 'burned  ',\n",
       " 'seize  ',\n",
       " 'lie  sure',\n",
       " 'is  ',\n",
       " 'wept  ',\n",
       " 'what  ',\n",
       " 'touch watch ',\n",
       " 'seize  ',\n",
       " 'needy  ',\n",
       " 'old  ',\n",
       " 'taste ive ',\n",
       " 'snowed  ',\n",
       " 'get  ',\n",
       " 'am  ',\n",
       " 'warn  ',\n",
       " 'answer  ',\n",
       " 'what  ',\n",
       " 'is wonderful ',\n",
       " 'awake  ',\n",
       " 'times help ',\n",
       " ' watch ',\n",
       " 'brief  ',\n",
       " 'woke  ',\n",
       " 'old all ',\n",
       " 'leave  ',\n",
       " 'im  ',\n",
       " 'im want ',\n",
       " 'like  ',\n",
       " 'pull  ',\n",
       " 'beat  ',\n",
       " 'paid perfect ',\n",
       " 'swam  ',\n",
       " 'pull  ',\n",
       " 'excuse  ',\n",
       " 'true  ',\n",
       " 'lied may ',\n",
       " 'going  ',\n",
       " '745 get ',\n",
       " 'lied  ',\n",
       " 'awake  ',\n",
       " 'long ignore ',\n",
       " 'tries  ',\n",
       " 'hit  ',\n",
       " 'yawned  ',\n",
       " 'then  forget',\n",
       " 'awesome  ',\n",
       " 'home  ',\n",
       " 'us  ',\n",
       " 'of aboard ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preds_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_df = pd.DataFrame({'actual' : test[:,0], 'predicted' : preds_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pd.set_option('display.max_colwidth', 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>actual</th>\n",
       "      <th>predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>im rich</td>\n",
       "      <td>one far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ignore it</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>keep warm</td>\n",
       "      <td>far</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im bored</td>\n",
       "      <td>bald</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>who quit</td>\n",
       "      <td>inside  then</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>stay cool</td>\n",
       "      <td>then  forget</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>watch us</td>\n",
       "      <td>awesome</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>who quit</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>go away</td>\n",
       "      <td>us</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>toms shy</td>\n",
       "      <td>of aboard</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>200 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        actual     predicted\n",
       "0      im rich      one far \n",
       "1    ignore it              \n",
       "2    keep warm         far  \n",
       "3     im bored        bald  \n",
       "4     who quit  inside  then\n",
       "..         ...           ...\n",
       "195  stay cool  then  forget\n",
       "196   watch us     awesome  \n",
       "197   who quit        home  \n",
       "198    go away          us  \n",
       "199   toms shy    of aboard \n",
       "\n",
       "[200 rows x 2 columns]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "actual_df_list = list(pred_df['actual'])\n",
    "pred_df_list = list(pred_df['predicted'])\n",
    "#pred_df_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.2882297539194154e-231\n",
      "1.5319719891192393e-231\n",
      "1.6034157163765524e-231\n",
      "1.5319719891192393e-231\n",
      "1.384292958842266e-231\n",
      "1.6954057018456463e-231\n",
      "1.384292958842266e-231\n",
      "1.5319719891192393e-231\n",
      "1.646211035903463e-231\n",
      "1.1988328686372911e-231\n",
      "1.2183324802375697e-231\n",
      "1.2508498911928379e-231\n",
      "1.4147351699132998e-231\n",
      "1.384292958842266e-231\n",
      "1.4488496539373276e-231\n",
      "1.4488496539373276e-231\n",
      "1.2183324802375697e-231\n",
      "1.0832677820940877e-231\n",
      "1.4488496539373276e-231\n",
      "1.331960397810445e-231\n",
      "1.4740564900137075e-231\n",
      "1.4740564900137075e-231\n",
      "1.5319719891192393e-231\n",
      "1.6034157163765524e-231\n",
      "1.1640469867513693e-231\n",
      "1.5319719891192393e-231\n",
      "1.2183324802375697e-231\n",
      "1.5319719891192393e-231\n",
      "1.384292958842266e-231\n",
      "1.2882297539194154e-231\n",
      "1.384292958842266e-231\n",
      "1.331960397810445e-231\n",
      "1.4740564900137075e-231\n",
      "1.384292958842266e-231\n",
      "1.1640469867513693e-231\n",
      "1.4488496539373276e-231\n",
      "1.583976781977924e-231\n",
      "1.4488496539373276e-231\n",
      "1.4347128449946335e-231\n",
      "1.384292958842266e-231\n",
      "1.384292958842266e-231\n",
      "1.2183324802375697e-231\n",
      "1.2882297539194154e-231\n",
      "1.4740564900137075e-231\n",
      "1.2183324802375697e-231\n",
      "1.5319719891192393e-231\n",
      "1.2882297539194154e-231\n",
      "1.331960397810445e-231\n",
      "1.384292958842266e-231\n",
      "1.5319719891192393e-231\n",
      "1.384292958842266e-231\n",
      "1.331960397810445e-231\n",
      "1.4488496539373276e-231\n",
      "1.384292958842266e-231\n",
      "1.4256605770826504e-231\n",
      "1.5319719891192393e-231\n",
      "1.1896457329133973e-231\n",
      "1.384292958842266e-231\n",
      "1.2882297539194154e-231\n",
      "1.6034157163765524e-231\n",
      "1.384292958842266e-231\n",
      "1.6034157163765524e-231\n",
      "1.4740564900137075e-231\n",
      "1.384292958842266e-231\n",
      "1.4488496539373276e-231\n",
      "1.5319719891192393e-231\n",
      "1.7229823170315744e-231\n",
      "1.4637115948630222e-231\n",
      "1.646211035903463e-231\n",
      "1.4147351699132998e-231\n",
      "1.384292958842266e-231\n",
      "1.384292958842266e-231\n",
      "1.331960397810445e-231\n",
      "1.583976781977924e-231\n",
      "1.0832677820940877e-231\n",
      "1.4347128449946335e-231\n",
      "1.4256605770826504e-231\n",
      "1.2183324802375697e-231\n",
      "1.6034157163765524e-231\n",
      "1.5319719891192393e-231\n",
      "1.1896457329133973e-231\n",
      "1.2183324802375697e-231\n",
      "1.4256605770826504e-231\n",
      "1.331960397810445e-231\n",
      "1.3483065280626046e-231\n",
      "1.384292958842266e-231\n",
      "1.2183324802375697e-231\n",
      "1.384292958842266e-231\n",
      "1.1200407237786664e-231\n",
      "1.2183324802375697e-231\n",
      "1.3483065280626046e-231\n",
      "1.384292958842266e-231\n",
      "1.1896457329133973e-231\n",
      "1.2627076138080564e-231\n",
      "1.5319719891192393e-231\n",
      "1.1200407237786664e-231\n",
      "1.6034157163765524e-231\n",
      "1.6954057018456463e-231\n",
      "1.384292958842266e-231\n",
      "1.6034157163765524e-231\n",
      "1.5319719891192393e-231\n",
      "1.4740564900137075e-231\n",
      "1.1200407237786664e-231\n",
      "1.384292958842266e-231\n",
      "1.384292958842266e-231\n",
      "1.646211035903463e-231\n",
      "1.5319719891192393e-231\n",
      "1.4488496539373276e-231\n",
      "1.2508498911928379e-231\n",
      "1.5319719891192393e-231\n",
      "1.331960397810445e-231\n",
      "1.2882297539194154e-231\n",
      "1.5319719891192393e-231\n",
      "1.6034157163765524e-231\n",
      "1.384292958842266e-231\n",
      "1.5319719891192393e-231\n",
      "1.6034157163765524e-231\n",
      "1.1896457329133973e-231\n",
      "1.4147351699132998e-231\n",
      "1.1896457329133973e-231\n",
      "1.4147351699132998e-231\n",
      "1.331960397810445e-231\n",
      "1.5319719891192393e-231\n",
      "1.7406562429229635e-231\n",
      "1.4488496539373276e-231\n",
      "1.1640469867513693e-231\n",
      "1.1896457329133973e-231\n",
      "1.2183324802375697e-231\n",
      "1.2183324802375697e-231\n",
      "1.4147351699132998e-231\n",
      "1.4488496539373276e-231\n",
      "1.3483065280626046e-231\n",
      "1.646211035903463e-231\n",
      "1.331960397810445e-231\n",
      "1.331960397810445e-231\n",
      "1.4256605770826504e-231\n",
      "1.2882297539194154e-231\n",
      "1.5319719891192393e-231\n",
      "1.4959003140449574e-231\n",
      "1.331960397810445e-231\n",
      "1.384292958842266e-231\n",
      "1.2183324802375697e-231\n",
      "1.331960397810445e-231\n",
      "1.583976781977924e-231\n",
      "1.3483065280626046e-231\n",
      "1.2882297539194154e-231\n",
      "1.0832677820940877e-231\n",
      "1.5319719891192393e-231\n",
      "1.5319719891192393e-231\n",
      "1.4488496539373276e-231\n",
      "1.583976781977924e-231\n",
      "1.4256605770826504e-231\n",
      "1.331960397810445e-231\n",
      "1.2508498911928379e-231\n",
      "1.5319719891192393e-231\n",
      "1.384292958842266e-231\n",
      "1.384292958842266e-231\n",
      "1.2882297539194154e-231\n",
      "1.583976781977924e-231\n",
      "1.4740564900137075e-231\n",
      "1.2183324802375697e-231\n",
      "1.3483065280626046e-231\n",
      "1.2882297539194154e-231\n",
      "1.4488496539373276e-231\n",
      "1.5319719891192393e-231\n",
      "1.5319719891192393e-231\n",
      "1.2882297539194154e-231\n",
      "1.5319719891192393e-231\n",
      "1.4347128449946335e-231\n",
      "1.4740564900137075e-231\n",
      "1.5656618337072542e-231\n",
      "1.4740564900137075e-231\n",
      "1.331960397810445e-231\n",
      "1.384292958842266e-231\n",
      "1.4256605770826504e-231\n",
      "1.331960397810445e-231\n",
      "1.5319719891192393e-231\n",
      "1.2882297539194154e-231\n",
      "1.384292958842266e-231\n",
      "1.5319719891192393e-231\n",
      "1.384292958842266e-231\n",
      "1.2627076138080564e-231\n",
      "1.384292958842266e-231\n",
      "1.384292958842266e-231\n",
      "0\n",
      "1.384292958842266e-231\n",
      "1.384292958842266e-231\n",
      "1.331960397810445e-231\n",
      "1.2882297539194154e-231\n",
      "1.384292958842266e-231\n",
      "1.1200407237786664e-231\n",
      "1.2882297539194154e-231\n",
      "1.1200407237786664e-231\n",
      "1.4488496539373276e-231\n",
      "1.4256605770826504e-231\n",
      "1.2882297539194154e-231\n",
      "1.4875195904069663e-231\n",
      "1.5319719891192393e-231\n",
      "1.2882297539194154e-231\n",
      "1.2183324802375697e-231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/albertliang/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/albertliang/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/Users/albertliang/opt/anaconda3/lib/python3.7/site-packages/nltk/translate/bleu_score.py:523: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "#reference = [['this', 'is', 'a', 'test'], ['this', 'is' 'test']]\n",
    "#candidate = ['this', 'is', 'a', 'baby']\n",
    "for i in range(0,200):\n",
    "    reference = actual_df_list[i]\n",
    "    candidate = pred_df_list[i]\n",
    "    score = sentence_bleu(reference, candidate)\n",
    "    print(score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
